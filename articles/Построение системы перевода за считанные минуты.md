# Построение системы перевода за считанные минуты

Последовательность за последовательностью (seq2seq)\[1\] - это универсальная структура, способная на многие вещи (языковой перевод, суммирование текста \[2\], субтитры \[3\] и т. д.). Для краткого введения в seq2seq, вот несколько хороших сообщений: [\[4\]](https://medium.com/towards-data-science/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d)  [\[5\]](https://medium.com/@devnag/seq2seq-the-clown-car-of-deep-learning-f88e1204dac3) .

Sean Robertson’s [tutorial notebook\[6\]](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb) а также Jeremy Howard’s лекции [\[6\]](http://course.fast.ai/lessons/lesson12.html)  [\[7\]](http://course.fast.ai/lessons/lesson13.html) являются отличной отправной точкой, чтобы получить четкое представление о технических деталях seq2seq. Тем не менее, я постараюсь избежать реализации всех этих деталей самостоятельно, когда имею дело с реальными проблемами. Изобретать колесо, как правило, не очень хорошая идея, особенно если вы новичок в этой области. Я обнаружил, что проект OpenNMT очень активен, имеет хорошую документацию и может быть использован "из коробки":

 [ **OpenNMT - Open-Source Neural Machine Translation**   
 _OpenNMT is an industrial-strength, open-source (MIT) neural machine translation system utilizing the Torch/ PyTorch…_ opennmt.net](http://opennmt.net/ "http://opennmt.net/") 

Существуют также более общие рамки ([например, \[8\]](https://github.com/google/seq2seq)), но, возможно, потребуется некоторая настройка, чтобы она работала над вашей конкретной проблемой.

Существует две официальные версии OpenNMT:

> [OpenNMT-Lua](https://github.com/OpenNMT/OpenNMT) (a.k.a. OpenNMT): основной проект, разработанный с помощью [LuaTorch](http://torch.ch/).
> Оптимизированный и стабильный код для производственных и масштабных экспериментов.

>  [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py) : облегченная версия OpenNMT с использованием [PyTorch](http://pytorch.org/) .  
> Первоначально созданная исследовательской группой Facebook AI в качестве примера проекта для PyTorch, эта версия проще в расширении и подходит для исследовательских целей, но не включает в себя все функции.

Мы собираемся использовать версию PyTorch в следующих разделах. Мы проведем вас через шаги, необходимые для создания очень простой системы перевода с набором данных среднего размера.

#### Шаг 1 Получить OpenNMT-py

Клонируйте репозиторий OpenNMT-py git на Github в локальную папку:

 [ **OpenNMT/OpenNMT-py**   
 _OpenNMT-py - Open-Source Neural Machine Translation in PyTorch http://opennmt.net/_ github.com](https://github.com/OpenNMT/OpenNMT-py "https://github.com/OpenNMT/OpenNMT-py") 

Возможно, вы захотите раскошелиться на Github, если вы планируете настроить или расширить его позже. Также предлагается в README:

> Codebase приближается к стабильной версии 0.1. В настоящее время мы рекомендуем разветвить, если вы хотите стабильный код.

#### Шаг 2: Загрузите набор данных

Здесь мы собираемся использовать набор данных из [AI Challenger - English-китайский конкурс машинного перевода](https://challenger.ai/competition/translation/subject?lan=en). Это набор данных с 10 миллионами англо-китайских пар предложений. Английская копора - это разговорный английский, извлеченный из веб-сайтов по изучению английского языка и субтитров к фильмам. Насколько я понимаю, большая часть перевода представлена энтузиастами, а не профессионалами. Переведенные китайские предложения проверяются аннотаторами-людьми.

 [ **English-Chinese Machine Translation - AI Challenger**   
 _English-Chinese Machine Translation - Prize:¥300,000 - Improve the performance of English-Chinese machine translation…_ challenger.ai](https://challenger.ai/competition/translation/subject?lan=en "https://challenger.ai/competition/translation/subject?lan=en")  [](https://challenger.ai/competition/translation/subject?lan=en) 

Для загрузки набора данных требуется регистрация учетной записи и, возможно, проверка идентификатора (не могу вспомнить, является ли последний обязательным). Если это проблема для вас, вы можете попробовать [datasets from WMT17](http://www.statmt.org/wmt17/translation-task.html#download) .

У набора данных AI Challenger есть некоторые проблемы: 1. Качество перевода не соответствует. 2. Поскольку многие предложения взяты из субтитров фильма, перевод часто зависит от контекста (относится к предыдущему или следующему предложению). Однако в наборе данных нет контекстной информации.

Давайте посмотрим, как стандартная модель работает с этим набором данных. Из-за ограничений памяти я сократил набор данных до **1 миллиона** предложений.

«Предположим, что вы поместили intofolder набора данных **_challenger_** в корневой каталог OpenNMT».

#### Шаг 3. Преобразование набора данных в простые тексты.

Набор данных для проверки и тестирования поставляется в формате XML. Нам нужно преобразовать его в простые текстовые файлы, где строка состоит из одного предложения. Простой способ сделать это - использовать BeautifulSoup. Вот пример кода:

```python
with open(input_file, "r") as f:  
    soup = BeautifulSoup(f.read(), "lxml")  
    lines = [  
      (int(x["id"]), x.text.strip()) for x in soup.findAll("seg")]  
    # Ensure the same order  
    lines = sorted(lines, key=lambda x: x[0])
```

#### Шаг 4: токенизируйте английские и китайские предложения

Входное предложение должно быть маркировано токенами через пробел.

Для английского есть несколько токенизаторов на выбор. Одним из примеров является `nltk.tokenize.word_tokenize`:

```python
with open(output_file, "w") as f:  
    f.write(  
        "\\n".join([  
             " ".join(word\_tokenize(l[1]))  
             for l in lines  
        ])  
    )
```

Получается “It’s a neat one — two. Walker to Burton.” в “It ‘s a neat one — two . Walker to Burton .”.

Для китайского языка мы используем простейший токенизация на уровне символов, то есть каждый символ рассматривается как токен:

```python
with open(output_file, "w") as f:  
    f.write(  
        "\\n".join([  
            " ".join([c if c != " " else "<s>" for c in l[1]])  
            for l in lines  
        ])  
    )
```

Получается “我就一天24小时都得在她眼皮子底下。” в “我 就 一 天 2 4 小 时 都 得 在 她 眼 皮 子 底 下 。”. (Обратите внимание, поскольку токен разделен пробелом, нам нужен специальный токен «\<s\>» для представления символов пробела.)

(Я не предоставил полный код для шага 3 и шага 4, потому что это действительно программирование на Python начального уровня. Вы должны быть в состоянии выполнить эти задачи самостоятельно.)

#### Step 5: Preprocess the dataset

Simply run the following command in the root directory:

```python
python preprocess.py -train_src challenger/train.en.sample \  
      -train_tg challenger/train.zh.sample \  
      -valid_src challenger/valid.en \  
      -valid_tgt challenger/valid.zh  \  
      -save_data challenger/opennmt -report_every 10000
```
	
The preprocessing script will go through the dataset, keep track of token frequencies, and construct a vocabulary list. I ran into memory problem here and had to down-sample the training dataset to 1 million rows, but I think the raw dataset should fit into 16GB memory with some optimization.

#### Step 6: Train the model

```python
python train.py -data challenger/opennmt \  
    -save_model models/baseline -gpuid 0 \  
    -learning_rate 0.001 -opt adam -epochs 20
```
	
It’ll use your first GPU to train a model. The default model structure is:

```
NMTModel (  
  (encoder): RNNEncoder (  
    (embeddings): Embeddings (  
      (make\_embedding): Sequential (  
        (emb\_luts): Elementwise (  
          (0): Embedding(50002, 500, padding\_idx=1)  
        )  
      )  
    )  
    (rnn): LSTM(500, 500, num\_layers=2, dropout=0.3)  
  )  
  (decoder): InputFeedRNNDecoder (  
    (embeddings): Embeddings (  
      (make\_embedding): Sequential (  
        (emb\_luts): Elementwise (  
          (0): Embedding(6370, 500, padding\_idx=1)  
        )  
      )  
    )  
    (dropout): Dropout (p = 0.3)  
    (rnn): StackedLSTM (  
      (dropout): Dropout (p = 0.3)  
      (layers): ModuleList (  
        (0): LSTMCell(1000, 500)  
        (1): LSTMCell(500, 500)  
      )  
    )  
    (attn): GlobalAttention (  
      (linear\_in): Linear (500 -> 500)  
      (linear\_out): Linear (1000 -> 500)  
      (sm): Softmax ()  
      (tanh): Tanh ()  
    )  
  )  
  (generator): Sequential (  
    (0): Linear (500 -> 6370)  
    (1): LogSoftmax ()  
  )  
)
```
	
The vocabulary size of source and target corpora is 50,002 and 6,370, respectively. The source vocabulary is obviously truncated to 50,000. The target vocabulary is relatively small because there are not that many common Chinese characters.

#### Step 7: Translate test/validation sentences

```python
python translate.py \  
    -model models/baseline_acc_58.79_ppl_7.51_e14  \  
    -src challenger/valid.en -tgt challenger/valid.zh \  
    -output challenger/valid_pred.58.79 -gpu 0 -replace_unk
```
	
Replace `models/baseline_acc_58.79_ppl_7.51_e14` with your own model. The model naming should be obvious: _this is a model after 14 epochs of training, with 58.79 accuracy and 7.51 perplexity on validation set_ .

You can also calculate BLEU score with the following:

```console
$ wget https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/generic/multi-bleu.perl 

$ perl multi-bleu.perl challenger/valid.zh \  
     < challenger/valid_pred.58.79
```
																	 
Now you have a working translation system!

#### Step 8: (Optional) Detokenize and convert the output

If you want to submit the translation to AI Challenger, you need to reverse Step 4 and then Step 3. Again, they should be quite simple to implement.

#### Some examples

English: You knew it in your heart you haven’t washed your hair  
Chinese(pred): 你心里清楚你没洗头发  
Chinese(gold): 你心里知道你压根就没洗过头

English: I never dreamed that one of my own would be going off to a University, but here I stand,  
Chinese(pred): 我从来没梦到过我的一个人会去大学，但是我站在这里，  
Chinese(gold): 我从没想过我的孩子会上大学，但我站在这，

English: We just don’t have time to waste on the wrong man.  
Chinese(pred): 我们只是没时间浪费人。  
Chinese(gold): 如果找错了人我们可玩不起。

The above three examples are, from top to bottom, _semantically correct_ , _partially correct_ , and _entirely incomprehensible_ . After examining a few examples, I found most of the machine translated sentences were partially correct, and there were surprising amount of semantically correct ones. Not a bad result, considering how little effort we’ve had put in so far.

#### The Next Step

If you submit the result you should get around  ** _.22_ ** BLEU. The current top BLEU score is  ** _.33_ ** , so there’s a lot of rooms for improvement. You can check out `opts.py` in the root folder for more built-in model parameters. Or dive deep into the codebase to figure out how things work and where might be improved.

The other paths include applying word segmentation on Chinese sentences, adding named entity recognition, using pronunciation dictionary [\[10\]](http://www.speech.cs.cmu.edu/cgi-bin/cmudict) to guess translation to unseen English names, etc.

(Update on 2017/10/14: If you use [jieba](https://github.com/fxsjy/jieba) and jieba.cut with default settings to tokenize the Chinese sentence, you’d get around  ** _.20_ ** BLEU on public leaderboad. One of the possible reasons to the score drop is its much larger Chinese vocabulary size. You can tell from the number of _\<unk>_ in the output.)

#### References:

1.   [Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) .
2.   [Nallapati, R., Zhou, B., dos Santos, C., & Xiang, B. (2016). Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.](https://arxiv.org/pdf/1602.06023.pdf) 
3.   [Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T., & Saenko, K. (2015). Sequence to Sequence](http://arxiv.org/abs/1505.00487) 
4.   [Sequence to sequence model: Introduction and concepts](https://medium.com/towards-data-science/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d) 
5.   [seq2seq: the clown car of deep learning](https://medium.com/@devnag/seq2seq-the-clown-car-of-deep-learning-f88e1204dac3) 
6.   [Practical PyTorch: Translation with a Sequence to Sequence Network and Attention](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb) 
7.   [Cutting Edge Deep Learning For Coders, Part 2, Lecture 12 — Attention Models](http://course.fast.ai/lessons/lesson12.html) 
8.   [Cutting Edge Deep Learning For Coders, Part 2, Lecture 13 — Neural Translation](http://course.fast.ai/lessons/lesson13.html) 
9.   [Google/seq2seq: A general-purpose encoder-decoder framework for Tensorflow](https://github.com/google/seq2seq) 
10.   [The CMU Pronouncing Dictionary](http://www.speech.cs.cmu.edu/cgi-bin/cmudict)