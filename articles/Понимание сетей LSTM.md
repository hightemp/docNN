# Понимание сетей LSTM
## Recurrent Neural Networks

Люди не начинают думать с нуля каждую секунду. Когда вы читаете это эссе, вы понимаете каждое слово на основе вашего понимания предыдущих слов. Вы не выбрасываете все и снова начинаете думать с нуля. Ваши мысли настойчивы.

Традиционные нейронные сети не могут этого сделать, и это кажется серьезным недостатком. Например, представьте, что вы хотите классифицировать, какое событие происходит в каждой точке фильма. Неясно, как традиционная нейронная сеть могла бы использовать свои рассуждения о предыдущих событиях в фильме для информирования более поздних.

Периодические нейронные сети решают эту проблему. Это сети с петлями в них, позволяющими сохранять информацию.

 ![](/images/684601aa63886d86a1b4dafcf8ab079c.png)  
**Периодические нейронные сети имеют петли.**

На приведенной выше диаграмме блок нейронной сети \\(A\\) просматривает некоторый вход \\(x\ _t\\) и выводит значение \\(h\_t\\). Цикл позволяет передавать информацию от одного шага сети к следующему.

Эти петли заставляют повторяющиеся нейронные сети казаться загадочными. Однако, если вы думаете немного больше, оказывается, что они ничем не отличаются от обычной нейронной сети. Периодическая нейронная сеть может рассматриваться как несколько копий одной и той же сети, каждая из которых передает сообщение преемнику. Посмотрим, что произойдет, если мы развернем цикл:

![An unrolled recurrent neural network.](/images/34a870b0e60d513e7153b3f27fa66786.png)  
**развернутая рекуррентная нейронная сеть.**

Эта цепочечная природа показывает, что рекуррентные нейронные сети тесно связаны с последовательностями и списками. Это естественная архитектура нейронной сети, используемая для таких данных.

И они, безусловно, используются! За последние несколько лет был достигнут невероятный успех в применении RNN для решения разнообразных задач: распознавания речи, языкового моделирования, перевода, субтитров изображения... Список можно продолжать. Я оставлю обсуждение удивительных подвигов, которых можно достичь с помощью RNN, замечательному посту Андрея Карпати, [Необоснованная эффективность рекуррентных нейронных сетей](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Но они действительно потрясающие.

Существенным для этих успехов является использование «LSTM», очень особого вида рекуррентной нейронной сети, которая для многих задач работает намного лучше, чем стандартная версия. Почти все захватывающие результаты, основанные на повторяющихся нейронных сетях, достигаются с их помощью. Именно эти LSTMs, которые исследует это эссе.

## Проблема долгосрочных зависимостей

Одна из привлекательных сторон сетей RNN заключается в том, что они могут подключить предыдущую информацию к текущей задаче, например, использование предыдущих видеокадров может дать понимание понимания текущего кадра. Если бы RNN могли сделать это, они были бы чрезвычайно полезны. Но могут ли они? Это зависит.

Иногда нам нужно только взглянуть на недавнюю информацию, чтобы выполнить текущую задачу. Например, рассмотрим языковую модель, пытающуюся предсказать следующее слово на основе предыдущих. Если мы пытаемся предсказать последнее слово в «облаках в _sky_», нам не нужен какой-либо дальнейший контекст - вполне очевидно, что следующим словом будет небо. В таких случаях, когда разрыв между соответствующей информацией и местом, в котором она необходима, невелик, RNN могут научиться использовать прошлую информацию.

 ![](/images/724834cc384c9fae8c3b3aff0d4a7a3a.png) 

Но есть также случаи, когда нам нужно больше контекста. Подумайте о том, чтобы попытаться предсказать последнее слово в тексте: «Я вырос во Франции. Я свободно говорю по-французски.» Недавняя информация говорит о том, что следующим словом, вероятно, является название языка, но если мы хотим сузить язык, нам нужен контекст Франции, если смотреть дальше. Вполне возможно, что разрыв между соответствующей информацией и точкой, в которой она необходима, станет очень большим.

К сожалению, по мере того, как этот разрыв увеличивается, RNN становятся неспособными научиться соединять информацию.

 ![Neural networks struggle with long term dependencies.](/images/6b911b2e1cb16cf47bf0f86257912c8e.png) 

Теоретически, RNN абсолютно способны обрабатывать такие «долгосрочные зависимости». Человек может тщательно подобрать параметры для них, чтобы решить игрушечные проблемы этой формы. К сожалению, на практике RNN, кажется, не в состоянии изучить их. Проблема была глубоко изучена [Hochreiter (1991) \[German\]](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf) а также [Bengio, et al. (1994)](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf), кто нашел несколько довольно фундаментальных причин, почему это может быть трудно.

К счастью, у LSTM нет этой проблемы!

## LSTM Сети

Сети с долговременной кратковременной памятью, обычно называемые «LSTM», представляют собой особый тип RNN, способный изучать долгосрочные зависимости. Они были представлены [Hochreiter & Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf) и были усовершенствованы и популяризированы многими людьми в следующей работе. [1](https://colah.github.io/posts/2015-08-Understanding-LSTMs/#fn1) Они отлично работают по широкому кругу проблем и в настоящее время широко используются.

LSTM явно разработаны, чтобы избежать проблемы долгосрочной зависимости. Запоминание информации в течение длительных периодов времени - это практически поведение по умолчанию, а не то, чему они пытаются научиться!

Все рекуррентные нейронные сети имеют форму цепочки повторяющихся модулей нейронной сети. В стандартных RNN этот повторяющийся модуль будет иметь очень простую структуру, такую как один слой tanh.

 ![](/images/bce0d99e8bc969fa3c2ffa99f93935c5.png)  
 **Повторяющийся модуль в стандартном RNN содержит один слой.** 

LSTM также имеют эту цепочечную структуру, но повторяющийся модуль имеет другую структуру. Вместо одного слоя нейронной сети существует четыре, взаимодействующих совершенно особым образом.

 ![Нейронная сеть LSTM.](/images/2795bc16b012322f7767cd4d940ba2e3.png)  
 **Повторяющийся модуль в LSTM содержит четыре взаимодействующих слоя.** 

Не беспокойтесь о деталях происходящего. Мы рассмотрим диаграмму LSTM шаг за шагом позже. А пока давайте попробуем освоиться с обозначениями, которые мы будем использовать.

 ![](/images/1aed12786463eade9585fd20d894f49c.png) 

На приведенной выше диаграмме каждая строка несет целый вектор от выхода одного узла до входов других. Розовые кружки представляют собой точечные операции, такие как сложение векторов, а желтые прямоугольники - это обученные слои нейронной сети. Линии слияния обозначают конкатенацию, в то время как линии разветвления обозначают копируемое содержимое, а копии перемещаются в разные места.

## Основная идея, стоящая за LSTM

Ключом к LSTM является состояние ячейки, горизонтальная линия, проходящая через верх диаграммы.

Состояние ячейки напоминает конвейерную ленту. Он идет прямо по всей цепочке с небольшими линейными взаимодействиями. Для информации очень просто течь по ней без изменений.

 ![](/images/1c663cbd3de24a90a0a94011fda6defd.png) 

LSTM имеет возможность удалять или добавлять информацию о состоянии ячейки, тщательно регулируемую структурами, называемыми воротами.

Ворота - это возможность, по желанию, пропустить информацию. Они состоят из слоя сигмовидной нейронной сети и операции точечного умножения.

 ![](/images/6ba1445193a5731e297922efdde6559f.png) 

Сигмовидный слой выводит числа от нуля до единицы, описывающие, сколько каждого компонента должно быть пропущено. Значение «ноль» означает «ничего не пропустить», а значение «один» означает «пропустить все через!»

У LSTM есть три из этих ворот, чтобы защитить и управлять состоянием ячейки.

## Пошаговое прохождение LSTM

Первый шаг в нашем LSTM - решить, какую информацию мы собираемся выбросить из состояния ячейки. Это решение принимается сигмовидным слоем, который называется «слой забытых ворот». Он просматривает \\(h\_{t-1}\\) и \\(x\_t\\) и выводит число между \\(0\\) и \\(1\\) для каждого числа в состоянии ячейки \\(C\_{t-1}\\).\\(1\\) представляет "полностью сохранить это", в то время как \\(0\\) представляет "полностью избавиться от этого".

Давайте вернемся к нашему примеру языковой модели, пытающейся предсказать следующее слово на основе всех предыдущих. В такой проблеме состояние ячейки может включать пол данного субъекта, так что можно использовать правильные местоимения. Когда мы видим новый предмет, мы хотим забыть пол старого предмета.

 ![](/images/4035f737ddf1b26add27f4f69ccc1483.png) 

Следующий шаг - решить, какую новую информацию мы будем хранить в состоянии ячейки. Это состоит из двух частей. Во-первых, сигмовидный слой, называемый «входной слой затвора», решает, какие значения мы будем обновлять. Затем слой tanh создает вектор новых значений-кандидатов \\(\\tilde {C}\_t\\), которые можно добавить в состояние. На следующем шаге мы скомбинируем эти два, чтобы создать обновление состояния.

В примере нашей языковой модели мы хотим добавить пол нового субъекта к состоянию ячейки, чтобы заменить старый, который мы забываем.

 ![](/images/5a460ea6f112de7332ca9584300c6e9c.png) 

Теперь пришло время обновить старое состояние ячейки, \\(C\_{t-1}\\), в новое состояние ячейки \\(C\_t\\). Предыдущие шаги уже решили, что делать, нам просто нужно сделать это на самом деле.

Мы умножаем старое состояние на \\(f\_t\\), забывая вещи, которые мы решили забыть ранее. Затем мы добавляем \\(i\_t\*\\tilde{C}\_t\\). Это новые значения кандидатов, в зависимости от того, насколько мы решили обновить каждое значение состояния.

В случае языковой модели, это то место, где мы фактически отбрасываем информацию о поле старого субъекта и добавляем новую информацию, как мы решили на предыдущих шагах.

 ![](/images/830f08d8fff0c45ff6350c8c473b50ba.png) 

Наконец, нам нужно решить, что мы собираемся выводить. Этот вывод будет основан на состоянии нашей ячейки, но будет отфильтрованной версией. Сначала мы запускаем сигмовидный слой, который решает, какие части состояния ячейки мы будем выводить. Затем мы помещаем состояние ячейки через \\(\\tanh\\) (чтобы значения оказались между \\(-1\\) и \\(1\\)) и умножаем его на выход sigmoid gate, так что мы выводим только те части, которые мы решили.

Для примера языковой модели, поскольку он только что увидел предмет, он может захотеть вывести информацию, относящуюся к глаголу, на случай, если это будет следующим. Например, он может выводить, является ли объект единственным или множественным, так что мы знаем, в какую форму должен быть спряжен глагол, если это будет следующим.

 ![](/images/c3c06077f1a4e436ac442e0623ac284e.png) 

## Варианты на долгосрочную память

То, что я описал до сих пор, это довольно нормальный LSTM. Но не все LSTM такие же, как указано выше. На самом деле, похоже, что почти во всех документах с LSTM используется немного другая версия. Различия незначительны, но стоит упомянуть некоторые из них.

Один из популярных вариантов LSTM, представленный [Gers & Schmidhuber (2000)](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf), добавляет «соединения с глазками». Это означает, что мы позволим слоям ворот взглянуть на состояние клетки.

 ![](/images/838ddfa82b7c6886c3687fa69574c212.png) 

Приведенная выше диаграмма добавляет глазки ко всем воротам, но во многих работах одни глазки даются, а другие нет.

Другой вариант - использовать соединенные входные и выходные входы. Вместо того, чтобы отдельно решать, что забыть и к чему мы должны добавить новую информацию, мы принимаем эти решения вместе. Мы только забываем, когда собираемся ввести что-то на свое место. Мы вводим новые значения в состояние, только когда забываем что-то старое.

 ![](/images/89bf31964d7553fa78bba9711c3d24e3.png) 

A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by [Cho, et al. (2014)](http://arxiv.org/pdf/1406.1078v3.pdf) . It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.

 ![A gated recurrent unit neural network.](/images/f2716bc289734d8b545926b38a224692.png) 

These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by [Yao, et al. (2015)](http://arxiv.org/pdf/1508.03790v2.pdf) . There’s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by [Koutnik, et al. (2014)](http://arxiv.org/pdf/1402.3511v1.pdf) .

Which of these variants is best? Do the differences matter? [Greff, et al. (2015)](http://arxiv.org/pdf/1503.04069.pdf) do a nice comparison of popular variants, finding that they’re all about the same. [Jozefowicz, et al. (2015)](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.

## Conclusion

Earlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks!

Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable.

LSTMs were a big step in what we can accomplish with RNNs. It’s natural to wonder: is there another big step? A common opinion among researchers is: “Yes! There is a next step and it’s attention!” The idea is to let every step of an RNN pick information to look at from some larger collection of information. For example, if you are using an RNN to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, [Xu, _et al._ (2015)](http://arxiv.org/pdf/1502.03044v2.pdf) do exactly this – it might be a fun starting point if you want to explore attention! There’s been a number of really exciting results using attention, and it seems like a lot more are around the corner…

Attention isn’t the only exciting thread in RNN research. For example, Grid LSTMs by [Kalchbrenner, _et al._ (2015)](http://arxiv.org/pdf/1507.01526v1.pdf) seem extremely promising. Work using RNNs in generative models – such as [Gregor, _et al._ (2015)](http://arxiv.org/pdf/1502.04623.pdf) , [Chung, _et al._ (2015)](http://arxiv.org/pdf/1506.02216v3.pdf) , or [Bayer & Osendorfer (2015)](http://arxiv.org/pdf/1411.7610v3.pdf) – also seems very interesting. The last few years have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so!

## Acknowledgments

I’m grateful to a number of people for helping me better understand LSTMs, commenting on the visualizations, and providing feedback on this post.

I’m very grateful to my colleagues at Google for their helpful feedback, especially [Oriol Vinyals](http://research.google.com/pubs/OriolVinyals.html) , [Greg Corrado](http://research.google.com/pubs/GregCorrado.html) , [Jon Shlens](http://research.google.com/pubs/JonathonShlens.html) , [Luke Vilnis](http://people.cs.umass.edu/~luke/) , and [Ilya Sutskever](http://www.cs.toronto.edu/~ilya/) . I’m also thankful to many other friends and colleagues for taking the time to help me, including [Dario Amodei](https://www.linkedin.com/pub/dario-amodei/4/493/393) , and [Jacob Steinhardt](http://cs.stanford.edu/~jsteinhardt/) . I’m especially thankful to [Kyunghyun Cho](http://www.kyunghyuncho.me/) for extremely thoughtful correspondence about my diagrams.

Before this post, I practiced explaining LSTMs during two seminar series I taught on neural networks. Thanks to everyone who participated in those for their patience with me, and for their feedback.

* * *

1.  In addition to the original authors, a lot of people contributed to the modern LSTM. A non-comprehensive list is: Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer, Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo, and [Alex Graves](https://scholar.google.com/citations?user=DaFHynwAAAAJ&hl=en) . [↩](https://colah.github.io/posts/2015-08-Understanding-LSTMs/#fnref1)






**********
[LSTM](/tags/LSTM.md)
