# Как выбрать количество скрытых слоев и узлов в нейронной сети с прямой связью?

Я понимаю, что на этот вопрос дан ответ, но я не думаю, что существующий ответ действительно затрагивает вопрос, помимо указания ссылки, обычно связанной с предметом вопроса. В частности, ссылка описывает один метод для программной конфигурации сети, но это не «_стандартный и принятый метод_» для конфигурации сети.

Следуя небольшому набору четких правил, можно программно установить компетентную сетевую архитектуру (то есть количество и тип нейронных слоев и количество нейронов, составляющих каждый слой). Следуя этой схеме, вы получите грамотную архитектуру, но, вероятно, не оптимальную.

Но после инициализации этой сети вы можете итеративно настраивать конфигурацию во время обучения, используя ряд вспомогательных алгоритмов; одно семейство этих работ выполняется _pruning_ узлами, основанными на (малых) значениях вектора веса после определенного количества обучающих эпох - другими словами, устраняя ненужные / избыточные узлы (подробнее об этом ниже).

Таким образом, каждый NN имеет три типа слоев: _input_, _hidden_ и _output_.

* * *

Поэтому создание архитектуры NN означает получение значений для числа уровней каждого типа и количества узлов в каждом из этих уровней.

**Входной слой** 

Просто - у каждого NN есть ровно один из них - никаких исключений, о которых я знаю.

Что касается количества нейронов, составляющих этот слой, этот параметр полностью и однозначно определяется, как только вы узнаете форму ваших тренировочных данных. В частности, _ количество нейронов, составляющих этот слой, равно количеству объектов (столбцов) в ваших данных_. Некоторые конфигурации NN добавляют один дополнительный узел для термина смещения.

* * *

**Выходной слой** 

Как и входной слой, у каждого NN есть ровно один выходной слой. Определить его размер (количество нейронов) просто; это полностью определяется выбранной конфигурацией модели.

Работает ли ваш NN в режиме _Machine_ или в режиме _Regression_ (соглашение ML об использовании термина, который также используется в статистике, но при назначении ему другого значения, очень сбивает с толку). Режим машины: возвращает метку класса (например, «Премиум-аккаунт» / «Базовая учетная запись»). Режим регрессии возвращает значение (например, цена).

Если NN является регрессором, то выходной слой имеет один узел.

Если NN является классификатором, то он также имеет один узел, если только не используется _softmax_, и в этом случае выходной слой имеет один узел на метку класса в вашей модели.

**Скрытые слои** 

Таким образом, эти несколько правил устанавливают количество слоев и размер (нейроны / слой) для входного и выходного слоев. Это оставляет скрытые слои.

Сколько скрытых слоев? Хорошо, если ваши данные линейно разделимы (что вы часто знаете, когда начинаете кодировать NN), тогда вам вообще не нужны никакие скрытые слои. Конечно, вам также не нужен NN для разрешения ваших данных, но он все равно сделает свою работу.

Beyond that, as you probably know, there's a mountain of commentary on the question of hidden layer configuration in NNs (see the insanely thorough and insightful [NN FAQ](http://www.faqs.org/faqs/ai-faq/neural-nets/part1/preamble.html) for an [excellent summary](http://www.faqs.org/faqs/ai-faq/neural-nets/part1/preamble.html) of that commentary). One issue within this subject on which there is a consensus is the performance difference from adding additional hidden layers: the situations in which performance improves with a second (or third, etc.) hidden layer are very few. _One hidden layer is sufficient for the large majority of problems._ 

So what about size of the hidden layer(s)--how many neurons? There are some empirically-derived rules-of-thumb, of these, the most commonly relied on is ' _the optimal size of the hidden layer is usually between the size of the input and size of the output layers_ '. Jeff Heaton, author of [Introduction to Neural Networks in Java](https://www.heatonresearch.com/book/) offers a few more.

In sum, for most problems, one could probably get decent performance (even without a second optimization step) by setting the hidden layer configuration using just two rules: (i) number of hidden layers equals one; and (ii) the number of neurons in that layer is the mean of the neurons in the input and output layers.

* * *

 **Optimization of the Network Configuration** 

 ** _Pruning_ ** describes a set of techniques to trim network size (by nodes not layers) to improve computational performance and sometimes resolution performance. The gist of these techniques is removing nodes from the network during training by identifying those nodes which, if removed from the network, would not noticeably affect network performance (i.e., resolution of the data). (Even without using a formal pruning technique, you can get a rough idea of which nodes are not important by looking at your weight matrix after training; look weights very close to zero--it's the nodes on either end of those weights that are often removed during pruning.) Obviously, if you use a pruning algorithm during training then begin with a network configuration that is more likely to have excess (i.e., 'prunable') nodes--in other words, when deciding on a network architecture, err on the side of more neurons, if you add a pruning step.

Put another way, by applying a pruning algorithm to your network during training, you can approach optimal network configuration; whether you can do that in a single "up-front" (such as a genetic-algorithm-based algorithm) I don't know, though I do know that for now, this two-step optimization is more common.

 [@doug's answer](https://stats.stackexchange.com/a/1097/15974) has worked for me. There's one additional rule of thumb that helps for supervised learning problems. You can usually prevent over-fitting if you keep your number of neurons below:

 $$N\_h = \\frac{N\_s} {(\\alpha \* (N\_i + N\_o))}$$ 

 $N\_i$ \= number of input neurons.  
 $N\_o$ \= number of output neurons.  
 $N\_s$ \= number of samples in training data set.  
 $\\alpha$ \= an arbitrary scaling factor usually 2-10.

 [Others recommend](http://www.solver.com/training-artificial-neural-network-intro) setting $alpha$ to a value between 5 and 10, but I find a value of 2 will often work without overfitting. You can think of alpha as the effective branching factor or number of nonzero weights for each neuron. Dropout layers will bring the "effective" branching factor way down from the actual mean branching factor for your network.

As explained by this [excellent NN Design text](http://hagan.okstate.edu/NNDesign.pdf#page=469) , you want to limit the number of free parameters in your model (its [degree](https://stats.stackexchange.com/q/57027/15974) or number of nonzero weights) to a small portion of the degrees of freedom in your data. The degrees of freedom in your data is the number samples \* degrees of freedom (dimensions) in each sample or $N\_s \* (N\_i + N\_o)$ (assuming they're all independent). So $\\alpha$ is a way to indicate how general you want your model to be, or how much you want to prevent overfitting.

For an automated procedure you'd start with an alpha of 2 (twice as many degrees of freedom in your training data as your model) and work your way up to 10 if the error (loss) for your training dataset is significantly smaller than for your test dataset.

**********
[Скрытые слои](/tags/%D0%A1%D0%BA%D1%80%D1%8B%D1%82%D1%8B%D0%B5%20%D1%81%D0%BB%D0%BE%D0%B8.md)
