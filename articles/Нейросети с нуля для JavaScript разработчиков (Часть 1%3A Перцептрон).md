# Нейросети с нуля для JavaScript разработчиков (Часть 1: Перцептрон)

 _Перевод статьи [Elyx0](https://twitter.com/Elyx0) : [Neural networks from scratch for Javascript linguists (Part1 — The Perceptron)](https://hackernoon.com/neural-networks-from-scratch-for-javascript-linguists-part1-the-perceptron-632a4d1fbad2) ._ 

 [ ![](/images/ee37f727c9426a8d6b687ec1cfd515bf) ](https://camo.githubusercontent.com/816a30add6b1dff12dc2d7f85d9f888714c16413/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a6958786e6c374d70457854486a5670756b48504d73672e676966)   
 _[Игра в змейку с обучением](http://snakeneuralnetwork.herokuapp.com/)_ 

Существует большая вероятность, что за последние месяцы вы хотя бы раз слышали о нейронной сети или искусственном интеллекте, которые, как представляется, творят чудеса: [оценка вашего селфи](http://karpathy.github.io/2015/10/25/selfie/) , [SIRI, понимающая ваш голос](http://mashable.com/2014/08/18/siri-fails/#hMwSkraMiPqp) , победа над человеком в таких играх, как Шахматы и [Го](http://www.cbc.ca/news/technology/go-google-alphago-lee-sedol-deepmind-1.3488913) , [превращение лошади в зебру](https://twitter.com/goodfellow_ian/status/851124988903997440) на произвольной картинке или нейросеть, делающая вас моложе, старше или [меняющая ваш пол](https://www.faceapp.com/) на любом фото.

От частого использования словосочетания «Искусственный Интеллект» в разных контекстах само понятие стало настолько размытым, что оно в лучшем случае коррелирует с « [чем-то, что выглядит умным](https://twitter.com/amyhoy/status/847097034536554497) ».

Возможно, из-за такой путаницы машинное обучение кажется слишком сложным для понимания, типа: «Готов поспорить, столько математики - не для меня!».

Не парьтесь, у меня та же история, так что давайте отправимся в **путешествие** . Я расскажу вам все, что узнал, мои заблуждения, способ правильно интерпретировать результаты и некоторые основные словарные и интересные факты на этом пути.

##  [](https://github.com/devSchacht/translations/tree/master/articles/neural-networks-from-scratch-for-javascript-linguists-part1-the-perceptron?source=post_page---------------------------#%D0%BE-%D1%87%D0%B5%D0%BC-%D0%BC%D1%8B-%D0%B3%D0%BE%D0%B2%D0%BE%D1%80%D0%B8%D0%BC) О чем мы говорим?

Представьте себе **коробку** , в которой вы вырезаете несколько отверстий и размещаете **предопределенное количество чисел**  `0` или `1` . Затем коробка бурно вибрирует и выдает из каждого отверстия одно число: `0` или `1` . Отлично.

 [ ![](/images/d3bfb823feef852bc11f4c46001fe7dd) ](https://camo.githubusercontent.com/ae362b287d1b090d8780ead8cdacffdb987e6bfe/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a716c5976437136596f6c434e5169716450744e646e672e706e67)   
 _Что дальше?_ 

Изначально ваша коробка **тупая** , как картон: она не выдаст ожидаемый результат магическим образом. Вы должны **обучить** её достижению желаемой **цели** .

##  [](https://github.com/devSchacht/translations/tree/master/articles/neural-networks-from-scratch-for-javascript-linguists-part1-the-perceptron?source=post_page---------------------------#%D0%BF%D0%BE%D0%BD%D0%B8%D0%BC%D0%B0%D0%BD%D0%B8%D0%B5-%D1%87%D0%B5%D1%80%D0%B5%D0%B7-%D0%BE%D0%BF%D1%8B%D1%82) Понимание через опыт

Моя самая большая ошибка - попытка осилить концепции, просто глядя на верхушку айсберга, играя с библиотеками и бомбить, когда ничего не работает. При попытке понять нейронные сети это непозволительно.

 [ ![](/images/86bba831408ffe32e13cb78f45921a69) ](https://camo.githubusercontent.com/f5cca1026f4b3960c40cec457630426c48c31178/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a45516649466b636b636b31436a31544e4c44586967512e676966)   
 _Время вернуться назад_ 

Изобретение этого магического термина датируется около 1943 года, когда 45-летний нейрофизиолог [Уоррен Стургис Мак-Каллок](https://ru.wikipedia.org/wiki/%D0%9C%D0%B0%D0%BA-%D0%9A%D0%B0%D0%BB%D0%BB%D0%BE%D0%BA,_%D0%A3%D0%BE%D1%80%D1%80%D0%B5%D0%BD) и его коллега Уолтер Питтс написали статью под названием [«Логическое исчисление идей, присущих нервной деятельности»](http://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf) .

По примеру классических философов Греции, Мак-Каллок попытался математически **смоделировать работу мозга** .

Это была яркая идея, учитывая то, как долго на тот момент было известно о [существовании нейрона](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%BC%D0%BE%D0%BD-%D0%B8-%D0%9A%D0%B0%D1%85%D0%B0%D0%BB%D1%8C,_%D0%A1%D0%B0%D0%BD%D1%82%D1%8C%D1%8F%D0%B3%D0%BE) (примерно с 1900-х годов) и что электрическая природа их сигналов будет продемонстрирована только в конце 1950-х годов.

Давайте здесь остановимся и вспомним, что опубликованные статьи не означают, что написавший их человек был абсолютно **прав** . Это означало: у парня **была гипотеза** , он знал кое что в этой области, имел какие-то результаты (иногда неприменимые) и опубликовал это. Затем другие квалифицированные люди из этой области должны **воспроизвести результаты** и решить, стоит ли на этом строить новую теорию.

Некоторые научные работы, будучи никем не проверяемы, могут породить аберрации, подобные [этой статье 2011 года](https://www.youtube.com/watch?v=42QuXLucH3Q) , в которой говорится, что люди могут видеть будущее.

Тем не менее, теперь мы знаем, что Мак-Каллок был нормальным парнем и в своей статье он попытался смоделировать некоторые алгоритмы физического мозга, лежащие в основе **нервной системы** (которой обладает большинство многоклеточных животных), являющейся на самом деле сетью **нейронов** .

 [ ![](/images/e635cf1d28c2aa7f8847a47e4d30b5c5) ](https://camo.githubusercontent.com/4509259803ec30d34f39786608dbb6885460f13b/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a4e4f52425849584a353449636b434a44666363394f672e676966)   
 _Надеюсь, читая дальше, вы не потеряете слишком много таких штук ( [источник](http://snakeneuralnetwork.herokuapp.com/) )_ 

Человеческий мозг имеет **\~86 миллиардов** нейронов, каждый из которых имеет **аксоны** , **дендриты** и **синапсы** , соединяющие **каждый** нейрон с **\~7000 других** . Это почти столько же связей, сколько [галактик во Вселенной](http://www.esa.int/Our_Activities/Space_Science/Herschel/How_many_stars_are_there_in_the_Universe) .

Проблема для Мак-Каллока заключалась в том, что в 1943 году экономическая ситуация была не очень оптимистичной: США в состоянии войны, Франклин Д. Рузвельт, чтобы предотвратить инфляцию, заморозил цены и заработную плату, Никола Тесла скончался, а лучшим компьютером был [ЭНИАК](https://ru.wikipedia.org/wiki/%D0%AD%D0%9D%D0%98%D0%90%D0%9A) , который стоил 7 миллионов долларов за [30 тонн](http://img04.deviantart.net/73c7/i/2011/249/1/f/your_mom_by_khelsiieexkhaotika-d4943v7.jpg) (в то время в технике властвовал дикий сексизм, поэтому тот факт, что ЭНИАК был [изобретен **6 женщинами** ](http://blogs.smithsonianmag.com/smartnews/2013/10/computer-programming-used-to-be-womens-work/) , заставил их коллег мужчин ошибочно его недооценивать). Для сравнения: стандартный телефон Samsung 2005 года имел в **1300** раз большую вычислительную мощность, чем ЭНИАК.

Затем в **1958** году компьютеры стали немного лучше и **Фрэнк Розенблатт** , вдохновленный Мак-Каллоком, подарил нам [ **Перцептрон** ](https://ru.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD) .

Пока все были заняты развитием этих идей, 11 лет спустя **Марвин Мински** решил, что ему [не нравится эта идея](http://blogs.umass.edu/brain-wars/the-debates/minsky-vs-rosenblatt/) , как и то, что Фрэнк Розенблатт не отстранен от работы. Он объяснился, опубликовав книгу, в которой сказал: _«Большинство работ Розенблатта... Не имеют научной ценности...»_ . Влияние этой книги заключается в том, что она истощила и без того низкое финансирование в этой области.

 [ ![](/images/dbcdb8539ad3efa52785e2a36c2097a3) ](https://camo.githubusercontent.com/0daaf92cc7ef3a2d5e8d10477497d50d55c7d73d/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a485355504b7a665a746c42704167766b515a373532772e676966)   
 _Типичный Мински_ 

Какие были доводы Мински против падавана Мак-Каллока?

##  [](https://github.com/devSchacht/translations/tree/master/articles/neural-networks-from-scratch-for-javascript-linguists-part1-the-perceptron?source=post_page---------------------------#%D0%B3%D0%BE%D0%BB%D0%BE%D0%B2%D0%BE%D0%BB%D0%BE%D0%BC%D0%BA%D0%B0-%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%BE%D1%81%D1%82%D0%B8) Головоломка Линейности

Хотя это понятие может звучать как название серии _«Теории Большого Взрыва»_ , оно фактически представляет собой основу теории Минского, призванной отвлечь от оригинального Перцептрона.

Перцептрон Розенблатта очень похож на нашу коробку, в которой на этот раз мы **пробурили одно отверстие** :

 [ ![](/images/cf71f65d73ebe1964156dd6408e29ada) ](https://camo.githubusercontent.com/af478946c3e87630a87641a9e95b39a45ef92556/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a426a364367485f4753557342435270535f4e365869512e706e67)   
 _Нейронная сеть на самом деле может принимать на вход **значения** между 0 и 1_ 

Если сумма наших **входных сигналов (x1...x4)** , умноженная на их **весовые коэффициенты (w1...w4)** плюс **смещение (b)** , достаточна для того, чтобы заставить **результирующий** затвор превышать **порог (T)** , наша дверь выдаст значение `1` , иначе - `0` .

Чтобы это произошло, пороговое значение сравнивается с результатом [ **функции активации** ](https://en.wikipedia.org/wiki/Activation_function) . Точно так же, как мозговые нейроны реагируют на **стимул** . Если раздражитель слишком низок, нейрон не пропускает сигнал вдоль аксона к дендритам.

 [ ![](/images/c0e4f56c4218f7b2b378f835f15a7426) ](https://camo.githubusercontent.com/9247f0e201a5e39a81c3bf5337cd5080d9f0139a/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a794d4278446768562d6949374d3941426d58655931672e676966)   
 _Деполяризация нейрона_ 

В общем случае в коде это выглядит так:

```js
// Инициализируем входные сигналы
const x1 = 1;
const x2 = 0.3;
const x3 = 0.2;
const x4 = 0.5;

// Инициализируем весовые коэффициенты
const w1 = 1.5;
const w2 = 0.2;
const w3 = 1.1;
const w4 = 1.05;

const Threshold = 1;
const bias = 0.3;

// Значение, вычисляемое для сравнения с пороговым, - сумма
// произведений входных значений
// (1*1.5)+(.3*0.2)+(.2*1.1)+(.5*1.05)
const sumInputsWeights = x1*w1 + x2*w2 + x3*w3 + x4*w4; // 2.305
const doorWillOpen = activation(sumInputsWeights + bias) > Threshold; // true
```

В организме человека ток нейрона в спокойном состоянии составляет -70 мВ, а его порог активации -55 мВ. В случае оригинального Перцептрона эта активация обрабатывается ступенчатой **функцией Хевисайда** .

 [ ![](/images/866e1dda72ab7c15807500021e970296) ](https://camo.githubusercontent.com/1db2c28309896b6701810781e1e2d3175949618c/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a73475f5971733954482d76386745546f4e55304c6f412e706e67)   
 _0, если x отрицательный, 1 - если нет. X является суммой входных весов и смещения_ 

Одной из наиболее известных функций активации является сигмоидальная функция: `F (x) = 1 / (1 + exp (-x))` и сдвиг ( _bias_ ) обычно используется для смещения порога активации:

 [ ![](/images/97629c64fc2cb549900cab97e63d0559) ](https://camo.githubusercontent.com/6208add77398eda5840b5d1b4e69183f3a31ef8b/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a4b56506d6b51555a72574b65386c6f5466774d7779412e706e67)   
 _Изменение функции активации может привести к большей вариативности результатов при срабатывании нейронов_ 

Некоторые функции активации могут давать отрицательные значения, а некоторые - нет. Это будет иметь важное значение при использовании результатов функции активации одного перцептрона для подачи в качестве входов другого перцептрона.

Использование `0` в качестве входного сигнала исключает связанный с ним вес из конечной суммы.

Таким образом, вместо наличия возможности получения отрицательного веса, `0` действует как деактиватор. Тогда как нам может потребоваться этот вес для инверсии значимости входа в итоговой сумме.

 [ ![](/images/076178531ff67ad82fdbb6513f4fb661) ](https://camo.githubusercontent.com/b0b3929c5ffe3d568a4b3963601eb9a85ad75a58/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a64712d51574f6738386f446b674d4b426543756a54672e706e67)   
 _Существуют различные функции активации_ 

Перцептрон известен как **бинарный классификатор** . Это означает, что он может классифицировать только **2 класса** (спам или не спам, апельсин или не апельсин... и так далее).

Его также можно назвать [ **линейный классификатор** ](https://ru.wikipedia.org/wiki/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D0%B9_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80) . Его цель состоит в том, чтобы определить, **к какому классу принадлежит объект** в соответствии со своими **характеристиками** (или «функциями»: наши **x1-x4** ), производя итерации до тех пор, пока не будет найдена **линия** , которая правильно **отделяет** объекты разных классов друг от друга.

Мы даем нашему классификатору некоторые примеры ожидаемых результатов, учитывая некоторые исходные данные, и **он тренируется** на поиск этого разделения путём регулировки **сдвига** и **весов** на каждом вводе.

В качестве примера давайте с использованием перцептрона классифицируем некоторые сущности по принципу **«опасен или нет»** в соответствии с двумя характеристиками: **количество зубов** и **размер** .

 [ ![](/images/2209353f38cbabf8d22e1a71c5232427) ](https://camo.githubusercontent.com/4c15a8b93cc5d348f8639cffe73d0bbec944bc23/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a5145777573466669696c666f65654e4e4c58467967672e676966)   
 _В зависимости от случая кошка может быть не того класса, что ожидается_ 

Теперь, когда мы натренировали наш перцептрон, мы можем классифицировать образцы, которых он никогда не видел:

 ** [Здесь](https://rosenblattperceptron.herokuapp.com/) живая демка. Нет, серьезно, зацените.** ( _прим. пер.: [здесь](https://github.com/Dvorson/rosenblattperceptronjs) исходный код с переводом интерфейса._ )

То, в чем Минский упрекал Розенблатта, состояло примерно в следующем: что, если вдруг мой тренировочный набор содержал **гигантскую змею** , почти не имеющую зубов, но большую, как слон.

 [ ![](/images/94df20fc1908b3dab3b842da3266e2d6) ](https://camo.githubusercontent.com/5e9bdbfe5c769f9f42a5d19035c8c4160a6b774b/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a697a72664f585275334231644c3259733566425a6b672e706e67)   
 _Тренировочный набор бывает невозможно разделить одной линией_ 

Теперь требуется две линии, чтобы правильно отделить красные объекты от зеленых. При использовании одного перцептрона, эта невозможность заставит его приспосабливаться вечно, будучи неспособным разделить входные данные на классы **одной прямой** .

Вы уже, должно быть, понимаете обычное представление перцептрона:

 [ ![](/images/0e43b8ff064cc3f4a37a33931337530a) ](https://camo.githubusercontent.com/b8d1150e58bb2ac6c0c36a145659ef31ae77617a/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a4b5f2d657258623437723643656f4a553631596736772e706e67)   
 _Сдвиг (bias) может применяться либо после вычисления суммы, либо в качестве добавленного веса для мнимого ввода, всегда равного 1_ 

##  [](https://github.com/devSchacht/translations/tree/master/articles/neural-networks-from-scratch-for-javascript-linguists-part1-the-perceptron?source=post_page---------------------------#%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B1%D0%B8%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%BE%D0%B9-%D0%BF%D1%80%D0%BE%D0%B1%D0%BB%D0%B5%D0%BC%D1%8B) Решение билинейной проблемы:

Один из способов - просто обучить два перцептрона: один ответственный за верхнее левое разделение, другой - за нижнее правое. **Подключая их вместе** через **логическое «и»** , мы создаём некий [ **мультиклассовый перцептрон** ](https://en.wikipedia.org/wiki/Perceptron#Multiclass_perceptron) .

```js
const p1 = new Perceptron();
p1.train(trainingSetTopLeft);
p1.learn(); // P1 готов.
const p2 = new Perceptron();
p2.train(trainingSetBottomRight);
p2.learn(); // P2 готов.
const inputs = [x1,x2];
const result = p1.predict(inputs) & p2.predict(inputs);
```

 [ ![](/images/b85670dad9ccc6b953b7efd752a9e2b7) ](https://camo.githubusercontent.com/bf07efe4164816f5c573ebd7d39665cb819a1e5a/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a31576744695a3275523148304a57467a6e44754c61512e706e67)   
 _Существуют и другие способы решения этой проблемы, которые мы рассмотрим в части 2_ 

Операция «логическое И», обозначаемая A^B, - одна из логических операций, которые может выполнять перцептрон. Полагая **w5** и **w6** равными примерно `0,6` и применяя смещение `-1` к нашей сумме, мы действительно создали предиктор AND.

Помните, что входы, связанные с w5 и w6, поступают от активации ступенчатой функции Heaviside, которая даёт только `0` или `1` в качестве выхода.

```
Для 0 & 0 : (0*0.6 + 0*0.6)-1 меньше 0, Вывод: 0
Для 0 & 1 : (0*0.6 + 1*0.6)-1 меньше 0, Вывод: 0
Для 1 & 1 : (1*0.6 + 1*0.6)-1 больше 0, Вывод: 1

```

Выполняя эту цепочку и имея «удлиненный» наш первоначальный перцептрон, мы создали так называемый **скрытый слой** , который в основном представляет собой несколько нейронов, вставленных между исходными входами и конечным выходом.

 [ ![](/images/8b046d6fd521141b4710d08dc1ea82aa) ](https://camo.githubusercontent.com/82183c631e2cf7e9e6ca641c427de4baef180a60/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a466962526d386a4762484f38686c77474f79416d42512e706e67)   
 _На самом деле их следует называть [«слоями детекторов признаков»](https://www.cs.cmu.edu/~dst/pubs/byte-hiddenlayer-1989.pdf) или «слоями подпроцессов»_ 

Хуже всего то, что **Минский все это знал** и решил сосредоточиться на простейшей версии перцептрона, чтобы дискредитировать все работы Розенблатта. Розенблатт уже рассмотрел многослойный и перекрестно-связанный перцептрон в своей книге _«Принципы нейродинамики: перцептроны и теория механизмов мозга»_ в 1962 году.

Все это подобно публикации книги про _транзисторы_ , в которой непризнание компьютера мотивируется тем, что обособленный транзистор бесполезен.

##  [](https://github.com/devSchacht/translations/tree/master/articles/neural-networks-from-scratch-for-javascript-linguists-part1-the-perceptron?source=post_page---------------------------#%D0%BD%D0%BE-%D0%BE%D1%82%D0%BA%D1%83%D0%B4%D0%B0-%D0%B1%D0%B5%D1%80%D0%B5%D1%82%D1%81%D1%8F-%D0%BB%D0%B8%D0%BD%D0%B8%D1%8F-%D0%BF%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD%D0%B0) Но откуда берется линия перцептрона?

Вы все должны быть знакомы со следующими уравнениями: `y = f(x)` , `Y = mx + c` или `y = ax + b` .

Итак, как мы получаем эти `m` и `c` из нашего тренированного перцептрона?

Чтобы понять это, мы должны вспомнить исходное уравнение нашего перцептрона: `x1 * w1 + x2 * w2 + bias > T` .

Смещение ( _bias_ ) и порог ( _threshold_ ) - это [одинаковые понятия](http://stackoverflow.com/questions/16609310/in-neural-networks-does-a-bias-change-the-threshold-of-an-activation-function) , а для перцептрона `T = 0` , что означает, что уравнение принимает вид:

 `x1 * w1 + x2 * w2 > -bias` , что можно переписать как:

 `X2 > (-w1 / w2) * x1 + (-bias / w2)` , сравнивая это с:

 `Y = m * x + b` , мы можем видеть, что

 `Y` это `x2` ,

 `X` это `x1` ,

 `M` это `(-w1 / w2)` и `b` это `(-bias / w2)` .

Это означает, что **градиент (m)** нашей линии определяется двумя весами, а **точка пересечения прямой и вертикальной оси** определяется сдвигом ( _bias_ ) и вторым весом.

Теперь мы можем просто выбрать два значения `x` ( `0` и `1` ), заменить их в линейном уравнении и найти соответствующие значения `y` , а затем проследить линию между двумя точками `(0; f(0))` и `(1; f(1))` .

Подобно оцепленному месту преступления, имея доступ к уравнению, мы можем включить дедукцию:

```
y = (-w1/w2)x + (-bias/w2)

```

*    **Градиент (угол поворота)** линии зависит только от двух весов
*   Знак **минус** перед `w1` означает, что если оба веса имеют один и тот же знак, линия будет наклонена вниз, как `\` , а если они отличаются друг от друга, она будет направлена вверх `/` 
*   Регулировка **w1** будет влиять на угол поворота, но не на координату точки пересечения с вертикальной осью, вместе с тем **w2** будет влиять и на **угол поворота и на координату точки пересечения** 
*   Поскольку **сдвиг (bias) - это числитель** (верхняя половина фракции), его увеличение поднимает линию выше по оси. (точка пересечения с вертикальной осью будет выше)

Вы можете проверить окончательные веса в [демо](https://rosenblattperceptron.herokuapp.com/) , введя в консоли `app.perceptron.weights` .

И это именно то, что наш перцептрон пытается оптимизировать, пока не найдет правильную линию.

 [ ![](/images/7ade2fea4c93649d886b5dd0553e7fe5) ](https://camo.githubusercontent.com/10c495d85f86c4250f06d8db9d1748243340b406/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a31685171414336377a6d53396e6f6d547578613755772e676966)   
 _Я знаю, это может выглядеть как 2 линии, но это одна, движущаяся супер быстро_ 

##  [](https://github.com/devSchacht/translations/tree/master/articles/neural-networks-from-scratch-for-javascript-linguists-part1-the-perceptron?source=post_page---------------------------#%D1%87%D1%82%D0%BE-%D0%B7%D0%BD%D0%B0%D1%87%D0%B8%D1%82-%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F) Что значит «оптимизация»?

Очевидно, наш перцептрон не может угадать вслепую и пробует каждое значение для своих весов, пока не найдет линию, правильно разделяющую сущности. Мы должны применять [ **правило дельты** ](https://ru.wikipedia.org/wiki/%D0%94%D0%B5%D0%BB%D1%8C%D1%82%D0%B0-%D0%BF%D1%80%D0%B0%D0%B2%D0%B8%D0%BB%D0%BE) .

Это правило обучения для обновления весов, связанных с входами в однослойной нейронной сети. Оно имеет следующее представление:

 [ ![](/images/a56845d492bec826cef71e443a3ea9ab) ](https://camo.githubusercontent.com/d125ab41677e8eb289137442d89a0719434734e7/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a42613863354e6c4f38566b4375397439676f2d3238512e706e67)   
 _Только не математика опять!_ 

Не беспокойтесь, это может быть «упрощено» следующим образом:

 [ ![](/images/fed7ef229cf3019e2129212d7d1fee36) ](https://camo.githubusercontent.com/18203ecc5b17e82c6551304c8d23ea6638f28546/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a3742434458764b543754435668716542685766696e412e706e67)   
 _Мы делаем это для каждого вхождения в тренировочном наборе_ 

Сопоставление `ожидаемое - фактическое` показывает значение **ошибки** (или стоимость). Цель состоит в том, чтобы пройти через тренировочный набор и уменьшить эту ошибку/стоимость до минимума, добавляя или вычитая небольшое количество весов каждого входа до тех пор, пока не совпадут с ответами все ожидания тренировочного набора.

Если после некоторых итераций **ошибка** равна ** `0` ** для каждого элемента в наборе тренировок, наш перцептрон обучен, и наше линейное уравнение, использующее эти конечные веса, правильно разделяет входящие наборы на классы.

##  [](https://github.com/devSchacht/translations/tree/master/articles/neural-networks-from-scratch-for-javascript-linguists-part1-the-perceptron?source=post_page---------------------------#%D0%B1%D1%83%D0%B4%D1%8C%D1%82%D0%B5-%D0%B2%D0%BD%D0%B8%D0%BC%D0%B0%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B-%D0%BA-%D1%81%D0%BA%D0%BE%D1%80%D0%BE%D1%81%D1%82%D0%B8-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F) Будьте внимательны к скорости обучения

В приведенном выше уравнении **α** представляет собой константу: скорость обучения, которая будет влиять на то, как изменяется каждый вес.

*   Если **α** слишком **мала** , для поиска правильных весов потребуется **больше итераций** и вы можете попасть в локальные минимумы.
*   Если **α** слишком **велика** , обучение может никогда не найти правильных весов.

Один из способов увидеть это - вообразить бедного парня в [железных сапогах](https://i.ytimg.com/vi/qHuhZxyWiFA/hqdefault.jpg) , связанных вместе, который хочет достичь сокровища на дне скалы, но он может двигаться только прыжком на **α** метров:

 [ ![](/images/00e5a26cf5cc3745eeebea3881d4b4b5) ](https://camo.githubusercontent.com/08e1b0f58b09c746adad464259da12d0004aa321/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a635f5f4a34505a4f6863364e563557376876335234772e706e67)   
 _α слишком велика_ 

 [ ![](/images/74270b0d8f6b52d1c25f1cff1438a74a) ](https://camo.githubusercontent.com/3123829cb719f67e8dbb66cb12b292e82c7d5355/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a354c5546433075673046454f423048504a42755463412e706e67)   
 _α слишком мала_ 

Одна вещь, которую вы, возможно, захотите сделать при чтении статьи в Википедии, - это перейти в раздел _«Обсуждение»_ , в котором обсуждаются спорные области содержимого.

 [ ![](/images/b3874652093fdf0ed8f3b8aa7ed3cec1) ](https://camo.githubusercontent.com/6a79cb29fdbe3adc28ec82edaf3a044c86c1490e/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a68486e637a5474307576703467367072596566434e412e706e67) 

В случае дельта-формулы в содержании говорилось, что она не может быть применена к перцептрону, потому что у функции Хевисайда не существует производной в точке `0` , но в разделе «Разговор» приведены статьи преподавателей MIT, использующих её.

Собрав воедино все, что мы узнали, мы, наконец, кодим перцептрон:

```js
// Полная версия: https://github.com/Elyx0/rosenblattperceptronjs/blob/master/src/Perceptron.js
class Perceptron {
  constructor(bias=1,learningRate=0.1,weights=[]) {
    this.bias = bias;
    this.learningRate = learningRate;
    this.weights = weights;
    this.trainingSet = [];
  }

  init(inputs,bias=this.bias) {
    // Инициализируем веса случайными значениями и добавляем сдвиговый вес
    this.weights = [...inputs.map(i => Math.random()), bias];
  }

  adjustWeights(inputs,expected) {
    const actual = this.evaluate(inputs);
    if (actual == expected) return true; // Если ошибки нет, возвращаем ничего не трогая

    // В противном случае корректируем веса, добавляя error * learningRate относительно очередного
    this.weights = this.weights.map((w,i) => w += this.delta(actual, expected,inputs[i]));
  }

  // Вычисляем разницу между выводом и ожиданием для текущего ввода
  delta(actual, expected, input,learningRate=this.learningRate) {
    const error = expected - actual; // Насколько мы ошиблись
    return error * learningRate * input;
  }

  // Сумма inputs * weights
  weightedSum(inputs=this.inputs,weights=this.weights) {
    return inputs.map((inp,i) => inp * weights[i]).reduce((x,y) => x+y,0);
  }

  // Вычисляем результат с текущими весами
  evaluate(inputs) {
    return this.activate(this.weightedSum(inputs));
  }

  // Heaviside в качестве функции активации
  activate(value) {
    return value >= 0 ? 1 : 0;
  }
}
```

Кроме того, Перцептрон относится к категории **нейронных сетей прямого распространения** , которая является просто причудливой формулировкой, говорящей о том, что связи между элементами сети не образуют цикл.

В чем была некоторая правота Мински: при том что алгоритм перцептрона гарантированно сходится на каком-либо решении в случае линейно разделяемого обучающего набора, он все равно может выбрать любое допустимое решение, а некоторые проблемы могут допускать множество решений различного качества.

Ваш мозг делает почти то же самое. Как только он найдет правильный способ взаимодействовать с мышцей (например: мышца реагирует сокращением), он будет использовать этот способ и далее. Эти связи «мозг-мышца» для всех разные.

Интеллектуалы любят спорить друг с другом, как Мински и Розенблатт. Даже **Эйнштейн** оказался неправ в своей борьбе против **Нильса Бора** по вопросу [квантового индетерминизма](https://en.wikipedia.org/wiki/Bohr%E2%80%93Einstein_debates) , утверждая своей знаменитой цитатой: «Бог не играет в кости».

Мы закончим поэтическими строками от отца нейронных сетей Уоррена Мак-Каллока (он [действительно](https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch#Biography) был поэтом).

Надеюсь, вы кое-чему научились, и я скоро увижу вас в Части 2.

```
Our knowledge of the world, including ourselves,
is incomplete as to space and indefinite as to time.
This ignorance, implicit in all our brains,
is the counterpart of the abstraction which renders our knowledge useful.
```

**********
[javascript](/tags/javascript.md)
