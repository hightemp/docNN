# Глубинные рекуррентные нейронные сети
> Depth-Gated Recurrent Neural Networks

https://arxiv.org/pdf/1508.03790v2.pdf

* Kaisheng Yao Microsoft Research kaisheny@microsoft.edu
* Trevor Cohn University of Melbourne tcohn@unimelb.edu.au
* Katerina Vylomova University of Melbourne
* Kevin Duh Nara Institute of Science and Technology
* Chris Dyer Carnegie Mellon University cdyer@cs.cmu.edu

## Abstract 

В этой короткой заметке мы представляем расширение LSTM для использования шлюза глубины для соединения ячеек памяти смежных слоев. Это вводит линейную зависимость между нижними и верхними рекуррентными единицами. Важно отметить, что линейная зависимость стробируется через стробирующую функцию, которую мы называем ворота-забывания. Эти ворота являются функцией ячейки памяти нижнего уровня, ее входа и ее прошлой памяти. Мы провели эксперименты и убедились, что эта новая архитектура LSTM способна улучшить характеристики машинного перевода и языкового моделирования.

## 1 Вступление

Глубокие нейронные сети (DNN) были успешно применены во многих областях, включая речь \[1\] и зрение \[2\]. В задачах обработки на естественном языке рекуррентные нейронные сети (RNN) \[3\] более широко используются благодаря своей способности запоминать долговременную зависимость. Однако у простого RNN есть проблемы уменьшения градиента или взрыва. Поскольку RNN могут рассматриваться как глубокие нейронные сети во многих случаях, градиенты в конце предложения могут не иметь возможности обратного распространения в начале предложения, поскольку существует много уровней нелинейного преобразования. Нейронные сети с кратковременной памятью (LSTM) \[4\] являются продолжением простого RNN \[3\]. Вместо использования нелинейной связи между прошлой скрытой активностью и скрытой активностью этого слоя, он использует линейную зависимость, чтобы связать свою прошлую память с текущей памятью. Важно отметить, что в LSTM введен элемент забытия, чтобы модулировать каждый элемент прошлой памяти, который будет добавлен в текущую ячейку памяти. LSTM и их расширения, например, Gated Recurrent Units \[5\] были успешно использованы во многих задачах обработки естественного языка \[5\], включая машинный перевод \[5, 6\] и понимание языка \[7, 8 \]. Для построения глубоких нейронных сетей стандартным способом является сложение множества слоев нейронных сетей. Это, однако, имеет ту же проблему построения простых рекуррентных сетей. Разница здесь в том, что сигналы ошибок сверху, а не из последнего времени, должны распространяться в обратном направлении через множество уровней нелинейного преобразования, и поэтому сигналы ошибок либо уменьшаются, либо взрываются.
В этой статье предлагается расширение LSTM. Ключевая концепция - это ворота глубины, которые модулируют линейную зависимость ячеек памяти в верхнем и нижнем уровнях.

## 2 Обзор текущих нейронных сетей 

### 2.1 Простой RNN

Простые рекуррентные нейронные сети (простые RNN) вычисляют вывод $y\_{t}$ следующим образом

![](/images/a2cd4f27bf900eb18e992789a4f3e67b)

где $W\_{y}$, $W\_{h}$ и $W\_{x}$ - матрицы для вывода скрытого слоя $h_{t}$, прошедшего действия скрытого слоя $h\_{t-1}$ и вход $x\_{t}$. Время повторения вводится в формуле. (2) связывает текущую активность скрытого слоя ht с прошлой активностью скрытого слоя $h\_{t-1} $. Эта зависимость нелинейна из-за использования логистической функции $\sigma\(·\)$.

### 2.2 Долгосрочная кратковременная память (LSTM)

Простой RNN, представленный выше, трудно обучить из-за проблем уменьшения градиента и взрыва. Это потому, что нелинейная связь между $h_{t}$ и $h_{t−1}$. LSTM был первоначально предложен в \[4\], а затем изменен в \[9\]. Мы следим за реализацией в \[9\]. LSTM вводит линейную зависимость между ячейками памяти $c_{t}$ и прошлым $c_{t−1}$. Кроме того, LSTM имеет входные и выходные вентили. Эти два элемента применяются к нелинейной функции на входе и нелинейной функции на выходе из LSTM. Специально LSTM написано ниже как

![](/images/c8d0416b9cace473fd931ad0655c3a8a)

где $i_{t}$, $f_{t}$ и $o_{t}$ - входной вентиль, забытый вентиль и выходной вентиль.

### 2.3 Закрытый рекуррентный блок

Закрытая рекуррентная единица (GRU) была предложена в \[10\]. Это похоже на LSTM в использовании стробирующих функций, но отличается от LSTM тем, что в нем нет ячейки памяти. Его операции могут быть кратко изложены в следующем

![](/images/4cb2929981ae19b8910a7c28ea7d69cd)

где вывод из GRU составляет $h_{t}$. $z_{t}$ и $r_{t}$ - ворота обновления и сброса. $\widetilde{h}\_{t} $ является выходным кандидатом. $W_{z}$, $W_{h}$, $W_{r}$, $U_{z}$ и $U_{r}$ - матрицы в GRU.

## 3 Глубинные рекуррентные нейронные сети

В 3.1 представлено расширение LSTM.

### 3.1 Глубинный(Depth-gated) LSTM

Глубинный LSTM показан на рис. 4. Он имеет затвор глубины, который соединяет ячейки памяти $c_{t}^{L + 1}$ в верхнем слое $L + 1$ и ячейку памяти c $L_{t}$ в нижнем слое $L$. Управление воротами глубины

![](/images/8858ec3cdfe5bb0e924c3a05b7fe997b)

сколько потока из нижней ячейки памяти непосредственно в ячейку памяти верхнего уровня. Функция затвора на слое $L + 1$ в момент времени $t$ является логистической функцией, так как

![](/images/b49967480a259e0857bddb65f5b20caa)

где $b_{d}^{L + 1}$ - это термин смещения. $W_{xd}^{L + 1}$ - это весовая матрица, связывающая глубинные врата со входом этого слоя. Это также относится к прошлой ячейке памяти через вектор весов $W_{cd}^{L + 1}$. Чтобы связать память нижнего уровня, он использует вектор весов $W_{l}^{L + 1}$.

![](/images/680a74bab0836325b2d099eee108f9fc)

Используя глубинный затвор, модуль DGLSTM можно записать как

![](/images/161054c18fefaa408c0b467772f9a439)

где $i_{t}^{L + 1}$, $f_{t}^{L + 1}$ $o_{t}^{L + 1}$ и $d_{t}^{L + 1}$ - входной вентиль, забытый вентиль, выходной вентиль и вентиль глубины.

## 4 Эксперименты

Мы применили DGLSTM к двум наборам данных. Первый - BTEC машинного перевода с китайского на английский. Второй набор данных PennTreeBank.

![](/images/48f8ddb0973575de63dec7204ba13ffc)

### 4.1 Результаты машинного перевода

Сначала мы сравнили DGLSTM с GRU и LSTM. Результаты показаны в таблице 1, которая показывает, что DGLSTM превосходит LSTM и GRU на всех глубинах. В другом эксперименте для эксперимента по машинному переводу мы используем DGLSTM для вывода результатов. Мы обучили 2 DGLSTMs. Один был с 3 слоями, а другой был с 5 слоями. Оба они использовали 50 скрытых размерных слоев. Они используются в качестве основной рекуррентной единицы в модели внимания \[5\]. Баллы используются для обучения реранкера. Мы запустили реранкер 10 раз. Их средние значения BLEU приведены в Таблице 2. По сравнению с исходным уровнем DGLSTM получил улучшение по 3 показателям BLEU.

### 4.2 Языковое моделирование

Мы провели эксперименты с набором данных PennTreeBank(PTB). Мы обучили двухслойный DGLSTM. Каждый слой имеет 200 векторов измерения. Результаты растерянности тестового набора показаны в Таблице 3. По сравнению с ранее опубликованными результатами для набора данных PTB, DGLSTM получил наименьшее недоумение в тестовом наборе PTB.

## 5 Сопутствующие работы

Последние работы по созданию глубоких сетей включают \[12\]. В \[12\] выход из слоя является линейной функцией для входа, в дополнение к пути из нелинейной части. Они оба закрыты, как следует

![](/images/0dfd103ef3e0ba48044288cc21a47bf7)

где $T$ и $C$, каждый, называется стробом преобразования и переносом переноса. Следовательно, выход сети автомагистрали имеет прямое соединение, хотя и стробированное, со входом. Тем не менее, глубинный LSTM линейно соединяет вход и выход через ячейку памяти. Следовательно, ключевое отличие глубинного LSTM от сети автомагистралей состоит в том, что ячейка памяти имеет ошибки как из будущего, так и из верхнего уровня, хотя и линейно, хотя и стробированного. Напротив, ячейка памяти в сети магистралей имеет только линейную зависимость между соседними временами.

## 6 Выводы

Мы представили углубленную архитектуру RNN. В частности, мы расширили LSTM, чтобы использовать вентиль глубины, который модулирует линейную зависимость ячеек памяти в рекуррентных единицах верхнего и нижнего уровней. Мы наблюдали лучшие результаты, используя эту новую модель в эксперименте по машинному переводу и в задаче моделирования языка.

![](/images/8a226d6a503ebea03414cdc80ad6fa09)

## Рекомендации 

\[1\] G. Hinton, L. Deng, D. Yu, G. Dahl, A. r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, and T. Sainath and. B. Kingsbury, “Deep neural networks for acoustic modeling in speech recognition,” 2012. \[2\] A Krizhevsky, I Sutskever, and G Hinton, “Imagenet classification with deep convolution neural networks,” in NIPS, 2012, vol. 25, pp. 1090–1098. \[3\] T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. Khudanpur, “Recurrent neural network based language model,” in INTERSPEECH, 2010, pp. 1045–1048. \[4\] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Computation, vol. 9, pp. 1735–1780, 1997. \[5\] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in arXiv:1409.0473 \[cs.CL\], 2014. \[6\] I. Sutskever, O. Vinyals, and Q.V. Le, “Sequence to sequence learning with neural networks,” in NIPS, 2014. \[7\] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, “Natural language processing (almost) from scratch,” Journal of Machine Learning Research, vol. 12, pp. 2493–2537, 2011. \[8\] K. Yao, B. Peng, G. Zweig, D. Yu, X. Li, and F. Gao, “Recurrent conditional random field for language understanding,” in ICASSP, 2014. \[9\] A. Graves, “Generating sequences with recurrent neural networks,” in arXiv:1308.0850 \[cs.NE\], 2013. \[10\] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio, “On the properties of neural machine translation,” in arXiv:1409.125, 2014. \[11\] R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio, “How to construct deep recurrent neural networks,” in arXiv:1312.6026 \[cs.NE\], 2013. \[12\] R. K. Srivastava, K. Greff, and Jurgen Schmidhuber, “Highway networks,” in arxiv:1505.00387v1, May 2015.

**********
[LSTM](/tags/LSTM.md)
