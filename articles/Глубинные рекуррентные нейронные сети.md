# Глубинные рекуррентные нейронные сети
> Depth-Gated Recurrent Neural Networks

https://arxiv.org/pdf/1508.03790v2.pdf

* Kaisheng Yao Microsoft Research kaisheny@microsoft.edu
* Trevor Cohn University of Melbourne tcohn@unimelb.edu.au
* Katerina Vylomova University of Melbourne
* Kevin Duh Nara Institute of Science and Technology
* Chris Dyer Carnegie Mellon University cdyer@cs.cmu.edu

## Abstract 

В этой короткой заметке мы представляем расширение LSTM для использования шлюза глубины для соединения ячеек памяти смежных слоев. Это вводит линейную зависимость между нижними и верхними рекуррентными единицами. Важно отметить, что линейная зависимость стробируется через стробирующую функцию, которую мы называем ворота-забывания. Эти ворота являются функцией ячейки памяти нижнего уровня, ее входа и ее прошлой памяти. Мы провели эксперименты и убедились, что эта новая архитектура LSTM способна улучшить характеристики машинного перевода и языкового моделирования.

## 1 Вступление

Глубокие нейронные сети (DNN) были успешно применены во многих областях, включая речь \[1\] и зрение \[2\]. В задачах обработки на естественном языке рекуррентные нейронные сети (RNN) \[3\] более широко используются благодаря своей способности запоминать долговременную зависимость. Однако у простого RNN есть проблемы уменьшения градиента или взрыва. Поскольку RNN могут рассматриваться как глубокие нейронные сети во многих случаях, градиенты в конце предложения могут не иметь возможности обратного распространения в начале предложения, поскольку существует много уровней нелинейного преобразования. Нейронные сети с кратковременной памятью (LSTM) \[4\] являются продолжением простого RNN \[3\]. Вместо использования нелинейной связи между прошлой скрытой активностью и скрытой активностью этого слоя, он использует линейную зависимость, чтобы связать свою прошлую память с текущей памятью. Важно отметить, что в LSTM введен элемент забытия, чтобы модулировать каждый элемент прошлой памяти, который будет добавлен в текущую ячейку памяти. LSTM и их расширения, например, Gated Recurrent Units \[5\] были успешно использованы во многих задачах обработки естественного языка \[5\], включая машинный перевод \[5, 6\] и понимание языка \[7, 8 \]. Для построения глубоких нейронных сетей стандартным способом является сложение множества слоев нейронных сетей. Это, однако, имеет ту же проблему построения простых рекуррентных сетей. Разница здесь в том, что сигналы ошибок сверху, а не из последнего времени, должны распространяться в обратном направлении через множество уровней нелинейного преобразования, и поэтому сигналы ошибок либо уменьшаются, либо взрываются.
В этой статье предлагается расширение LSTM. Ключевая концепция - это ворота глубины, которые модулируют линейную зависимость ячеек памяти в верхнем и нижнем уровнях.

## 2 Обзор текущих нейронных сетей 

### 2.1 Простой RNN

Простые рекуррентные нейронные сети (простые RNN) вычисляют вывод $y\_{t}$ следующим образом

![](/images/a2cd4f27bf900eb18e992789a4f3e67b)

где $W\_{y}$, $W\_{h}$ и $W\_{x}$ - матрицы для вывода скрытого слоя ht, прошедшего действия скрытого слоя $h\_{t-1}$ и вход $x\_{t}$. Время повторения вводится в формуле. (2) связывает текущую активность скрытого слоя ht с прошлой активностью скрытого слоя $h\_{t-1} $. Эта зависимость нелинейна из-за использования логистической функции $\sigma\(·\)$.

### 2.2 Long short-term memory (LSTM)

The simple RNN presented above is hard to train because of gradient diminishing and exploding problems. This is because the nonlinear relation between ht and $h_{t−1}$. LSTM was initially proposed in \[4\] and later modified in \[9\]. We follow the implementation in \[9\]. LSTM introduces a linear dependence between its memory cells ct and its past $c_{t−1}$. Additionally, LSTM has input and output gates. These two gates are applied on a nonlinear function on the input and a nonlinear function on the output from LSTM. Specially, LSTM is written below as

![](/images/c8d0416b9cace473fd931ad0655c3a8a)

where $i_{t}$, $f_{t}$ and $o_{t}$ are the input gate, forget gate and output gate.

### 2.3 Gated Recurrent Unit

A gated recurrent unit (GRU) was proposed in \[10\]. It is similar to LSTM in using gating functions, but differs from LSTM in that it doesn’t have a memory cell. Its operations can be summarized in the following

![](/images/4cb2929981ae19b8910a7c28ea7d69cd)

where the output from GRU is $h_{t}$. $z_{t}$ and $r_{t}$ are the update gate and reset gate. $\widetilde{h}\_{t}$ is the candidate output. $W_{z}$, $W_{h}$, $W_{r}$, $U_{z}$, and $U_{r}$ are the matrices in GRU.

## 3 The depth-gated recurrent neural networks

Sec. 3.1 presents the extension of LSTM.

### 3.1 Depth-gated LSTM

The depth-gated LSTM is illustrated in Fig. 4. It has a depth gate that connects the memory cells $c_{t}^{L+1}$ in the upper layer $L + 1$ and the memory cell c $L_{t}$ in the lower layer $L$. The depth-gate controls

![](/images/8858ec3cdfe5bb0e924c3a05b7fe997b)

how much flow from the lower memory cell directly to the upper layer memory cell. The gate function at layer $L + 1$ at time $t$ is a logistic function as

![](/images/b49967480a259e0857bddb65f5b20caa)

where $b_{d}^{L+1}$ is a bias term. $W_{xd}^{L+1}$ is the weight matrix to relate depth gate to the input of this layer. It also relates to the past memory cell via a weight vector $W_{cd}^{L+1}$. To relate the lower layer memory, it uses a weight vector $W_{l}^{L+1}$.

![](/images/680a74bab0836325b2d099eee108f9fc)

Using the depth gate, a DGLSTM unit can be written as

![](/images/161054c18fefaa408c0b467772f9a439)

where $i_{t}^{L+1}$, $f_{t}^{L+1}$ $o_{t}^{L+1}$, and $d_{t}^{L+1}$ are the input gate, forget gate, output gate and the depth gate.

## 4 Experiments

We applied DGLSTMs on two datasets. The first is BTEC Chinese to English machine translation. The second is PennTreeBank dataset.

![](/images/48f8ddb0973575de63dec7204ba13ffc)

### 4.1 Machine translation results

We first compared DGLSTM with GRU and LSTM. Results are shown in Table 1, which shows DGLSTM outperforms LSTM and GRU in all of the depths. In another experiment for the machine translation experiment, we use DGLSTM to output scores. We trained 2 DGLSTMs. One was with 3 layers and the other was with 5 layers. Both of them used 50 dimension hidden layers. They are used as the basic recurrent unit in an attention model \[5\]. The scores are used to train a reranker. We ran reranker 10 times. Their mean BLEU scores are listed in Table 2. Compared to the baseline, DGLSTM obtained 3 pointer BLEU score improvement.

### 4.2 Language modeling

We conducted experiments on PennTreeBank(PTB) dataset. We trained a two layer DGLSTM. Each layer has 200 dimension vector. Test set perplexity results are shown in Table 3. Compared the previously published results on PTB dataset, DGLSTM obtained the lowest perplexity on PTB test set.

## 5 Related works

Recent work to build deep networks include \[12\]. In \[12\], the output from a layer is a linear function to the input, in addition to the path from the nonlinear part. Both of them are gated, as follows

![](/images/0dfd103ef3e0ba48044288cc21a47bf7)

where $T$ and $C$ are each called transform gate and carry gate. Therefore, the highway network output has a direct connection, albeit gated, to the input. However, the depth-gated LSTM linearly connects input and output through memory cell. Therefore, the key difference of depth-gated LSTM from highway network is that memory cell has errors from both the future and also from the top layer, linearly albeit gated. In contrast, the memory cell in highway network only has linear dependence between adjacent times.

## 6 Conclusions

We have presented a depth-gated RNN architecture. In particular, we have extended LSTM to use the depth gate that modulates a linear dependence of the memory cells in the upper and lower layer recurrent units. We observed better performances using this new model on a machine translation experiment and a language modeling task.

![](/images/8a226d6a503ebea03414cdc80ad6fa09)

## References 

\[1\] G. Hinton, L. Deng, D. Yu, G. Dahl, A. r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, and T. Sainath and. B. Kingsbury, “Deep neural networks for acoustic modeling in speech recognition,” 2012. \[2\] A Krizhevsky, I Sutskever, and G Hinton, “Imagenet classification with deep convolution neural networks,” in NIPS, 2012, vol. 25, pp. 1090–1098. \[3\] T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. Khudanpur, “Recurrent neural network based language model,” in INTERSPEECH, 2010, pp. 1045–1048. \[4\] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Computation, vol. 9, pp. 1735–1780, 1997. \[5\] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in arXiv:1409.0473 \[cs.CL\], 2014. \[6\] I. Sutskever, O. Vinyals, and Q.V. Le, “Sequence to sequence learning with neural networks,” in NIPS, 2014. \[7\] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, “Natural language processing (almost) from scratch,” Journal of Machine Learning Research, vol. 12, pp. 2493–2537, 2011. \[8\] K. Yao, B. Peng, G. Zweig, D. Yu, X. Li, and F. Gao, “Recurrent conditional random field for language understanding,” in ICASSP, 2014. \[9\] A. Graves, “Generating sequences with recurrent neural networks,” in arXiv:1308.0850 \[cs.NE\], 2013. \[10\] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio, “On the properties of neural machine translation,” in arXiv:1409.125, 2014. \[11\] R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio, “How to construct deep recurrent neural networks,” in arXiv:1312.6026 \[cs.NE\], 2013. \[12\] R. K. Srivastava, K. Greff, and Jurgen Schmidhuber, “Highway networks,” in arxiv:1505.00387v1, May 2015.