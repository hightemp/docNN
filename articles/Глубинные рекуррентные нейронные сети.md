# Глубинные рекуррентные нейронные сети
> Depth-Gated Recurrent Neural Networks

https://arxiv.org/pdf/1508.03790v2.pdf

* Kaisheng Yao Microsoft Research kaisheny@microsoft.edu
* Trevor Cohn University of Melbourne tcohn@unimelb.edu.au
* Katerina Vylomova University of Melbourne
* Kevin Duh Nara Institute of Science and Technology
* Chris Dyer Carnegie Mellon University cdyer@cs.cmu.edu

## Abstract 

В этой короткой заметке мы представляем расширение LSTM для использования шлюза глубины для соединения ячеек памяти смежных слоев. Это вводит линейную зависимость между нижними и верхними рекуррентными единицами. Важно отметить, что линейная зависимость стробируется через стробирующую функцию, которую мы называем забудьте ворота. Эти ворота являются функцией ячейки памяти нижнего уровня, ее входа и ее прошлой памяти. Мы провели эксперименты и убедились, что эта новая архитектура LSTM способна улучшить характеристики машинного перевода и языкового моделирования.


