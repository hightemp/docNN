# Глубинные рекуррентные нейронные сети
> Depth-Gated Recurrent Neural Networks

https://arxiv.org/pdf/1508.03790v2.pdf

* Kaisheng Yao Microsoft Research kaisheny@microsoft.edu
* Trevor Cohn University of Melbourne tcohn@unimelb.edu.au
* Katerina Vylomova University of Melbourne
* Kevin Duh Nara Institute of Science and Technology
* Chris Dyer Carnegie Mellon University cdyer@cs.cmu.edu

## Abstract 

В этой короткой заметке мы представляем расширение LSTM для использования шлюза глубины для соединения ячеек памяти смежных слоев. Это вводит линейную зависимость между нижними и верхними рекуррентными единицами. Важно отметить, что линейная зависимость стробируется через стробирующую функцию, которую мы называем ворота-забывания. Эти ворота являются функцией ячейки памяти нижнего уровня, ее входа и ее прошлой памяти. Мы провели эксперименты и убедились, что эта новая архитектура LSTM способна улучшить характеристики машинного перевода и языкового моделирования.

## 1 Вступление

Глубокие нейронные сети (DNN) были успешно применены во многих областях, включая речь \[1\] и зрение \[2\]. В задачах обработки на естественном языке рекуррентные нейронные сети (RNN) \[3\] более широко используются благодаря своей способности запоминать долговременную зависимость. Однако у простого RNN есть проблемы уменьшения градиента или взрыва. Поскольку RNN могут рассматриваться как глубокие нейронные сети во многих случаях, градиенты в конце предложения могут не иметь возможности обратного распространения в начале предложения, поскольку существует много уровней нелинейного преобразования. Нейронные сети с кратковременной памятью (LSTM) \[4\] являются продолжением простого RNN \[3\]. Вместо использования нелинейной связи между прошлой скрытой активностью и скрытой активностью этого слоя, он использует линейную зависимость, чтобы связать свою прошлую память с текущей памятью. Важно отметить, что в LSTM введен элемент забытия, чтобы модулировать каждый элемент прошлой памяти, который будет добавлен в текущую ячейку памяти. LSTM и их расширения, например, Gated Recurrent Units \[5\] были успешно использованы во многих задачах обработки естественного языка \[5\], включая машинный перевод \[5, 6\] и понимание языка \[7, 8 \]. Для построения глубоких нейронных сетей стандартным способом является сложение множества слоев нейронных сетей. Это, однако, имеет ту же проблему построения простых рекуррентных сетей. Разница здесь в том, что сигналы ошибок сверху, а не из последнего времени, должны распространяться в обратном направлении через множество уровней нелинейного преобразования, и поэтому сигналы ошибок либо уменьшаются, либо взрываются.
В этой статье предлагается расширение LSTM. Ключевая концепция - это ворота глубины, которые модулируют линейную зависимость ячеек памяти в верхнем и нижнем уровнях.

## 2 Обзор текущих нейронных сетей 

### 2.1 Простой RNN

Простые рекуррентные нейронные сети (простые RNN) вычисляют вывод $y\_{t}$ следующим образом

![](/images/a2cd4f27bf900eb18e992789a4f3e67b)

где $W\_{y}$, $W\_{h}$ и $W\_{x}$ - матрицы для вывода скрытого слоя $h_{t}$, прошедшего действия скрытого слоя $h\_{t-1}$ и вход $x\_{t}$. Время повторения вводится в формуле. (2) связывает текущую активность скрытого слоя ht с прошлой активностью скрытого слоя $h\_{t-1} $. Эта зависимость нелинейна из-за использования логистической функции $\sigma\(·\)$.

### 2.2 Долгосрочная кратковременная память (LSTM)

Простой RNN, представленный выше, трудно обучить из-за проблем уменьшения градиента и взрыва. Это потому, что нелинейная связь между $h_{t}$ и $h_{t−1}$. LSTM был первоначально предложен в \[4\], а затем изменен в \[9\]. Мы следим за реализацией в \[9\]. LSTM вводит линейную зависимость между ячейками памяти $c_{t}$ и прошлым $c_{t−1}$. Кроме того, LSTM имеет входные и выходные вентили. Эти два элемента применяются к нелинейной функции на входе и нелинейной функции на выходе из LSTM. Специально LSTM написано ниже как

![](/images/c8d0416b9cace473fd931ad0655c3a8a)

где $i_{t}$, $f_{t}$ и $o_{t}$ - входной вентиль, забытый вентиль и выходной вентиль.

### 2.3 Закрытый рекуррентный блок

Закрытая рекуррентная единица (GRU) была предложена в \[10\]. Это похоже на LSTM в использовании стробирующих функций, но отличается от LSTM тем, что в нем нет ячейки памяти. Его операции могут быть кратко изложены в следующем

![](/images/4cb2929981ae19b8910a7c28ea7d69cd)

где вывод из GRU составляет $h_{t}$. $z_{t}$ и $r_{t}$ - ворота обновления и сброса. $\widetilde{h}\_{t} $ является выходным кандидатом. $W_{z}$, $W_{h}$, $W_{r}$, $U_{z}$ и $U_{r}$ - матрицы в GRU.

## 3 Глубинные рекуррентные нейронные сети

В 3.1 представлено расширение LSTM.

### 3.1 Глубинный(Depth-gated) LSTM

Глубинный LSTM показан на рис. 4. Он имеет затвор глубины, который соединяет ячейки памяти $c_{t}^{L + 1}$ в верхнем слое $L + 1$ и ячейку памяти c $L_{t}$ в нижнем слое $L$. Управление воротами глубины

![](/images/8858ec3cdfe5bb0e924c3a05b7fe997b)

сколько потока из нижней ячейки памяти непосредственно в ячейку памяти верхнего уровня. Функция затвора на слое $L + 1$ в момент времени $t$ является логистической функцией, так как

![](/images/b49967480a259e0857bddb65f5b20caa)

где $b_{d}^{L + 1}$ - это термин смещения. $W_{xd}^{L + 1}$ - это весовая матрица, связывающая глубинные врата со входом этого слоя. Это также относится к прошлой ячейке памяти через вектор весов $W_{cd}^{L + 1}$. Чтобы связать память нижнего уровня, он использует вектор весов $W_{l}^{L + 1}$.

![](/images/680a74bab0836325b2d099eee108f9fc)

Using the depth gate, a DGLSTM unit can be written as

![](/images/161054c18fefaa408c0b467772f9a439)

where $i_{t}^{L+1}$, $f_{t}^{L+1}$ $o_{t}^{L+1}$, and $d_{t}^{L+1}$ are the input gate, forget gate, output gate and the depth gate.

## 4 Experiments

We applied DGLSTMs on two datasets. The first is BTEC Chinese to English machine translation. The second is PennTreeBank dataset.

![](/images/48f8ddb0973575de63dec7204ba13ffc)

### 4.1 Machine translation results

We first compared DGLSTM with GRU and LSTM. Results are shown in Table 1, which shows DGLSTM outperforms LSTM and GRU in all of the depths. In another experiment for the machine translation experiment, we use DGLSTM to output scores. We trained 2 DGLSTMs. One was with 3 layers and the other was with 5 layers. Both of them used 50 dimension hidden layers. They are used as the basic recurrent unit in an attention model \[5\]. The scores are used to train a reranker. We ran reranker 10 times. Their mean BLEU scores are listed in Table 2. Compared to the baseline, DGLSTM obtained 3 pointer BLEU score improvement.

### 4.2 Language modeling

We conducted experiments on PennTreeBank(PTB) dataset. We trained a two layer DGLSTM. Each layer has 200 dimension vector. Test set perplexity results are shown in Table 3. Compared the previously published results on PTB dataset, DGLSTM obtained the lowest perplexity on PTB test set.

## 5 Related works

Recent work to build deep networks include \[12\]. In \[12\], the output from a layer is a linear function to the input, in addition to the path from the nonlinear part. Both of them are gated, as follows

![](/images/0dfd103ef3e0ba48044288cc21a47bf7)

where $T$ and $C$ are each called transform gate and carry gate. Therefore, the highway network output has a direct connection, albeit gated, to the input. However, the depth-gated LSTM linearly connects input and output through memory cell. Therefore, the key difference of depth-gated LSTM from highway network is that memory cell has errors from both the future and also from the top layer, linearly albeit gated. In contrast, the memory cell in highway network only has linear dependence between adjacent times.

## 6 Conclusions

We have presented a depth-gated RNN architecture. In particular, we have extended LSTM to use the depth gate that modulates a linear dependence of the memory cells in the upper and lower layer recurrent units. We observed better performances using this new model on a machine translation experiment and a language modeling task.

![](/images/8a226d6a503ebea03414cdc80ad6fa09)

## References 

\[1\] G. Hinton, L. Deng, D. Yu, G. Dahl, A. r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, and T. Sainath and. B. Kingsbury, “Deep neural networks for acoustic modeling in speech recognition,” 2012. \[2\] A Krizhevsky, I Sutskever, and G Hinton, “Imagenet classification with deep convolution neural networks,” in NIPS, 2012, vol. 25, pp. 1090–1098. \[3\] T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. Khudanpur, “Recurrent neural network based language model,” in INTERSPEECH, 2010, pp. 1045–1048. \[4\] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Computation, vol. 9, pp. 1735–1780, 1997. \[5\] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in arXiv:1409.0473 \[cs.CL\], 2014. \[6\] I. Sutskever, O. Vinyals, and Q.V. Le, “Sequence to sequence learning with neural networks,” in NIPS, 2014. \[7\] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, “Natural language processing (almost) from scratch,” Journal of Machine Learning Research, vol. 12, pp. 2493–2537, 2011. \[8\] K. Yao, B. Peng, G. Zweig, D. Yu, X. Li, and F. Gao, “Recurrent conditional random field for language understanding,” in ICASSP, 2014. \[9\] A. Graves, “Generating sequences with recurrent neural networks,” in arXiv:1308.0850 \[cs.NE\], 2013. \[10\] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio, “On the properties of neural machine translation,” in arXiv:1409.125, 2014. \[11\] R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio, “How to construct deep recurrent neural networks,” in arXiv:1312.6026 \[cs.NE\], 2013. \[12\] R. K. Srivastava, K. Greff, and Jurgen Schmidhuber, “Highway networks,” in arxiv:1505.00387v1, May 2015.