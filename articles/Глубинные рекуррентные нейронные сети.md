# Глубинные рекуррентные нейронные сети
> Depth-Gated Recurrent Neural Networks

https://arxiv.org/pdf/1508.03790v2.pdf

* Kaisheng Yao Microsoft Research kaisheny@microsoft.edu
* Trevor Cohn University of Melbourne tcohn@unimelb.edu.au
* Katerina Vylomova University of Melbourne
* Kevin Duh Nara Institute of Science and Technology
* Chris Dyer Carnegie Mellon University cdyer@cs.cmu.edu

## Abstract 

В этой короткой заметке мы представляем расширение LSTM для использования шлюза глубины для соединения ячеек памяти смежных слоев. Это вводит линейную зависимость между нижними и верхними рекуррентными единицами. Важно отметить, что линейная зависимость стробируется через стробирующую функцию, которую мы называем ворота-забывания. Эти ворота являются функцией ячейки памяти нижнего уровня, ее входа и ее прошлой памяти. Мы провели эксперименты и убедились, что эта новая архитектура LSTM способна улучшить характеристики машинного перевода и языкового моделирования.

## 1 Introduction 

Deep neural networks (DNNs) have been successfully applied to many areas, including speech \[1\] and vision \[2\]. On natural language processing tasks, recurrent neural networks (RNNs) \[3\] are more widely used because of its ability to memorize long-term dependency. However, simple RNN has problems of gradient diminishing or explosion. Since RNNs can be considered as a deep neural networks across many time instance, the gradients at the end of a sentence may not be able to back-propagated to the beginning of a sentence, because many layers of nonlinear transformation. The long-short-term memory (LSTM) \[4\] neural networks is an extension of simple RNN \[3\]. Instead of using nonlinear connection between the past hidden activity and this layer’s hidden activity, it uses a linear dependence to relate its past memory to the current memory. Importantly, a forget gate is introduced in LSTM to modulate each element of the past memory to be contributed to the current memory cell. LSTMs and its extensions, for instances Gated Recurrent Units \[5\] have been successfully used in many natural language processing tasks \[5\], including machine translation \[5, 6\] and language understanding \[7, 8\]. To construct a deep neural networks, the standard way is to stack many layers of neural networks. This however has the same problem of building simple recurrent networks. The difference here is that the error signals from the top, instead of from last time instance, have to back propagated through many layers of nonlinear transformation and therefore the error signals are either diminished or exploded.
This paper proposes an extension of LSTMs. The key concept is a depth gate that modulates the linear dependence of memory cells in the upper and lower layers.

## 2 Review of recurrent neural networks 
### 2.1 Simple RNN

The simple recurrent neural networks (simple RNN) computes output (y\_{t}) as follows
