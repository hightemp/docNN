# Глубинные рекуррентные нейронные сети
> Depth-Gated Recurrent Neural Networks

https://arxiv.org/pdf/1508.03790v2.pdf

* Kaisheng Yao Microsoft Research kaisheny@microsoft.edu
* Trevor Cohn University of Melbourne tcohn@unimelb.edu.au
* Katerina Vylomova University of Melbourne
* Kevin Duh Nara Institute of Science and Technology
* Chris Dyer Carnegie Mellon University cdyer@cs.cmu.edu

## Abstract 

В этой короткой заметке мы представляем расширение LSTM для использования шлюза глубины для соединения ячеек памяти смежных слоев. Это вводит линейную зависимость между нижними и верхними рекуррентными единицами. Важно отметить, что линейная зависимость стробируется через стробирующую функцию, которую мы называем ворота-забывания. Эти ворота являются функцией ячейки памяти нижнего уровня, ее входа и ее прошлой памяти. Мы провели эксперименты и убедились, что эта новая архитектура LSTM способна улучшить характеристики машинного перевода и языкового моделирования.

## 1 Introduction 

Deep neural networks (DNNs) have been successfully applied to many areas, including speech \[1\] and vision \[2\]. On natural language processing tasks, recurrent neural networks (RNNs) \[3\] are more widely used because of its ability to memorize long-term dependency. However, simple RNN has problems of gradient diminishing or explosion. Since RNNs can be considered as a deep neural networks across many time instance, the gradients at the end of a sentence may not be able to back-propagated to the beginning of a sentence, because many layers of nonlinear transformation. The long-short-term memory (LSTM) \[4\] neural networks is an extension of simple RNN \[3\]. Instead of using nonlinear connection between the past hidden activity and this layer’s hidden activity, it uses a linear dependence to relate its past memory to the current memory. Importantly, a forget gate is introduced in LSTM to modulate each element of the past memory to be contributed to the current memory cell. LSTMs and its extensions, for instances Gated Recurrent Units \[5\] have been successfully used in many natural language processing tasks \[5\], including machine translation \[5, 6\] and language understanding \[7, 8\]. To construct a deep neural networks, the standard way is to stack many layers of neural networks. This however has the same problem of building simple recurrent networks. The difference here is that the error signals from the top, instead of from last time instance, have to back propagated through many layers of nonlinear transformation and therefore the error signals are either diminished or exploded.
This paper proposes an extension of LSTMs. The key concept is a depth gate that modulates the linear dependence of memory cells in the upper and lower layers.

## 2 Review of recurrent neural networks 

### 2.1 Simple RNN

The simple recurrent neural networks (simple RNN) computes output $y\_{t}$ as follows

![](/images/a2cd4f27bf900eb18e992789a4f3e67b)

where $W\_{y}$, $W\_{h}$, and $W\_{x}$ are the matrices for hidden layer output ht, past hidden layer activity $h\_{t−1}$ and the input $x\_{t}$. The time recurrence is introduced in Eq. (2) which relates the current hidden layer activity ht with its past hidden layer activity $h\_{t−1}$. This dependence is nonlinear because of using a logistic function $σ\(·\)$.

### 2.2 Long short-term memory (LSTM)

The simple RNN presented above is hard to train because of gradient diminishing and exploding problems. This is because the nonlinear relation between ht and $h_{t−1}$. LSTM was initially proposed in \[4\] and later modified in \[9\]. We follow the implementation in \[9\]. LSTM introduces a linear dependence between its memory cells ct and its past $c_{t−1}$. Additionally, LSTM has input and output gates. These two gates are applied on a nonlinear function on the input and a nonlinear function on the output from LSTM. Specially, LSTM is written below as

![](/images/c8d0416b9cace473fd931ad0655c3a8a)

where $i_{t}$, $f_{t}$ and $o_{t}$ are the input gate, forget gate and output gate.

### 2.3 Gated Recurrent Unit

A gated recurrent unit (GRU) was proposed in \[10\]. It is similar to LSTM in using gating functions, but differs from LSTM in that it doesn’t have a memory cell. Its operations can be summarized in the following

![](/images/4cb2929981ae19b8910a7c28ea7d69cd)

where the output from GRU is $h_{t}$. $z_{t}$ and $r_{t}$ are the update gate and reset gate. h˜ t is the candidate output. Wz, Wh, Wr, Uz, and Ur are the matrices in GRU.
