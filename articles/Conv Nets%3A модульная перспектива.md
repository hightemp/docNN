# Conv Nets: модульная перспектива
## Вступление

В последние несколько лет глубокие нейронные сети привели к прорывным результатам по ряду проблем распознавания образов, таких как компьютерное зрение и распознавание голоса. Одним из важных компонентов, приводящих к этим результатам, была особая разновидность нейронной сети, называемая _конволюционная нейронная сеть_.

По своей сути, сверточные нейронные сети можно рассматривать как разновидность нейронной сети, которая использует много идентичных копий одного и того же нейрона. [1](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fn1) Это позволяет сети иметь много нейронов и выражать вычислительно большие модели, сохраняя при этом количество фактических параметров - значения, описывающие, как ведут себя нейроны, - которые нужно выучить довольно мало.

 ![](/images/5bab67340789f49120933392e45e3c0f.png) 
 2D сверточная нейронная сеть

Этот трюк с несколькими копиями одного и того же нейрона примерно аналогичен абстракции функций в математике и информатике. При программировании мы пишем функцию один раз и используем ее во многих местах - если не писать один и тот же код сто раз в разных местах, это ускоряет программирование и приводит к меньшему количеству ошибок. Точно так же сверточная нейронная сеть может однажды выучить нейрон и использовать его во многих местах, упрощая изучение модели и уменьшая количество ошибок.

## Структура сверточных нейронных сетей

Предположим, вы хотите, чтобы нейронная сеть смотрела на аудиосэмплы и предсказывала, говорит человек или нет. Может быть, вы хотите сделать больше анализа, если кто-то говорит.

Вы получаете аудио образцы в разные моменты времени. Образцы расположены равномерно.

 ![](/images/3e84b8472f1a16756cadfd92cc6a1b53.png) 

Самый простой способ попытаться классифицировать их с помощью нейронной сети - просто подключить их все к полностью подключенному слою. Существует множество разных нейронов, и каждый вход соединяется с каждым нейроном.

 ![](/images/5eb9f54084c0376a1d31a82bc8408e55.png) 

Более сложный подход обращает внимание на своего рода _симметрию_ в свойствах, которые полезно искать в данных. Мы очень заботимся о локальных свойствах данных: какая частота звуков присутствует в данное время? Они увеличиваются или уменьшаются? И так далее.

Мы заботимся об одинаковых свойствах во все моменты времени. Полезно знать частоты в начале, полезно знать частоты в середине, а также полезно знать частоты в конце. Опять же, обратите внимание, что это локальные свойства, так как нам нужно только взглянуть на небольшое окно аудиосэмпла, чтобы определить их.

Таким образом, мы можем создать группу нейронов, \\(A\\), которые смотрят на небольшие временные сегменты наших данных. [2](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fn2) \\(A\\) просматривает все такие сегменты, вычисляя определенные _функции_. Затем выход этого _сверточного слоя_ подается в полностью связанный слой \\(F\\).

 ![](/images/fbcd03e2376115c47ca4807efc8a0692.png) 

В приведенном выше примере \\(A\\) просматривал только сегменты, состоящие из двух точек. Это нереально. Обычно окно сверточного слоя будет намного больше.

В следующем примере \\(A\\) смотрит на 3 пункта. Это также нереально - к сожалению, сложно представить, как можно соединиться со многими точками.

 ![](/images/8edca52d8fb6135ad47059206dfd2360.png) 

Одним из очень хороших свойств сверточных слоев является то, что они являются композитными. Вы можете передать вывод одного сверточного слоя в другой. На каждом уровне сеть может обнаруживать более абстрактные и высокоуровневые функции.

В следующем примере у нас есть новая группа нейронов, \\(B\\). \\(B\\) используется для создания другого сверточного слоя, наложенного поверх предыдущего.

 ![](/images/3fd1a47ec0ce231670d88ce1d4beb9af.png) 

Сверточные слои часто переплетаются с объединяющимися слоями. В частности, существует своего рода слой, называемый слоем max-pooling, который очень популярен.

Часто, с точки зрения высокого уровня, мы не заботимся о точном моменте времени, когда функция присутствует. Если сдвиг частоты происходит немного раньше или позже, имеет ли это значение?

Слой с максимальным пулом берет максимум функций по сравнению с небольшими блоками предыдущего слоя. Вывод сообщает нам, присутствовал ли объект в области предыдущего слоя, но не точно где.

Слои Max-Pooling как бы уменьшают масштаб. Они позволяют более поздним сверточным слоям работать с большими разделами данных, потому что небольшой патч после пула соответствует гораздо большему патчу перед ним. Они также делают нас инвариантными к некоторым очень маленьким преобразованиям данных.

 ![](/images/1174702401de977b1ddb5a6c684cb49b.png) 

 ![](/images/ca26196a50220824f0fdf928f7b424a8.png) 

В наших предыдущих примерах мы использовали одномерные сверточные слои. Однако сверточные слои также могут работать с многомерными данными. Фактически, самые известные успехи сверточных нейронных сетей - это применение двумерных сверточных нейронных сетей для распознавания изображений.

В двумерном сверточном слое вместо просмотра сегментов \\(A\\) теперь будет смотреть на патчи.

Для каждого патча \\(A\\) будет вычислять функции. Например, он может научиться обнаруживать наличие ребра. Или это может научиться обнаруживать текстуру. Или, может быть, контраст между двумя цветами.

 ![](/images/99e34372a582842947dd5437cc46c26d.png) 

В предыдущем примере мы передавали вывод нашего сверточного слоя в полностью связанный слой. Но мы также можем составить два сверточных слоя, как мы это делали в одномерном случае.

 ![](/images/5bab67340789f49120933392e45e3c0f.png) 

Мы также можем сделать максимальное объединение в двух измерениях. Здесь мы берем максимум возможностей за небольшой патч.

На самом деле это сводится к тому, что при рассмотрении всего изображения мы не заботимся о точном положении края, вплоть до пикселя. Достаточно знать, где он находится с точностью до нескольких пикселей.

 ![](/images/9d85a42c7a864c0d810302b7dac8124e.png) 

Трехмерные сверточные сети также иногда используются для данных, таких как видео или объемные данные (например, трехмерные медицинские сканы). Однако они не очень широко используются, и их гораздо сложнее визуализировать.

Теперь мы ранее говорили, что \\(A\\) была группа нейронов. Мы должны быть немного более точными в этом: что такое \\(A\\) точно?

В традиционных сверточных слоях \\(A\\) - это параллельная группа нейронов, которые все получают одинаковые входные данные и вычисляют различные функции.

Например, в двумерном сверточном слое один нейрон может обнаруживать горизонтальные края, другой - вертикальные, а другой - зелено-красные контрасты.

 ![](/images/7efaf64b70dca12e35445b466db5ff1e.png) 

Тем не менее, в недавней статье «Сеть в сети» ([Lin _et al._ (2013)](http://arxiv.org/abs/1312.4400)) предложен новый слой «Mlpconv». , В этой модели \\(A\\) будет иметь несколько слоев нейронов, а последний слой будет выводить объекты более высокого уровня для региона. В статье модель достигает очень впечатляющих результатов, устанавливая новый уровень техники в ряде эталонных наборов данных.

 ![](/images/2491522bf3ad43cc76f9e04bff0101e5.png) 

Тем не менее, для целей этого поста мы сосредоточимся на стандартных сверточных слоях. Нам уже достаточно рассмотреть это!

## Результаты сверточных нейронных сетей

Ранее мы ссылались на недавние достижения в области компьютерного зрения с использованием сверточных нейронных сетей. Прежде чем мы продолжим, я хотел бы кратко обсудить некоторые из этих результатов в качестве мотивации.

В 2012 году Алекс Крижевский, Илья Суцкевер и Джефф Хинтон взорвали существующие результаты классификации изображений из воды ( [Krizehvsky _et al._ (2012)](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) ).

Их прогресс был результатом объединения кучки разных частей. Они использовали графические процессоры для обучения очень большой, глубокой, нейронной сети. Они использовали новый тип нейрона (ReLU) и новую технику, чтобы уменьшить проблему, называемую «переоснащение» (DropOut). Они использовали очень большой набор данных с большим количеством категорий изображений ( [ImageNet](http://www.image-net.org/) ). И, конечно, это была сверточная нейронная сеть.

Их архитектура, показанная ниже, была очень глубокой. Он имеет 5 сверточных слоев, [3](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fn3) с объединенным пулом и три полностью связанных слоя. Ранние уровни разделены на два графических процессора.
 
 ![](/images/05bc84347432d78ea35d31d653556023.png) 
 из [Krizehvsky _et al._ (2012)](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) 

Они обучили свою сеть классифицировать изображения на тысячу разных категорий.

Случайно угадывая, можно угадать правильный ответ в 0,1% случаев. Модель Крижевского _et al._ может дать правильный ответ в 63% случаев. Кроме того, один из 5 лучших ответов, которые он дает, правильный в 85% случаев!

 ![](/images/88cd1e0db30c06fb510b8108f8dbbba6.png) 
 Вверху: 4 правильно классифицированных примера. Внизу: 4 неправильно классифицированных примера. У каждого примера есть изображение, сопровождаемое его меткой, за которой следуют 5 главных догадок с вероятностями. 
 Из [Krizehvsky _et al._ (2012)](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf).

Даже некоторые из его ошибок кажутся мне вполне разумными!

Мы также можем изучить то, чему научится делать первый уровень сети.

Напомним, что сверточные слои были разделены между двумя графическими процессорами. Информация не перемещается назад и вперед по каждому слою, поэтому разделенные стороны отсоединяются по-настоящему. Оказывается, что каждый раз, когда модель запускается, обе стороны специализируются.

 ![](/images/7b7d120378c1c9ce9540c1c16d456ff3.png) 
 Фильтры учились по первому сверточному слою. Верхняя половина соответствует слою на одном графическом процессоре, нижняя - на другом. Из [Krizehvsky _et al._ (2012)](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) 

Нейроны с одной стороны фокусируются на черно-белом изображении, обучаясь распознавать края разной ориентации и размеров. Нейроны с другой стороны специализируются на цвете и текстуре, обнаруживая цветовые контрасты и узоры. [4](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fn4) Помните, что нейроны _cлучайно_ инициализированы. Никто из людей не пошел и не установил их в качестве детекторов краев или для разделения таким образом. Это возникло просто из-за обучения сети классификации изображений.

Эти замечательные результаты (и другие захватывающие результаты того времени) были только началом. За ними быстро последовало множество других работ, тестирующих модифицированные подходы и постепенно улучшающих результаты, или применяя их в других областях. И, в дополнение к сообществу нейронных сетей, многие в сообществе компьютерного зрения приняли глубокие сверточные нейронные сети.

Сверточные нейронные сети являются важным инструментом в компьютерном зрении и современном распознавании образов.

## Формализация сверточных нейронных сетей

Рассмотрим одномерный сверточный слой с входами \\(\\{x\_n\\}\\) и выходами \\(\\{y\_n \\}\\):

 ![](/images/7be8a90f11666f023ad5fcc55d2a92e7.png) 

Относительно легко описать результаты в терминах входных данных:

 \\\[y\_n = A(x\_{n}, x\_{n+1}, ...)\\\] 

Например, в приведенном выше:

 \\\[y\_0 = A(x\_0, x\_1)\\\]  \\\[y\_1 = A(x\_1, x\_2)\\\] 

Аналогичным образом, если мы рассмотрим двумерный сверточный слой, с входами \\(\\{x\_{n, m}\\}\\) и выходами \\(\\{y\_{n, m}\\}\\):

 ![](/images/e800550c411136d69f5266e16559a2f5.png) 

Мы можем снова записать результаты в терминах входных данных:

  \\\[y\_{n,m} = A\\left(\\begin{array}{ccc} x\_{n,\~m}, & x\_{n+1,\~m},& ...,\~\\\\ x\_{n,\~m+1}, & x\_{n+1,\~m+1}, & ..., \~\\\\ &...\\\\\\end{array}\\right)\\\] 

Например:

  \\\[y\_{0,0} = A\\left(\\begin{array}{cc} x\_{0,\~0}, & x\_{1,\~0},\~\\\\ x\_{0,\~1}, & x\_{1,\~1}\~\\\\\\end{array}\\right)\\\]  \\\[y\_{1,0} = A\\left(\\begin{array}{cc} x\_{1,\~0}, & x\_{2,\~0},\~\\\\ x\_{1,\~1}, & x\_{2,\~1}\~\\\\\\end{array}\\right)\\\] 

Если объединить это с уравнением для \\(A(x)\\),

 \\\[A(x) = \\sigma(Wx + b)\\\] 

у каждого есть все, что нужно для реализации сверточной нейронной сети, по крайней мере, в теории.

На практике это часто не лучший способ думать о сверточных нейронных сетях. Существует альтернативная формулировка, в терминах математической операции, называемой _convolution_, которая часто более полезна.

Операция свертки является мощным инструментом. В математике это встречается в различных контекстах, начиная от изучения уравнений с частными производными до теории вероятностей. Частично из-за своей роли в PDE, свертка очень важна в физических науках. Он также играет важную роль во многих прикладных областях, таких как компьютерная графика и обработка сигналов.

Для нас свертка обеспечит ряд преимуществ. Во-первых, это позволит нам создать гораздо более эффективные реализации сверточных слоев, чем наивная перспектива. Во-вторых, это уберет много беспорядка из нашей формулировки, обрабатывая всю бухгалтерию, которая в настоящее время обнаруживается при индексировании \\(x\\) s, - настоящая формулировка может пока не показаться грязной, но это только потому что мы еще не попали в сложные элементы. Наконец, свертка даст нам существенно другую перспективу для рассуждений о сверточных слоях.

> Я восхищаюсь элегантностью вашего метода вычислений; должно быть, приятно ехать через эти поля на лошади настоящей математики, в то время как нам, как нам, приходится кропотливо идти пешком. - Альберт Эйнштейн

## Следующие сообщения в этой серии

 [ **Прочитайте следующий пост!** ](https://colah.github.io/posts/2014-07-Understanding-Convolutions/) 

Этот пост является частью серии о сверточных нейронных сетях и их обобщениях. Первые два поста будут рассмотрены для тех, кто знаком с глубоким обучением, а последующие должны быть интересны всем. Чтобы получать обновления, подпишитесь на мой [RSS-канал](https://colah.github.io/rss.xml) !

Пожалуйста, прокомментируйте ниже или на стороне. Запросы на извлечение могут быть сделаны на [github](https://github.com/colah/Conv-Nets-Series) .

## Благодарности

Я благодарен Элиане Лорч, Аарону Курвиллу и Себастьяну Зани за их комментарии и поддержку.

* * *

1. Следует отметить, что не все нейронные сети, которые используют несколько копий одного и того же нейрона, являются сверточными нейронными сетями. Сверточные нейронные сети - это всего лишь один тип нейронных сетей, использующий более общий прием - «весовая привязка». Другие виды нейронных сетей, которые делают это, являются рекуррентными нейронными сетями и рекурсивными нейронными сетями. [↩](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fnref1) 
    
2.  Группы нейронов, такие как \\(A\\), которые появляются в нескольких местах, иногда называют _modules_, а сети, которые их используют, иногда называют _modular neural networks_. [↩](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fnref2) 
    
3.  Они также проверяют, используя 7 в статье. [↩](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fnref3) 
    
4.  Это, кажется, имеет интересные аналогии с палочками и колбочками в сетчатке. [↩](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fnref4)



**********
[Свёрточная нейронная сеть](/tags/%D0%A1%D0%B2%D1%91%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%B0%D1%8F%20%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F%20%D1%81%D0%B5%D1%82%D1%8C.md)
[CNN](/tags/CNN.md)
[Convolutional Neural Networks](/tags/Convolutional%20Neural%20Networks.md)
