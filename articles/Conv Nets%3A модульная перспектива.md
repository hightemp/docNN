# Conv Nets: модульная перспектива
## Вступление

В последние несколько лет глубокие нейронные сети привели к прорывным результатам по ряду проблем распознавания образов, таких как компьютерное зрение и распознавание голоса. Одним из важных компонентов, приводящих к этим результатам, была особая разновидность нейронной сети, называемая _конволюционная нейронная сеть_.

По своей сути, сверточные нейронные сети можно рассматривать как разновидность нейронной сети, которая использует много идентичных копий одного и того же нейрона. [1](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fn1) Это позволяет сети иметь много нейронов и выражать вычислительно большие модели, сохраняя при этом количество фактических параметров - значения, описывающие, как ведут себя нейроны, - которые нужно выучить довольно мало.

 ![](/images/5bab67340789f49120933392e45e3c0f.png) 
 2D сверточная нейронная сеть

Этот трюк с несколькими копиями одного и того же нейрона примерно аналогичен абстракции функций в математике и информатике. При программировании мы пишем функцию один раз и используем ее во многих местах - если не писать один и тот же код сто раз в разных местах, это ускоряет программирование и приводит к меньшему количеству ошибок. Точно так же сверточная нейронная сеть может однажды выучить нейрон и использовать его во многих местах, упрощая изучение модели и уменьшая количество ошибок.

## Структура сверточных нейронных сетей

Предположим, вы хотите, чтобы нейронная сеть смотрела на аудиосэмплы и предсказывала, говорит человек или нет. Может быть, вы хотите сделать больше анализа, если кто-то говорит.

Вы получаете аудио образцы в разные моменты времени. Образцы расположены равномерно.

 ![](/images/3e84b8472f1a16756cadfd92cc6a1b53.png) 

Самый простой способ попытаться классифицировать их с помощью нейронной сети - просто подключить их все к полностью подключенному слою. Существует множество разных нейронов, и каждый вход соединяется с каждым нейроном.

 ![](/images/5eb9f54084c0376a1d31a82bc8408e55.png) 

Более сложный подход обращает внимание на своего рода _симметрию_ в свойствах, которые полезно искать в данных. Мы очень заботимся о локальных свойствах данных: какая частота звуков присутствует в данное время? Они увеличиваются или уменьшаются? И так далее.

Мы заботимся об одинаковых свойствах во все моменты времени. Полезно знать частоты в начале, полезно знать частоты в середине, а также полезно знать частоты в конце. Опять же, обратите внимание, что это локальные свойства, так как нам нужно только взглянуть на небольшое окно аудиосэмпла, чтобы определить их.

Таким образом, мы можем создать группу нейронов, \\(A\\), которые смотрят на небольшие временные сегменты наших данных. [2](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fn2) \\(A\\) просматривает все такие сегменты, вычисляя определенные _функции_. Затем выход этого _сверточного слоя_ подается в полностью связанный слой \\(F\\).

 ![](/images/fbcd03e2376115c47ca4807efc8a0692.png) 

В приведенном выше примере \\(A\\) просматривал только сегменты, состоящие из двух точек. Это нереально. Обычно окно сверточного слоя будет намного больше.

В следующем примере \\(A\\) смотрит на 3 пункта. Это также нереально - к сожалению, сложно представить, как можно соединиться со многими точками.

 ![](/images/8edca52d8fb6135ad47059206dfd2360.png) 

Одним из очень хороших свойств сверточных слоев является то, что они являются композитными. Вы можете передать вывод одного сверточного слоя в другой. На каждом уровне сеть может обнаруживать более абстрактные и высокоуровневые функции.

В следующем примере у нас есть новая группа нейронов, \\(B\\). \\(B\\) используется для создания другого сверточного слоя, наложенного поверх предыдущего.

 ![](/images/3fd1a47ec0ce231670d88ce1d4beb9af.png) 

Сверточные слои часто переплетаются с объединяющимися слоями. В частности, существует своего рода слой, называемый слоем max-pooling, который очень популярен.

Часто, с точки зрения высокого уровня, мы не заботимся о точном моменте времени, когда функция присутствует. Если сдвиг частоты происходит немного раньше или позже, имеет ли это значение?

Слой с максимальным пулом берет максимум функций по сравнению с небольшими блоками предыдущего слоя. Вывод сообщает нам, присутствовал ли объект в области предыдущего слоя, но не точно где.

Слои Max-Pooling как бы уменьшают масштаб. Они позволяют более поздним сверточным слоям работать с большими разделами данных, потому что небольшой патч после пула соответствует гораздо большему патчу перед ним. Они также делают нас инвариантными к некоторым очень маленьким преобразованиям данных.

 ![](/images/1174702401de977b1ddb5a6c684cb49b.png) 

 ![](/images/ca26196a50220824f0fdf928f7b424a8.png) 

В наших предыдущих примерах мы использовали одномерные сверточные слои. Однако сверточные слои также могут работать с многомерными данными. Фактически, самые известные успехи сверточных нейронных сетей - это применение двумерных сверточных нейронных сетей для распознавания изображений.

В двумерном сверточном слое вместо просмотра сегментов \\(A\\) теперь будет смотреть на патчи.

Для каждого патча \\(A\\) будет вычислять функции. Например, он может научиться обнаруживать наличие ребра. Или это может научиться обнаруживать текстуру. Или, может быть, контраст между двумя цветами.

 ![](/images/99e34372a582842947dd5437cc46c26d.png) 

В предыдущем примере мы передавали вывод нашего сверточного слоя в полностью связанный слой. Но мы также можем составить два сверточных слоя, как мы это делали в одномерном случае.

 ![](/images/5bab67340789f49120933392e45e3c0f.png) 

Мы также можем сделать максимальное объединение в двух измерениях. Здесь мы берем максимум возможностей за небольшой патч.

На самом деле это сводится к тому, что при рассмотрении всего изображения мы не заботимся о точном положении края, вплоть до пикселя. Достаточно знать, где он находится с точностью до нескольких пикселей.

 ![](/images/9d85a42c7a864c0d810302b7dac8124e.png) 

Трехмерные сверточные сети также иногда используются для данных, таких как видео или объемные данные (например, трехмерные медицинские сканы). Однако они не очень широко используются, и их гораздо сложнее визуализировать.

Теперь мы ранее говорили, что \\(A\\) была группа нейронов. Мы должны быть немного более точными в этом: что такое \\(A\\) точно?

В традиционных сверточных слоях \\(A\\) - это параллельная группа нейронов, которые все получают одинаковые входные данные и вычисляют различные функции.

Например, в двумерном сверточном слое один нейрон может обнаруживать горизонтальные края, другой - вертикальные, а другой - зелено-красные контрасты.

 ![](/images/7efaf64b70dca12e35445b466db5ff1e.png) 

Тем не менее, в недавней статье «Сеть в сети» ([Lin _et al._ (2013)](http://arxiv.org/abs/1312.4400)) предложен новый слой «Mlpconv». , В этой модели \\(A\\) будет иметь несколько слоев нейронов, а последний слой будет выводить объекты более высокого уровня для региона. В статье модель достигает очень впечатляющих результатов, устанавливая новый уровень техники в ряде эталонных наборов данных.

 ![](/images/2491522bf3ad43cc76f9e04bff0101e5.png) 

Тем не менее, для целей этого поста мы сосредоточимся на стандартных сверточных слоях. Нам уже достаточно рассмотреть это!

## Результаты сверточных нейронных сетей

Ранее мы ссылались на недавние достижения в области компьютерного зрения с использованием сверточных нейронных сетей. Прежде чем мы продолжим, я хотел бы кратко обсудить некоторые из этих результатов в качестве мотивации.

В 2012 году Алекс Крижевский, Илья Суцкевер и Джефф Хинтон взорвали существующие результаты классификации изображений из воды ( [Krizehvsky _et al._ (2012)](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) ).

Их прогресс был результатом объединения кучки разных частей. Они использовали графические процессоры для обучения очень большой, глубокой, нейронной сети. Они использовали новый тип нейрона (ReLU) и новую технику, чтобы уменьшить проблему, называемую «переоснащение» (DropOut). Они использовали очень большой набор данных с большим количеством категорий изображений ( [ImageNet](http://www.image-net.org/) ). И, конечно, это была сверточная нейронная сеть.

Их архитектура, показанная ниже, была очень глубокой. Он имеет 5 сверточных слоев, [3](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fn3) с объединенным пулом и три полностью связанных слоя. Ранние уровни разделены на два графических процессора.
 
 ![](/images/05bc84347432d78ea35d31d653556023.png) 
 из [Krizehvsky _et al._ (2012)](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) 

Они обучили свою сеть классифицировать изображения на тысячу разных категорий.

Случайно угадывая, можно угадать правильный ответ в 0,1% случаев. Модель Крижевского _et al._ может дать правильный ответ в 63% случаев. Кроме того, один из 5 лучших ответов, которые он дает, правильный в 85% случаев!

 ![](/images/88cd1e0db30c06fb510b8108f8dbbba6.png) 
 Вверху: 4 правильно классифицированных примера. Внизу: 4 неправильно классифицированных примера. У каждого примера есть изображение, сопровождаемое его меткой, за которой следуют 5 главных догадок с вероятностями. 
 Из [Krizehvsky _et al._ (2012)](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf).

Даже некоторые из его ошибок кажутся мне вполне разумными!

Мы также можем изучить то, чему научится делать первый уровень сети.

Напомним, что сверточные слои были разделены между двумя графическими процессорами. Информация не перемещается назад и вперед по каждому слою, поэтому разделенные стороны отсоединяются по-настоящему. Оказывается, что каждый раз, когда модель запускается, обе стороны специализируются.

 ![](/images/7b7d120378c1c9ce9540c1c16d456ff3.png) 
 Фильтры учились по первому сверточному слою. Верхняя половина соответствует слою на одном графическом процессоре, нижняя - на другом. Из [Krizehvsky _et al._ (2012)](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) 

Нейроны с одной стороны фокусируются на черно-белом изображении, обучаясь распознавать края разной ориентации и размеров. Нейроны с другой стороны специализируются на цвете и текстуре, обнаруживая цветовые контрасты и узоры. [4](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fn4) Помните, что нейроны _cлучайно_ инициализированы. Никто из людей не пошел и не установил их в качестве детекторов краев или для разделения таким образом. Это возникло просто из-за обучения сети классификации изображений.

Эти замечательные результаты (и другие захватывающие результаты того времени) были только началом. За ними быстро последовало множество других работ, тестирующих модифицированные подходы и постепенно улучшающих результаты, или применяя их в других областях. И, в дополнение к сообществу нейронных сетей, многие в сообществе компьютерного зрения приняли глубокие сверточные нейронные сети.

Сверточные нейронные сети являются важным инструментом в компьютерном зрении и современном распознавании образов.

## Формализация сверточных нейронных сетей

Рассмотрим одномерный сверточный слой с входами \\(\\{x\_n\\}\\) и выходами \\(\\{y\_n \\}\\):

 ![](/images/7be8a90f11666f023ad5fcc55d2a92e7.png) 

Относительно легко описать результаты в терминах входных данных:

 \\\[y\_n = A(x\_{n}, x\_{n+1}, ...)\\\] 

Например, в приведенном выше:

 \\\[y\_0 = A(x\_0, x\_1)\\\]  \\\[y\_1 = A(x\_1, x\_2)\\\] 

Аналогичным образом, если мы рассмотрим двумерный сверточный слой, с входами \\(\\{x\_{n, m}\\}\\) и выходами \\(\\{y\_{n, m}\\}\\):

 ![](/images/e800550c411136d69f5266e16559a2f5.png) 

We can, again, write down the outputs in terms of the inputs:

  \\\[y\_{n,m} = A\\left(\\begin{array}{ccc} x\_{n,\~m}, & x\_{n+1,\~m},& ...,\~\\\\ x\_{n,\~m+1}, & x\_{n+1,\~m+1}, & ..., \~\\\\ &...\\\\\\end{array}\\right)\\\] 

For example:

  \\\[y\_{0,0} = A\\left(\\begin{array}{cc} x\_{0,\~0}, & x\_{1,\~0},\~\\\\ x\_{0,\~1}, & x\_{1,\~1}\~\\\\\\end{array}\\right)\\\]  \\\[y\_{1,0} = A\\left(\\begin{array}{cc} x\_{1,\~0}, & x\_{2,\~0},\~\\\\ x\_{1,\~1}, & x\_{2,\~1}\~\\\\\\end{array}\\right)\\\] 

If one combines this with the equation for \\(A(x)\\) ,

 \\\[A(x) = \\sigma(Wx + b)\\\] 

one has everything they need to implement a convolutional neural network, at least in theory.

In practice, this is often not best way to think about convolutional neural networks. There is an alternative formulation, in terms of a mathematical operation called _convolution_ , that is often more helpful.

The convolution operation is a powerful tool. In mathematics, it comes up in diverse contexts, ranging from the study of partial differential equations to probability theory. In part because of its role in PDEs, convolution is very important in the physical sciences. It also has an important role in many applied areas, like computer graphics and signal processing.

For us, convolution will provide a number of benefits. Firstly, it will allow us to create much more efficient implementations of convolutional layers than the naive perspective might suggest. Secondly, it will remove a lot of messiness from our formulation, handling all the bookkeeping presently showing up in the indexing of \\(x\\) s – the present formulation may not seem messy yet, but that’s only because we haven’t got into the tricky cases yet. Finally, convolution will give us a significantly different perspective for reasoning about convolutional layers.

> I admire the elegance of your method of computation; it must be nice to ride through these fields upon the horse of true mathematics while the like of us have to make our way laboriously on foot.  — Albert Einstein

## Next Posts in this Series

 [ **Read the next post!** ](https://colah.github.io/posts/2014-07-Understanding-Convolutions/) 

This post is part of a series on convolutional neural networks and their generalizations. The first two posts will be review for those familiar with deep learning, while later ones should be of interest to everyone. To get updates, subscribe to my [RSS feed](https://colah.github.io/rss.xml) !

Please comment below or on the side. Pull requests can be made on [github](https://github.com/colah/Conv-Nets-Series) .

## Acknowledgments

I’m grateful to Eliana Lorch, Aaron Courville, and Sebastian Zany for their comments and support.

* * *

1.  It should be noted that not all neural networks that use multiple copies of the same neuron are convolutional neural networks. Convolutional neural networks are just one type of neural network that uses the more general trick, _weight-tying_ . Other kinds of neural network that do this are recurrent neural networks and recursive neural networks. [↩](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fnref1) 
    
2.  Groups of neurons, like \\(A\\) , that appear in multiple places are sometimes called _modules_ , and networks that use them are sometimes called _modular neural networks_ . [↩](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fnref2) 
    
3.  They also test using 7 in the paper. [↩](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fnref3) 
    
4.  This seems to have interesting analogies to rods and cones in the retina. [↩](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/#fnref4)



**********
[Свёрточная нейронная сеть](/tags/%D0%A1%D0%B2%D1%91%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%B0%D1%8F%20%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F%20%D1%81%D0%B5%D1%82%D1%8C.md)
[CNN](/tags/CNN.md)
[Convolutional Neural Networks](/tags/Convolutional%20Neural%20Networks.md)
