# Какие графические процессоры получить для глубокого обучения: мой опыт и советы по использованию графических процессоров для глубокого обучения

Глубокое обучение - это область с высокими вычислительными требованиями, и выбор вашего графического процессора будет в основном определять ваш опыт глубокого обучения. Но какие функции важны, если вы хотите купить новый графический процессор? ОЗУ ГП, ядра, тензорные ядра? Как сделать экономичный выбор? Этот пост будет посвящен этим вопросам и даст вам совет, который поможет вам сделать правильный выбор.  
 [TL;DR](https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/#tldr)   
Наличие быстрого GPU является очень важным аспектом, когда человек начинает изучать глубокое обучение, поскольку это позволяет быстро получить практический опыт, который является ключом к накоплению опыта, с помощью которого вы сможете применить глубокое обучение к новым задачам. Без этой быстрой обратной связи просто требуется слишком много времени, чтобы учиться на своих ошибках, и это может быть обескураживающим и разочаровывающим, чтобы продолжить глубокое обучение. С помощью графических процессоров я быстро научился применять глубокое обучение на ряде соревнований Kaggle, и мне удалось заработать второе место в конкурсе Partly Sunny с шансом Hashtags Kaggle, используя [метод глубокого обучения](https://www.kaggle.com/c/crowdflower-weather-twitter/forums/t/6488/congratulations/35640#post35640), где была задача предсказать рейтинги погоды для данного твита. В конкурсе я использовал довольно большую двухслойную глубокую нейронную сеть с выпрямленными линейными единицами и выпадением для регуляризации, и эта глубокая сеть едва помещалась в мою 6 ГБ память GPU. Графические процессоры GTX Titan, которые поддерживали меня в соревновании, были основным фактором, по которому я занял 2 место в соревновании.

## Обзор

Этот пост блога структурирован следующим образом. Сначала я расскажу о том, насколько полезно иметь несколько графических процессоров, затем я расскажу обо всех соответствующих аппаратных опциях, таких как графические процессоры NVIDIA и AMD, Intel Xeon Phis, Google TPU и новое загрузочное оборудование. Затем я расскажу, какие спецификации графических процессоров являются хорошими показателями для глубокого обучения Основная часть посвящена анализу эффективности и эффективности затрат. Я заканчиваю общими и более конкретными рекомендациями GPU.

## Несколько графических процессоров делают мои тренировки быстрее?

Когда я начал использовать несколько графических процессоров, я был рад использовать параллелизм данных для повышения производительности во время соревнования Kaggle. Однако я обнаружил, что очень сложно добиться прямого ускорения с помощью нескольких графических процессоров. Мне было любопытно об этой проблеме, и поэтому я начал проводить исследование параллелизма в глубоком обучении. Я проанализировал распараллеливание в архитектурах глубокого обучения, разработал [метод 8-разрядного квантования](https://arxiv.org/abs/1511.04561), чтобы увеличить ускорения в кластерах графических процессоров с 23x до 50x для системы из 96 графических процессоров, и опубликовал свой исследование в ICLR 2016.

Основная идея заключалась в том, что сверточные и рекуррентные сети довольно легко распараллелить, особенно если вы используете только один компьютер или 4 графических процессора. Тем не менее, полностью подключенные сети, включая трансформаторы, не так просто распараллелить и нуждаются в специализированных алгоритмах для хорошей работы.

 [ ![GPU pic](/images/911d5bf5696df767f09d580412e115eb) ](https://i1.wp.com/timdettmers.com/wp-content/uploads/2014/08/gpu-pic.jpg) 

Рисунок 1: Настройка на моем главном компьютере: вы видите три графических процессора и карту InfiniBand. Это хорошая установка для глубокого обучения?

Современные библиотеки, такие как TensorFlow и PyTorch, отлично подходят для распараллеливания рекуррентных и сверточных сетей, а для свертки можно ожидать ускорения примерно 1,9x / 2,8x / 3,5x для 2/3/4 графических процессоров. Для рекуррентных сетей длина последовательности является наиболее важным параметром, а для распространенных проблем НЛП можно ожидать аналогичного или несколько худшего ускорения по сравнению с сверточными сетями. Однако полностью подключенные сети, включая трансформаторы, обычно имеют низкую производительность для параллелизма данных, и для ускорения этих частей сети необходимы более совершенные алгоритмы. Если вы запускаете преобразователи на нескольких графических процессорах, попробуйте запустить их на 1 графическом процессоре и посмотреть, работает ли он быстрее или нет.

## Использование нескольких графических процессоров без параллелизма

Другое преимущество использования нескольких графических процессоров, даже если вы не распараллеливаете алгоритмы, состоит в том, что вы можете запускать несколько алгоритмов или экспериментов отдельно на каждом графическом процессоре. Эффективный поиск гиперпараметров является наиболее распространенным использованием нескольких графических процессоров. Вы не получаете ускорений, но быстрее получаете информацию о производительности различных настроек гиперпараметра или другой сетевой архитектуры. Это также очень полезно для новичков, так как вы можете быстро получить представление и узнать, как вы можете обучить незнакомую архитектуру глубокого обучения.

Использование нескольких графических процессоров таким способом обычно более полезно, чем запуск одной сети на нескольких графических процессорах через параллелизм данных. Следует помнить об этом при покупке нескольких графических процессоров: качества для лучшего параллелизма, такие как количество линий PCIe, не так важны при покупке нескольких графических процессоров.

Кроме того, обратите внимание, что одного GPU должно хватить практически для любой задачи. Таким образом, диапазон, который вы можете получить с 1 GPU, не будет отличаться от того, когда у вас 4 GPU. Единственное отличие состоит в том, что вы можете проводить больше экспериментов за определенное время с несколькими графическими процессорами.

## Ваши варианты: NVIDIA против AMD против Intel против Google против Amazon против Microsoft против необычного стартапа

### NVIDIA: лидер

Стандартные библиотеки NVIDIA упростили создание первых библиотек глубокого обучения в CUDA, хотя таких мощных стандартных библиотек для OpenCL от AMD не было. Это раннее преимущество в сочетании с сильной поддержкой сообщества со стороны NVIDIA быстро увеличило размер сообщества CUDA. Это означает, что если вы используете графические процессоры NVIDIA, вы легко найдете поддержку, если что-то пойдет не так, вы найдете поддержку и совет, если будете самостоятельно программировать CUDA, и обнаружите, что большинство библиотек с углубленным изучением имеют наилучшую поддержку графических процессоров NVIDIA. В последние месяцы NVIDIA все еще вкладывала в [программное обеспечение](https://devblogs.nvidia.com/new-optimizations-accelerate-deep-learning-training-gpu/) еще больше ресурсов . Например, [библиотека Apex](https://github.com/NVIDIA/apex) предлагает поддержку для стабилизации 16-разрядных градиентов в PyTorch, а также включает быстрые оптимизаторы с плавкими переходами, такие как FusedAdam. В целом, программное обеспечение является сильной стороной графических процессоров NVIDIA.

С другой стороны, теперь у NVIDIA есть политика, согласно которой использование CUDA в центрах обработки данных разрешено только для графических процессоров Tesla, но не для карт GTX или RTX. Непонятно, что подразумевается под «центрами данных», но это означает, что организации и университеты часто вынуждены покупать дорогие и неэффективные с точки зрения затрат графические процессоры Tesla из-за страха перед юридическими проблемами. Однако карты Tesla не имеют реального преимущества перед картами GTX и RTX и стоят в 10 раз дороже.

То, что NVIDIA может сделать это без каких-либо серьезных препятствий, демонстрирует силу их монополии - они могут делать все, что пожелают, и мы должны принять условия. Если вы выберете основные преимущества, которые имеют графические процессоры NVIDIA с точки зрения сообщества и поддержки, вы также должны будете согласиться с тем, что вас могут вытолкнуть по желанию.

### AMD: мощная, но лишенная поддержки

 [HIP](https://github.com/ROCm-Developer-Tools/HIP) через [ROCm](https://github.com/ROCm-Developer-Tools) объединяет графические процессоры NVIDIA и AMD под общим языком программирования, который компилируется в соответствующий язык GPU перед его сборкой в сборку GPU. Если бы у нас был весь наш код GPU в HIP, это было бы важной вехой, но это довольно сложно, потому что трудно портировать базы кода TensorFlow и PyTorch. TensorFlow и PyTorch имеют некоторую поддержку графических процессоров AMD, и все основные сети могут работать на графических процессорах AMD, но если вы хотите разрабатывать новые сети, некоторые детали могут отсутствовать, что может помешать вам реализовать то, что вам нужно. Сообщество ROCm также не слишком велико, и поэтому не просто решить проблемы быстро. AMD мало инвестирует в свое программное обеспечение для глубокого обучения, и поэтому нельзя ожидать, что программный разрыв между NVIDIA и AMD сократится.

В настоящее время производительность графических процессоров AMD в порядке. Теперь у них есть 16-разрядные вычислительные возможности, что является важной вехой, но тензорные ядра графических процессоров NVIDIA обеспечивают гораздо более высокую вычислительную производительность для трансформаторов и сверточных сетей (хотя и не так много для рекуррентных сетей на уровне слов).

В целом, я все еще не могу дать четкую рекомендацию для графических процессоров AMD для обычных пользователей, которые просто хотят, чтобы их графические процессоры работали без сбоев. У более опытных пользователей должно быть меньше проблем, и, поддерживая графические процессоры AMD и разработчиков ROCm / HIP, они вносят свой вклад в борьбу с монопольным положением NVIDIA, поскольку это принесет большую пользу всем в долгосрочной перспективе. Если вы являетесь разработчиком графических процессоров и хотите внести важный вклад в вычисления на графических процессорах, то графический процессор AMD может стать лучшим способом оказать положительное влияние в долгосрочной перспективе. Для всех остальных графические процессоры NVIDIA могут быть более безопасным выбором.

### Intel: изо всех сил

Мой личный опыт работы с Intel Xeon Phis был очень разочаровывающим, и я не считаю их реальным конкурентом для карт NVIDIA или AMD, и поэтому я буду кратким: если вы решите пойти с Xeon Phi, учтите, что вы можете столкнуться с плохой поддержка, вычислительные проблемы, которые делают участки кода медленнее, чем у процессоров, трудности с написанием оптимизированного кода, отсутствие полной поддержки функций C ++ 11, отсутствие поддержки некоторых важных шаблонов проектирования графических процессоров, плохая совместимость с другими библиотеками, использующими подпрограммы BLAS (NumPy и SciPy) и, вероятно, многие другие разочарования, с которыми я не столкнулся.

Помимо Xeon Phi, я действительно с нетерпением ждал процессор нейронной сети Intel Nervana (NNP), потому что его спецификации были чрезвычайно мощными в руках разработчика графического процессора, и это позволило бы разработать новые алгоритмы, которые могли бы пересмотреть способ использования нейронных сетей, но это было отложено до бесконечности, и ходят слухи, что большие части развитых прыгнули с лодки. NNP запланирован на Q3 / Q4 2019. Если вы хотите подождать так долго, имейте в виду, что хорошее оборудование - это еще не все, как мы видим из AMD и Intel Xeon Phi. Вполне возможно, что это произойдет в 2020 или 2021 году, пока NNP не станет конкурентоспособным с GPU или TPU.

### Google: Powerful, Cheap On-Demand Processing

The Google TPU developed into a very mature cloud-based product that is cost-efficient. The easiest way to make sense of the TPU is by seeing it as multiple specialized GPUs packaged together that only have one purpose: Doing fast matrix multiplications. If we look at performance measures of the Tensor-Core-enabled V100 versus TPUv2 we find that both systems have nearly the same in performance for ResNet50 \[source is lost, not on Wayback Machine\]. However, the Google TPU is more cost-efficient. Since the TPUs have a sophisticated parallelization infrastructure, TPUs will have a major speed benefit over GPUs if you use more than 1 cloud TPU (equivalent to 4 GPUs).

Although still experimental, PyTorch is now also supporting TPUs which will help strengthen the TPU community and ecosystem.

TPUs still have some problems here and there, for example, a report from February 2018 said that the TPUv2 did not converge when LSTMs were used. I could not find a source if the problem has been fixed as of yet.

On the other hand, there is a big success story for training big transformers on TPUs. GPT-2, BERT, and machine translation models can be trained very efficiently on TPUs. According to my estimates from [my TPU vs GPU blog post](http://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/) , TPUs are about 56% faster than GPUs and thanks to their lower price compared to cloud GPUs they are an excellent choice for big transformer projects.

One issue with training large models on TPUs, however, can be cumulative cost. TPUs have high performance which is best used in the training phase. In the prototyping and inference phase, you should rely on non-cloud options to reduce costs. Thus training on TPUs, but prototyping and inferring on your personal GPU is the best choice.

To conclude, currently, TPUs seem to be best used for training convolutional network or large transformers and should be supplemented with other compute resources rather than a main deep learning resource.

### Amazon AWS and Microsoft Azure: Reliable but Expensive

GPU instances from Amazon AWS and Microsoft Azure are very attractive because one can easily scale up and scale down based on needs. This is very useful for paper deadlines or for larger one-off projects. However, similarly to TPUs the raw costs add up quickly. Currently, GPU cloud instances are too expensive to be used in isolation and I recommend to have some dedicated cheap GPUs for prototyping before one launches the final training jobs in the cloud.

### Fancy Startup: Revolutionary Hardware Concept with No Software

There is a range of startups which aim to produce the next generation of deep learning hardware. These companies usually have an excellent theoretical design, are then bought by Google/Intel or others to get the funding that they need to make a full design and produce the chip. For the next generation of chips (3nm), the costs for this are roughly $1 billion before a chip can be produced. Once this stage is completed (no company managed to do this as of yet) the main problem is software. No company managed to produce software which will work in the current deep learning stack. A full software suite needs to be developed to be competitive, which is clear from the AMD vs NVIDIA example: AMD has great hardware but only 90% of the software — this is not enough to be competitive with NVIDIA.

Currently, no company is anywhere close to completing both hardware and software steps. The Intel NNP might be the closest, but from all of this one cannot expect a competitive product before 2020 or 2021. So currently we will need to stick to GPUs and TPUs.

Thus, fancy new hardware from your favorite startup can be safely disregarded for now.

## What Makes One GPU Faster Than Another?

 [TL;DR](https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/#tldr) 

Your first question might be what is the most important feature for fast GPU performance for deep learning: Is it CUDA cores? Clock speed? RAM size?

In 2019 the choice of a GPU is more confusing then ever: 16-bit computing, Tensor Cores, 16-bit GPUs without Tensor Cores, multiple generations of GPUs which are still viable (Turning, Volta, Maxwell). But still there are some reliable performance indicators which people can use as a rule of thumb. Here some prioritization guidelines for different deep learning architectures:

Convolutional networks and Transformers: Tensor Cores > FLOPs > Memory Bandwidth > 16-bit capability  
Recurrent networks: Memory Bandwidth > 16-bit capability > Tensor Cores > FLOPs

This reads as follows: If I want to use, for example, convolutional networks, I should first prioritize a GPU that has tensor cores, then a high FLOPs number, then a high memory bandwidth, and then a GPU which has 16-bit capability. While prioritizing, it is important to pick a GPU which has enough GPU memory to run the models one is interested in.

## Why These Priorities?

One thing that to deepen your understanding to make an informed choice is to learn a bit about what parts of the hardware makes GPUs fast for the two most important tensor operations: Matrix multiplication and convolution.

A simple and effective way to think about matrix multiplication A\*B=C is that it is memory bandwidth bound: Copying memory of A, B unto the chip is more costly than to do the computations of A\*B. This means memory bandwidth is the most important feature of a GPU if you want to use LSTMs and other recurrent networks that do lots of small matrix multiplications. The smaller the matrix multiplications, the more important is memory bandwidth.

On the contrary, convolution is bound by computation speed. Thus TFLOPs on a GPU is the best indicator for the performance of ResNets and other convolutional architectures. Tensor Cores can increase FLOPs dramatically.

Large matrix multiplication as used in transformers is in-between convolution and small matrix multiplication of RNNs. Big matrix multiplications benefit a lot from 16-bit storage, Tensor Cores, and FLOPs but they still need high memory bandwidth.

Note that to use the benefits of Tensor Cores you should use 16-bit data and weights — avoid using 32-bit with RTX cards! If you encounter problems with 16-bit training using PyTorch, then you should use dynamic loss scaling as provided by the [Apex library](https://github.com/NVIDIA/apex) . If you use TensorFlow you can implement loss scaling yourself: (1) multiply your loss by a big number, (2) calculate the gradient, (3) divide by the big number, (4) update your weights. Usually, 16-bit training should be just fine, but if you are having trouble replicating results with 16-bit loss scaling will usually solve the issue.

 [ ![Figure 2: Normalized performance data of GPUs and TPU. Higher is better. RTX cards assume 16-bit computation. The word RNN numbers refer to biLSTM performance for short sequences of length <100. Benchmarking was done using PyTorch 1.0.1 and CUDA 10.](/images/e1a823b055384ce330c1f3b5a7c7abf9) ](https://i1.wp.com/timdettmers.com/wp-content/uploads/2019/04/performance_RTX.png) 

Figure 2: Normalized performance data of GPUs and TPU. Higher is better. RTX cards assume 16-bit computation. The word RNN numbers refer to biLSTM performance for short sequences of length <100. Benchmarking was done using PyTorch 1.0.1 and CUDA 10.

## Cost Efficiency Analysis

The cost-efficiency of a GPU is probably the most important criterion for selecting a GPU. The performance analysis for this blog post update was done as follows:  
(1) For transformers, I benchmarked Transformer-XL and BERT.  
(2) For word and char RNNs I benchmarked state-of-the-art biLSTM models.  
(3) The benchmarking in (1) and (2) was done for Titan Xp, Titan RTX, and RTX 2080 Ti. For other cards, I scaled the performance differences linearly.  
(4) I used the existing benchmark for CNNs: ( [1](https://www.phoronix.com/scan.php?page=article&item=nvidia-rtx2080ti-tensorflow&num=6) , [2](https://github.com/stefan-it/dl-benchmarks) , [3](https://blog.riseml.com/comparing-google-tpuv2-against-nvidia-v100-on-resnet-50-c2bbb6a51e5e) , [4](https://www.pugetsystems.com/labs/hpc/RTX-2080Ti-with-NVLINK---TensorFlow-Performance-Includes-Comparison-with-GTX-1080Ti-RTX-2070-2080-2080Ti-and-Titan-V-1267/) , [5](https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/) , [6](https://github.com/u39kun/deep-learning-benchmark) , [7](https://www.xcelerit.com/computing-benchmarks/insights/benchmarks-deep-learning-nvidia-p100-vs-v100-gpu/) ).  
(5) I used the mean cost of Amazon and eBay as a reference cost for a GPU.

 [ ![Figure 3: Normalized performance/cost numbers for convolutional networks (CNN), recurrent networks (RNN) and transformers. Higher is better. An RTX 2060 is more than 5 times more cost-efficient than a Tesla V100. The word RNN numbers refer to biLSTM performance for short sequences of length <100. Benchmarking was done using PyTorch 1.0.1 and CUDA 10.](/images/581d889faa36a18615ffe91941b55923) ](https://i1.wp.com/timdettmers.com/wp-content/uploads/2019/04/performance_per_dollar_RTX.png) 

Figure 3: Normalized performance/cost numbers for convolutional networks (CNN), recurrent networks (RNN) and transformers. Higher is better. An RTX 2060 is more than 5 times more cost-efficient than a Tesla V100. The word RNN numbers refer to biLSTM performance for short sequences of length <100. Benchmarking was done using PyTorch 1.0.1 and CUDA 10.

From this data, we see that the RTX 2060 is more cost-efficient than the RTX 2070, RTX 2080 or the RTX 2080 Ti. Why is this so? The ability to do 16-bit computation with Tensor Cores is much more valuable than just having a bigger ship with more Tensor Cores cores. With the RTX 2060, you get these features for the lowest price.

However, this analysis has certain biases which should be taken into account:  
(1) This analysis is strongly biased in favor of smaller cards. Smaller, cost-efficient GPUs might not have enough memory to run the models that you care about!  
(2) Overprices GTX 10xx cards: Currently, GTX 10XX cards seem to be overpriced since gamers do not like RTX cards.  
(3) Single-GPU bias: One computer with 4 cost-inefficient cards (4x RTX 2080 Ti) is much more cost-efficient than 2 computers with the most cost/efficient cards (8x RTX 2060).

## Warning: Multi-GPU RTX Heat Problems

There are problems with the RTX 2080 Ti and other RTX GPUs with the standard dual fan if you use multiple GPUs that run next to each other. This is especially so for multiple RTX 2080 Ti in one computer but multiple RTX 2080 and RTX 2070 can also be affected. The fan on some of the RTX cards is a new design developed by NVIDIA to improve the experience for gamers that run a single GPU (silent, lower heat for one GPU). However, the design is terrible if you use multiple GPUs that have this open dual fan design. If you want to use multiple RTX cards that run next to each other (directly in the next PCIe slot) then you should get the version that has a “blower-style” single fan design. This is especially true for RTX 2080 Ti cards. ASUS and PNY currently have RTX 2080 Ti models on the market with a blower-style fan. If you use two RTX 2070 you should be fine with any fan though, however, I would also get a blower-style fan with you run more than 2 RTX 2070 next to each other.

## Required Memory Size and 16-bit Training

The memory on a GPU can be critical for some applications like computer vision, machine translation, and certain other NLP applications and you might think that the RTX 2070 is cost-efficient, but its memory is too small with 8 GB. However, note that through 16-bit training you virtually have 16 GB of memory and any standard model should fit into your RTX 2070 easily if you use 16-bits. The same is true for the RTX 2080 and RTX 2080 Ti. Note though, that in most software frameworks you will not automatically save half of the memory by using 16-bit since some frameworks store weights in 32-bits to do more precise gradient updates and so forth. A good rule of thumb is to assume 50% more memory with 16-bit compute. So a 16-bit 8GB memory is about equivalent in size to a 12 GB 32-bit memory.

## General GPU Recommendations

Currently, my main recommendation is to get an RTX 2070 GPU and use 16-bit training. I would never recommend buying an XP Titan, Titan V, any Quadro cards, or any Founders Edition GPUs. However, there are some specific GPUs which also have their place:  
(1) For extra memory, I would recommend an RTX 2080 Ti. If you really need a lot of extra memory, the RTX Titan is the best option — but make sure you really do need that memory!  
(2) For extra performance, I would recommend an RTX 2080 Ti.  
(3) If you are short on money I would recommend any cheap GTX 10XX card from eBay (depending on how much memory you need) or an RTX 2060. If that is too expensive have a look at [Colab](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) .  
(4) If you just want to get started with deep learning a GTX 1060 (6GB) is a great option.  
(5) If you already have a GTX 1070 or better: Wait it out. An upgrade is not worth it unless you work with large transformers.  
(6) You want to learn quickly how to do deep learning: Multiple GTX 1060 (6GB).

## Deep Learning in the Cloud

Both GPU instances on AWS/Azure and TPUs in the Google Cloud are viable options for deep learning. While the TPU is a bit cheaper it is lacking the versatility and flexibility of cloud GPUs. TPUs might be the weapon of choice for training object recognition or transformer models. For other work-loads cloud GPUs are a safer bet — the good thing about cloud instances is that you can switch between GPUs and TPUs at any time or even use both at the same time.

However, mind the opportunity cost here: If you learn the skills to have a smooth work-flow with AWS/Azure, you lost time that could be spent doing work on a personal GPU, and you will also not have acquired the skills to use TPUs. If you use a personal GPU, you will not have the skills to expand into more GPUs/TPUs via the cloud. If you use TPUs you might be stuck with TensorFlow for a while if you want full features and it will not be straightforward to switch your code-base to PyTorch. Learning a smooth cloud GPU/TPU work-flow is an expensive opportunity cost and you should weight this cost if you make the choice for TPUs, cloud GPUs, or personal GPUs.

Another question is also about when to use cloud services. If you try to learn deep learning or you need to prototype then a personal GPU might be the best option since cloud instances can be pricey. However, once you have found a good deep network configuration and you just want to train a model using data parallelism then using cloud instances is a solid approach. This means that a small GPU will be sufficient for prototyping and one can rely on the power of cloud computing to scale up to larger experiments.

If you are short on money the cloud computing instances might also be a good solution: Prototype on a CPU and then roll out on GPU/TPU instances for a quick training run. This is not the best work-flow since prototyping on a CPU can be a big pain, but it can be a cost-efficient alternative.

## Conclusion 

With the information in this blog post, you should be able to reason which GPU is suitable for you. In general, I see three main strategies (1) stick with your GTX 1070 or better GPU, (2) buy a RTX GPU, (3) use some kind of GPU for prototyping and then train your model on TPUs or cloud GPUs in parallel.

## TL;DR advice

 **Best GPU overall** : RTX 2070  
 **GPUs to avoid** : Any Tesla card; any Quadro card; any Founders Edition card; Titan RTX, Titan V, Titan XP  
 **Cost-efficient but expensive** : RTX 2070  
 **Cost-efficient and cheap** :  RTX 2060, GTX 1060 (6GB).  
 **I have little money** : GTX 1060 (6GB)  
 **I have almost no money** : GTX 1050 Ti (4GB).Alternatively: CPU (prototyping) + AWS/TPU (training); or Colab.  
 **I do Kaggle: RTX 2070** . If you do not have enough money go for a GTX 1060 (6GB) or GTX Titan (Pascal) from eBay for prototyping and AWS for final training. Use fastai library.  
 **I am a competitive computer vision or machine translation researcher** : GTX 2080 Ti with the blower fan design. If you train very large networks get RTX Titans.  
 **I am an NLP researcher** : RTX 2080 Ti use 16-bit.  
 **I want to build a GPU cluster** : This is really complicated, you can get some ideas from my [multi-GPU blog post](http://timdettmers.com/2014/09/21/how-to-build-and-use-a-multi-gpu-system-for-deep-learning/) .  
 **I started deep learning and I am serious about it** : Start with an RTX 2070. Buy more RTX 2070 after 6-9 months and you still want to invest more time into deep learning. Depending on what area you choose next (startup, Kaggle, research, applied deep learning) sell your GPU and buy something more appropriate after about two years.  
 **I want to try deep learning, but I am not serious about it** : GTX 1050 Ti (4 or 2GB). This often fits into your standard desktop and does not require a new PSU. If it fits, do not buy a new computer!

Update 2019-04-03: Added RTX Titan and GTX 1660 Ti. Updated TPU section. Added startup hardware discussion.  
Update 2018-11-26: Added discussion of overheating issues of RTX cards.  
Update 2018-11-05: Added RTX 2070 and updated recommendations. Updated charts with hard performance data. Updated TPU section.  
Update 2018-08-21: Added RTX 2080 and RTX 2080 Ti; reworked performance analysis  
Update 2017-04-09: Added cost efficiency analysis; updated recommendation with NVIDIA Titan Xp  
Update 2017-03-19: Cleaned up blog post; added GTX 1080 Ti  
Update 2016-07-23: Added Titan X Pascal and GTX 1060; updated recommendations  
Update 2016-06-25: Reworked multi-GPU section; removed simple neural network memory section as no longer relevant; expanded convolutional memory section; truncated AWS section due to not being efficient anymore; added my opinion about the Xeon Phi; added updates for the GTX 1000 series  
Update 2015-08-20: Added section for AWS GPU instances; added GTX 980 Ti to the comparison relation  
Update 2015-04-22: GTX 580 no longer recommended; added performance relationships between cards  
Update 2015-03-16: Updated GPU recommendations: GTX 970 and GTX 580  
Update 2015-02-23: Updated GPU recommendations and memory calculations  
 Update 2014-09-28: Added emphasis for memory requirement of CNNs 

 **Acknowledgments** 

I want to thank  Mat Kelcey  for helping me to debug and test custom code for the GTX 970; I want to thank Sander Dieleman for making me aware of the shortcomings of my GPU memory advice for convolutional nets; I want to thank Hannes Bretschneider for pointing out software dependency problems for the GTX 580; and I want to thank Oliver Griesel for pointing out notebook solutions for AWS instances. I want to thank Brad Nemire for providing me with an RTX Titan for benchmarking purposes.

**********
[GPU](/tags/GPU.md)
[RTX](/tags/RTX.md)
[GTX](/tags/GTX.md)
[Titan](/tags/Titan.md)
