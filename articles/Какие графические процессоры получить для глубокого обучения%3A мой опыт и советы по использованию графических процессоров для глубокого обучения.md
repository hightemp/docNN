# Какие графические процессоры получить для глубокого обучения: мой опыт и советы по использованию графических процессоров для глубокого обучения

Глубокое обучение - это область с высокими вычислительными требованиями, и выбор вашего графического процессора будет в основном определять ваш опыт глубокого обучения. Но какие функции важны, если вы хотите купить новый графический процессор? ОЗУ ГП, ядра, тензорные ядра? Как сделать экономичный выбор? Этот пост будет посвящен этим вопросам и даст вам совет, который поможет вам сделать правильный выбор.  
 [TL;DR](https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/#tldr)   
Наличие быстрого GPU является очень важным аспектом, когда человек начинает изучать глубокое обучение, поскольку это позволяет быстро получить практический опыт, который является ключом к накоплению опыта, с помощью которого вы сможете применить глубокое обучение к новым задачам. Без этой быстрой обратной связи просто требуется слишком много времени, чтобы учиться на своих ошибках, и это может быть обескураживающим и разочаровывающим, чтобы продолжить глубокое обучение. С помощью графических процессоров я быстро научился применять глубокое обучение на ряде соревнований Kaggle, и мне удалось заработать второе место в конкурсе Partly Sunny с шансом Hashtags Kaggle, используя [метод глубокого обучения](https://www.kaggle.com/c/crowdflower-weather-twitter/forums/t/6488/congratulations/35640#post35640), где была задача предсказать рейтинги погоды для данного твита. В конкурсе я использовал довольно большую двухслойную глубокую нейронную сеть с выпрямленными линейными единицами и выпадением для регуляризации, и эта глубокая сеть едва помещалась в мою 6 ГБ память GPU. Графические процессоры GTX Titan, которые поддерживали меня в соревновании, были основным фактором, по которому я занял 2 место в соревновании.

## Обзор

Этот пост блога структурирован следующим образом. Сначала я расскажу о том, насколько полезно иметь несколько графических процессоров, затем я расскажу обо всех соответствующих аппаратных опциях, таких как графические процессоры NVIDIA и AMD, Intel Xeon Phis, Google TPU и новое загрузочное оборудование. Затем я расскажу, какие спецификации графических процессоров являются хорошими показателями для глубокого обучения Основная часть посвящена анализу эффективности и эффективности затрат. Я заканчиваю общими и более конкретными рекомендациями GPU.

## Несколько графических процессоров делают мои тренировки быстрее?

Когда я начал использовать несколько графических процессоров, я был рад использовать параллелизм данных для повышения производительности во время соревнования Kaggle. Однако я обнаружил, что очень сложно добиться прямого ускорения с помощью нескольких графических процессоров. Мне было любопытно об этой проблеме, и поэтому я начал проводить исследование параллелизма в глубоком обучении. Я проанализировал распараллеливание в архитектурах глубокого обучения, разработал [метод 8-разрядного квантования](https://arxiv.org/abs/1511.04561), чтобы увеличить ускорения в кластерах графических процессоров с 23x до 50x для системы из 96 графических процессоров, и опубликовал свой исследование в ICLR 2016.

Основная идея заключалась в том, что сверточные и рекуррентные сети довольно легко распараллелить, особенно если вы используете только один компьютер или 4 графических процессора. Тем не менее, полностью подключенные сети, включая трансформаторы, не так просто распараллелить и нуждаются в специализированных алгоритмах для хорошей работы.

 [ ![GPU pic](/images/911d5bf5696df767f09d580412e115eb) ](https://i1.wp.com/timdettmers.com/wp-content/uploads/2014/08/gpu-pic.jpg) 

Рисунок 1: Настройка на моем главном компьютере: вы видите три графических процессора и карту InfiniBand. Это хорошая установка для глубокого обучения?

Современные библиотеки, такие как TensorFlow и PyTorch, отлично подходят для распараллеливания рекуррентных и сверточных сетей, а для свертки можно ожидать ускорения примерно 1,9x / 2,8x / 3,5x для 2/3/4 графических процессоров. Для рекуррентных сетей длина последовательности является наиболее важным параметром, а для распространенных проблем НЛП можно ожидать аналогичного или несколько худшего ускорения по сравнению с сверточными сетями. Однако полностью подключенные сети, включая трансформаторы, обычно имеют низкую производительность для параллелизма данных, и для ускорения этих частей сети необходимы более совершенные алгоритмы. Если вы запускаете преобразователи на нескольких графических процессорах, попробуйте запустить их на 1 графическом процессоре и посмотреть, работает ли он быстрее или нет.

## Использование нескольких графических процессоров без параллелизма

Другое преимущество использования нескольких графических процессоров, даже если вы не распараллеливаете алгоритмы, состоит в том, что вы можете запускать несколько алгоритмов или экспериментов отдельно на каждом графическом процессоре. Эффективный поиск гиперпараметров является наиболее распространенным использованием нескольких графических процессоров. Вы не получаете ускорений, но быстрее получаете информацию о производительности различных настроек гиперпараметра или другой сетевой архитектуры. Это также очень полезно для новичков, так как вы можете быстро получить представление и узнать, как вы можете обучить незнакомую архитектуру глубокого обучения.

Использование нескольких графических процессоров таким способом обычно более полезно, чем запуск одной сети на нескольких графических процессорах через параллелизм данных. Следует помнить об этом при покупке нескольких графических процессоров: качества для лучшего параллелизма, такие как количество линий PCIe, не так важны при покупке нескольких графических процессоров.

Кроме того, обратите внимание, что одного GPU должно хватить практически для любой задачи. Таким образом, диапазон, который вы можете получить с 1 GPU, не будет отличаться от того, когда у вас 4 GPU. Единственное отличие состоит в том, что вы можете проводить больше экспериментов за определенное время с несколькими графическими процессорами.

## Ваши варианты: NVIDIA против AMD против Intel против Google против Amazon против Microsoft против необычного стартапа

### NVIDIA: лидер

Стандартные библиотеки NVIDIA упростили создание первых библиотек глубокого обучения в CUDA, хотя таких мощных стандартных библиотек для OpenCL от AMD не было. Это раннее преимущество в сочетании с сильной поддержкой сообщества со стороны NVIDIA быстро увеличило размер сообщества CUDA. Это означает, что если вы используете графические процессоры NVIDIA, вы легко найдете поддержку, если что-то пойдет не так, вы найдете поддержку и совет, если будете самостоятельно программировать CUDA, и обнаружите, что большинство библиотек с углубленным изучением имеют наилучшую поддержку графических процессоров NVIDIA. В последние месяцы NVIDIA все еще вкладывала в [программное обеспечение](https://devblogs.nvidia.com/new-optimizations-accelerate-deep-learning-training-gpu/) еще больше ресурсов . Например, [библиотека Apex](https://github.com/NVIDIA/apex) предлагает поддержку для стабилизации 16-разрядных градиентов в PyTorch, а также включает быстрые оптимизаторы с плавкими переходами, такие как FusedAdam. В целом, программное обеспечение является сильной стороной графических процессоров NVIDIA.

С другой стороны, теперь у NVIDIA есть политика, согласно которой использование CUDA в центрах обработки данных разрешено только для графических процессоров Tesla, но не для карт GTX или RTX. Непонятно, что подразумевается под «центрами данных», но это означает, что организации и университеты часто вынуждены покупать дорогие и неэффективные с точки зрения затрат графические процессоры Tesla из-за страха перед юридическими проблемами. Однако карты Tesla не имеют реального преимущества перед картами GTX и RTX и стоят в 10 раз дороже.

То, что NVIDIA может сделать это без каких-либо серьезных препятствий, демонстрирует силу их монополии - они могут делать все, что пожелают, и мы должны принять условия. Если вы выберете основные преимущества, которые имеют графические процессоры NVIDIA с точки зрения сообщества и поддержки, вы также должны будете согласиться с тем, что вас могут вытолкнуть по желанию.

### AMD: мощная, но лишенная поддержки

 [HIP](https://github.com/ROCm-Developer-Tools/HIP) через [ROCm](https://github.com/ROCm-Developer-Tools) объединяет графические процессоры NVIDIA и AMD под общим языком программирования, который компилируется в соответствующий язык GPU перед его сборкой в сборку GPU. Если бы у нас был весь наш код GPU в HIP, это было бы важной вехой, но это довольно сложно, потому что трудно портировать базы кода TensorFlow и PyTorch. TensorFlow и PyTorch имеют некоторую поддержку графических процессоров AMD, и все основные сети могут работать на графических процессорах AMD, но если вы хотите разрабатывать новые сети, некоторые детали могут отсутствовать, что может помешать вам реализовать то, что вам нужно. Сообщество ROCm также не слишком велико, и поэтому не просто решить проблемы быстро. AMD мало инвестирует в свое программное обеспечение для глубокого обучения, и поэтому нельзя ожидать, что программный разрыв между NVIDIA и AMD сократится.

В настоящее время производительность графических процессоров AMD в порядке. Теперь у них есть 16-разрядные вычислительные возможности, что является важной вехой, но тензорные ядра графических процессоров NVIDIA обеспечивают гораздо более высокую вычислительную производительность для трансформаторов и сверточных сетей (хотя и не так много для рекуррентных сетей на уровне слов).

В целом, я все еще не могу дать четкую рекомендацию для графических процессоров AMD для обычных пользователей, которые просто хотят, чтобы их графические процессоры работали без сбоев. У более опытных пользователей должно быть меньше проблем, и, поддерживая графические процессоры AMD и разработчиков ROCm / HIP, они вносят свой вклад в борьбу с монопольным положением NVIDIA, поскольку это принесет большую пользу всем в долгосрочной перспективе. Если вы являетесь разработчиком графических процессоров и хотите внести важный вклад в вычисления на графических процессорах, то графический процессор AMD может стать лучшим способом оказать положительное влияние в долгосрочной перспективе. Для всех остальных графические процессоры NVIDIA могут быть более безопасным выбором.

### Intel: изо всех сил

Мой личный опыт работы с Intel Xeon Phis был очень разочаровывающим, и я не считаю их реальным конкурентом для карт NVIDIA или AMD, и поэтому я буду кратким: если вы решите пойти с Xeon Phi, учтите, что вы можете столкнуться с плохой поддержка, вычислительные проблемы, которые делают участки кода медленнее, чем у процессоров, трудности с написанием оптимизированного кода, отсутствие полной поддержки функций C ++ 11, отсутствие поддержки некоторых важных шаблонов проектирования графических процессоров, плохая совместимость с другими библиотеками, использующими подпрограммы BLAS (NumPy и SciPy) и, вероятно, многие другие разочарования, с которыми я не столкнулся.

Помимо Xeon Phi, я действительно с нетерпением ждал процессор нейронной сети Intel Nervana (NNP), потому что его спецификации были чрезвычайно мощными в руках разработчика графического процессора, и это позволило бы разработать новые алгоритмы, которые могли бы пересмотреть способ использования нейронных сетей, но это было отложено до бесконечности, и ходят слухи, что большие части развитых прыгнули с лодки. NNP запланирован на Q3 / Q4 2019. Если вы хотите подождать так долго, имейте в виду, что хорошее оборудование - это еще не все, как мы видим из AMD и Intel Xeon Phi. Вполне возможно, что это произойдет в 2020 или 2021 году, пока NNP не станет конкурентоспособным с GPU или TPU.

### Google: мощная, дешевая обработка по требованию

Google TPU превратился в очень зрелый облачный продукт, который является экономически эффективным. Самый простой способ разобраться в TPU - это увидеть несколько специализированных графических процессоров, упакованных вместе, которые преследуют только одну цель: быстрое умножение матриц. Если мы посмотрим на показатели производительности V100 с поддержкой Tensor-Core и TPUv2, то обнаружим, что обе системы имеют примерно одинаковую производительность для ResNet50\[источник потерян, а не на Wayback Machine\]. Тем не менее, Google TPU является более экономичным. Поскольку TPU имеют сложную инфраструктуру распараллеливания, TPU будут иметь значительное преимущество в скорости по сравнению с GPU, если вы используете более 1 облачного TPU (эквивалентно 4 GPU).

Хотя PyTorch все еще экспериментален, он также поддерживает TPU, что поможет укрепить сообщество и экосистему TPU.

У TPU все еще есть некоторые проблемы, например, в отчете за февраль 2018 года говорится, что TPUv2 не сходился при использовании LSTM. Я не смог найти источник, если проблема была решена на данный момент.

С другой стороны, существует большой успех в обучении больших трансформаторов на ТПУ. Модели GPT-2, BERT и машинного перевода могут быть очень эффективно обучены на TPU. Согласно моим оценкам из [моего блога TPU vs GPU](http://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/), TPU примерно на 56% быстрее, чем GPU и благодаря их более низкой цене по сравнению с облачными графическими процессорами они являются отличным выбором для крупных проектов трансформаторов.

Однако одной проблемой при обучении больших моделей на TPU может быть совокупная стоимость. TPU имеют высокую производительность, которая лучше всего используется на этапе обучения. На этапе создания прототипа и вывода вы должны полагаться на не облачные опции для снижения затрат. Таким образом, обучение на TPU, но создание прототипов и вывод ваших персональных графических процессоров - лучший выбор.

В заключение следует сказать, что в настоящее время TPU лучше всего использовать для обучения сверточной сети или больших преобразователей, и их следует дополнять другими вычислительными ресурсами, а не основным ресурсом глубокого обучения.

### Amazon AWS и Microsoft Azure: надежно, но дорого

Экземпляры графических процессоров от Amazon AWS и Microsoft Azure очень привлекательны, поскольку их можно легко масштабировать в зависимости от потребностей. Это очень полезно для сроков подачи документов или для более крупных разовых проектов. Однако, как и в случае с ТПУ, сырые затраты быстро складываются. В настоящее время облачные экземпляры GPU слишком дороги для использования в изоляции, и я рекомендую иметь несколько выделенных дешевых графических процессоров для создания прототипов, прежде чем запускать финальные учебные задания в облаке.

### Необычный запуск: революционная аппаратная концепция без программного обеспечения

Существует целый ряд стартапов, целью которых является выпуск следующего поколения оборудования для глубокого обучения. Эти компании обычно имеют превосходный теоретический дизайн, а затем покупаются Google / Intel или другими, чтобы получить финансирование, необходимое для создания полного дизайна и производства чипа. Для следующего поколения чипов (3 нм) затраты на это составляют примерно 1 млрд. Долл. США, прежде чем чип может быть произведен. Когда этот этап завершен (ни одной компании пока не удалось этого сделать), основной проблемой является программное обеспечение. Ни одной компании не удалось создать программное обеспечение, которое будет работать в нынешнем стэке глубокого обучения. Для того чтобы быть конкурентоспособным, необходимо разработать полный пакет программного обеспечения, что ясно из примера AMD против NVIDIA: у AMD отличное оборудование, но только 90% программного обеспечения - этого недостаточно, чтобы конкурировать с NVIDIA.

В настоящее время ни одна компания не может приблизиться к выполнению аппаратных и программных этапов. Intel NNP может быть самым близким, но от всего этого нельзя ожидать конкурентоспособного продукта до 2020 или 2021 года. Поэтому в настоящее время нам нужно будет придерживаться GPU и TPU.

Таким образом, модное новое оборудование от вашего любимого стартапа пока можно спокойно игнорировать.

## Что делает один графический процессор быстрее другого?

 [TL;DR](https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/#tldr) 

Ваш первый вопрос может заключаться в том, что является наиболее важной функцией для быстрой производительности графического процессора для глубокого обучения: это ядра CUDA? Тактовая частота? Объем оперативной памяти?

В 2019 году выбор GPU стал еще более запутанным, чем когда-либо: 16-битные вычисления, Tensor Core, 16-битные GPU без Tensor Cores, несколько поколений GPU, которые все еще жизнеспособны (Turning, Volta, Maxwell). Но все же есть некоторые надежные показатели эффективности, которые люди могут использовать, как правило. Вот некоторые рекомендации по расстановке приоритетов для различных архитектур глубокого обучения:

Сверточные сети и трансформаторы: тензорные ядра> FLOPs> Пропускная способность памяти> 16-битная возможность
Рекуррентные сети: пропускная способность памяти> 16-битные возможности> тензорные ядра> FLOP

Это выглядит следующим образом: если я хочу использовать, например, сверточные сети, я должен сначала установить приоритетность графического процессора с тензорными ядрами, затем большим числом FLOP, затем высокой пропускной способностью памяти и затем графическим процессором с 16-битной возможностью , При расстановке приоритетов важно выбрать графический процессор, который имеет достаточно памяти для работы с интересующими вас моделями.

## Почему эти приоритеты?

Чтобы углубить ваше понимание и сделать осознанный выбор, нужно немного узнать о том, какие части аппаратного обеспечения ускоряют работу графических процессоров для двух наиболее важных тензорных операций: умножения матриц и свертки.

Простой и эффективный способ думать о матричном умножении A \* B = C состоит в том, что он ограничен пропускной способностью памяти: копирование памяти A, B в чип более затратно, чем вычисления A \* B. Это означает, что пропускная способность памяти является наиболее важной особенностью графического процессора, если вы хотите использовать LSTM и другие рекуррентные сети, которые выполняют много небольших умножений матриц. Чем меньше умножение матриц, тем важнее пропускная способность памяти.

Напротив, свертка связана со скоростью вычислений. Таким образом, TFLOP на графическом процессоре - лучший показатель производительности ResNets и других сверточных архитектур. Тензорные Ядра могут значительно увеличить FLOP.

Умножение с большой матрицей, используемое в преобразователях, является промежуточной сверткой и умножением с малой матрицей RNN. Умножение больших матриц значительно выигрывает от 16-битного хранилища, тензорных ядер и FLOP, но им все еще требуется высокая пропускная способность памяти.

Обратите внимание, что для использования преимуществ Tensor Cores вы должны использовать 16-битные данные и веса - избегайте использования 32-битных с картами RTX! Если у вас возникают проблемы с 16-разрядным обучением с использованием PyTorch, вам следует использовать динамическое масштабирование потерь, как предусмотрено в [библиотеке Apex](https://github.com/NVIDIA/apex). Если вы используете TensorFlow, вы можете реализовать масштабирование потерь самостоятельно: (1) умножьте ваши потери на большое число, (2) вычислите градиент, (3) разделите на большое число, (4) обновите ваши веса. Как правило, 16-битное обучение должно быть в порядке, но если у вас возникли проблемы с репликацией результатов с 16-битным масштабированием, это, как правило, решит проблему.

 [ ![Figure 2: Normalized performance data of GPUs and TPU. Higher is better. RTX cards assume 16-bit computation. The word RNN numbers refer to biLSTM performance for short sequences of length <100. Benchmarking was done using PyTorch 1.0.1 and CUDA 10.](/images/e1a823b055384ce330c1f3b5a7c7abf9) ](https://i1.wp.com/timdettmers.com/wp-content/uploads/2019/04/performance_RTX.png) 

Рисунок 2: Нормализованные данные производительности графических процессоров и TPU. Чем выше, тем лучше. Карты RTX предполагают 16-битное вычисление. Числа слова RNN относятся к характеристикам biLSTM для коротких последовательностей длиной <100. Тестирование проводилось с использованием PyTorch 1.0.1 и CUDA 10.

## Анализ экономической эффективности

Рентабельность графического процессора, вероятно, является наиболее важным критерием выбора графического процессора. Анализ производительности для этого обновления в блоге был сделан следующим образом:
(1) Для трансформаторов я сравнил Transformer-XL и BERT.
(2) Для RNNs типа «слово» и «char» я провел сравнительный анализ современных моделей biLSTM.
(3) Сравнительный анализ в (1) и (2) был сделан для Titan Xp, Titan RTX и RTX 2080 Ti. Для других карт я оценил разницу в производительности линейно.
(4) Я использовал существующий тест для CNN: ([1](https://www.phoronix.com/scan.php?page=article&item=nvidia-rtx2080ti-tensorflow&num=6), [2](https: / /github.com/stefan-it/dl-benchmarks), [3](https://blog.riseml.com/comparing-google-tpuv2-against-nvidia-v100-on-resnet-50-c2bbb6a51e5e), [4](https://www.pugetsystems.com/labs/hpc/RTX-2080Ti-with-NVLINK---TensorFlow-Performance-Includes-Comparison-with-GTX-1080Ti-RTX-2070-2080-2080Ti-and -Titan-V-1267 /), [5](https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/), [6](https://github.com/u39kun/deep -learning-benchmark), [7](https://www.xcelerit.com/computing-benchmarks/insights/benchmarks-deep-learning-nvidia-p100-vs-v100-gpu/)).
(5) Я использовал среднюю стоимость Amazon и eBay в качестве эталонной стоимости для GPU.

 [ ![Figure 3: Normalized performance/cost numbers for convolutional networks (CNN), recurrent networks (RNN) and transformers. Higher is better. An RTX 2060 is more than 5 times more cost-efficient than a Tesla V100. The word RNN numbers refer to biLSTM performance for short sequences of length <100. Benchmarking was done using PyTorch 1.0.1 and CUDA 10.](/images/581d889faa36a18615ffe91941b55923) ](https://i1.wp.com/timdettmers.com/wp-content/uploads/2019/04/performance_per_dollar_RTX.png) 

Рисунок 3: Нормализованные показатели производительности / стоимости для сверточных сетей (CNN), рекуррентных сетей (RNN) и трансформаторов. Чем выше, тем лучше. RTX 2060 более чем в 5 раз экономичнее Tesla V100. Числа слова RNN относятся к характеристикам biLSTM для коротких последовательностей длиной <100. Тестирование проводилось с использованием PyTorch 1.0.1 и CUDA 10.

Из этих данных мы видим, что RTX 2060 является более экономичным, чем RTX 2070, RTX 2080 или RTX 2080 Ti. Почему это так? Способность выполнять 16-битные вычисления с Tensor Cores гораздо более ценна, чем просто иметь большой корабль с большим количеством ядер Tensor Cores. С RTX 2060 вы получаете эти функции по самой низкой цене.

Однако этот анализ имеет определенные отклонения, которые следует принимать во внимание:
(1) Этот анализ сильно смещен в пользу меньших карт. Небольшие экономичные графические процессоры могут не иметь достаточно памяти для запуска моделей, которые вам интересны!
(2) Переоценка карт GTX 10xx: В настоящее время карты GTX 10XX кажутся завышенными, поскольку геймерам не нравятся карты RTX.
(3) Смещение с одним графическим процессором: один компьютер с 4 экономически неэффективными картами (4x RTX 2080 Ti) гораздо более экономически эффективен, чем 2 компьютера с наиболее экономичными картами (8x RTX 2060).

## Предупреждение: Multi-GPU RTX тепловые проблемы

Есть проблемы с RTX 2080 Ti и другими графическими процессорами RTX со стандартным двойным вентилятором, если вы используете несколько графических процессоров, которые работают рядом друг с другом. Это особенно актуально для нескольких RTX 2080 Ti на одном компьютере, но также могут быть затронуты несколько RTX 2080 и RTX 2070. Вентилятор на некоторых картах RTX - это новый дизайн, разработанный NVIDIA для улучшения работы геймеров, использующих один графический процессор (тихий, более низкий нагрев для одного графического процессора). Тем не менее, дизайн ужасен, если вы используете несколько графических процессоров с такой открытой конструкцией с двумя вентиляторами. Если вы хотите использовать несколько RTX-карт, которые работают рядом друг с другом (непосредственно в следующем слоте PCIe), вы должны получить версию с дизайном в стиле «вентилятора». Это особенно актуально для карт RTX 2080 Ti. ASUS и PNY в настоящее время предлагают на рынке модели RTX 2080 Ti с вентилятором. Если вы используете два RTX 2070, вам будет хорошо с любым вентилятором, однако, я бы также получил вентилятор в стиле вентилятора, когда вы запускаете более 2 RTX 2070 рядом друг с другом.

## Требуемый объем памяти и 16-битное обучение

Память на графическом процессоре может быть критичной для некоторых приложений, таких как компьютерное зрение, машинный перевод и некоторые другие приложения НЛП, и вы можете подумать, что RTX 2070 является экономически эффективным, но его память слишком мала - 8 ГБ. Однако учтите, что благодаря 16-разрядному обучению у вас фактически есть 16 ГБ памяти, и любая стандартная модель должна легко вписываться в ваш RTX 2070, если вы используете 16-разрядные. То же самое верно для RTX 2080 и RTX 2080 Ti. Обратите внимание, что в большинстве программных сред вы не будете автоматически экономить половину памяти, используя 16-разрядные, поскольку некоторые платформы хранят веса в 32-разрядных для более точного обновления градиента и так далее. Хорошее эмпирическое правило - предполагать увеличение памяти на 50% при 16-битных вычислениях. Таким образом, 16-битная 8-Гбайт память примерно эквивалентна 12-Гбайтной 32-битной памяти.

## Общие рекомендации GPU

В настоящее время моя основная рекомендация - приобрести графический процессор RTX 2070 и использовать 16-битное обучение. Я никогда не рекомендовал бы покупать XP Titan, Titan V, любые карты Quadro или любые графические процессоры Founders Edition. Тем не менее, есть некоторые конкретные графические процессоры, которые также имеют свое место:
(1) Для дополнительной памяти я бы рекомендовал RTX 2080 Ti. Если вам действительно нужно много дополнительной памяти, RTX Titan - лучший вариант, но убедитесь, что вам действительно нужна эта память!
(2) Для дополнительной производительности я бы рекомендовал RTX 2080 Ti.
(3) Если у вас мало денег, я бы порекомендовал любую дешевую карту GTX 10XX от eBay (в зависимости от того, сколько памяти вам нужно) или RTX 2060. Если это слишком дорого, взгляните на [Colab](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d).
(4) Если вы просто хотите начать с глубокого изучения GTX 1060 (6 ГБ), это отличный вариант.
(5) Если у вас уже есть GTX 1070 или лучше: подождите. Обновление не стоит, если вы не работаете с большими трансформаторами.
(6) Вы хотите быстро научиться выполнять глубокое обучение: Multiple GTX 1060 (6 ГБ).

## Глубокое обучение в облаке

Оба экземпляра графического процессора в AWS / Azure и TPU в Google Cloud являются жизнеспособными вариантами для углубленного изучения. Хотя TPU немного дешевле, ему не хватает универсальности и гибкости облачных графических процессоров. ТПУ могут быть предпочтительным оружием для распознавания объектов обучения или моделей трансформаторов. Для других рабочих нагрузок облачные графические процессоры безопаснее - хорошая вещь в облачных экземплярах заключается в том, что вы можете переключаться между графическими процессорами и TPU в любое время или даже использовать оба одновременно.

Тем не менее, обратите внимание на альтернативную цену: если вы изучите навыки работы в AWS / Azure, вы потеряли время, которое можно было бы потратить на работу на персональном графическом процессоре, а также не приобрели навыки для использования TPUs. Если вы используете персональный графический процессор, у вас не будет навыков для перехода на большее количество графических процессоров / TPU через облако. Если вы используете TPU, вы можете на некоторое время застрять с TensorFlow, если вам нужны полные возможности, и будет непросто переключить базу кода на PyTorch. Изучение гладкой облачной работы с графическим процессором / TPU является дорогостоящей альтернативной стоимостью, и вам следует взвесить эту стоимость, если вы сделаете выбор в пользу TPU, облачных графических процессоров или персональных графических процессоров.

Другой вопрос также о том, когда использовать облачные сервисы. Если вы пытаетесь научиться глубокому изучению или вам необходимо создать прототип, то лучшим вариантом может стать персональный графический процессор, поскольку облачные экземпляры могут быть дорогими. Однако, как только вы нашли хорошую глубокую сетевую конфигурацию и просто хотите обучить модель с использованием параллелизма данных, тогда использование облачных экземпляров является надежным подходом. Это означает, что небольшого графического процессора будет достаточно для создания прототипов, и можно рассчитывать на мощь облачных вычислений для масштабирования до более крупных экспериментов.

Если у вас мало денег, экземпляры облачных вычислений также могут быть хорошим решением: прототип на процессоре, а затем разверните на экземплярах GPU / TPU для быстрого обучения. Это не лучший рабочий процесс, поскольку создание прототипа на процессоре может быть большой болью, но это может быть экономически эффективной альтернативой.

## Conclusion 

With the information in this blog post, you should be able to reason which GPU is suitable for you. In general, I see three main strategies (1) stick with your GTX 1070 or better GPU, (2) buy a RTX GPU, (3) use some kind of GPU for prototyping and then train your model on TPUs or cloud GPUs in parallel.

## TL;DR advice

 **Best GPU overall** : RTX 2070  
 **GPUs to avoid** : Any Tesla card; any Quadro card; any Founders Edition card; Titan RTX, Titan V, Titan XP  
 **Cost-efficient but expensive** : RTX 2070  
 **Cost-efficient and cheap** :  RTX 2060, GTX 1060 (6GB).  
 **I have little money** : GTX 1060 (6GB)  
 **I have almost no money** : GTX 1050 Ti (4GB).Alternatively: CPU (prototyping) + AWS/TPU (training); or Colab.  
 **I do Kaggle: RTX 2070** . If you do not have enough money go for a GTX 1060 (6GB) or GTX Titan (Pascal) from eBay for prototyping and AWS for final training. Use fastai library.  
 **I am a competitive computer vision or machine translation researcher** : GTX 2080 Ti with the blower fan design. If you train very large networks get RTX Titans.  
 **I am an NLP researcher** : RTX 2080 Ti use 16-bit.  
 **I want to build a GPU cluster** : This is really complicated, you can get some ideas from my [multi-GPU blog post](http://timdettmers.com/2014/09/21/how-to-build-and-use-a-multi-gpu-system-for-deep-learning/) .  
 **I started deep learning and I am serious about it** : Start with an RTX 2070. Buy more RTX 2070 after 6-9 months and you still want to invest more time into deep learning. Depending on what area you choose next (startup, Kaggle, research, applied deep learning) sell your GPU and buy something more appropriate after about two years.  
 **I want to try deep learning, but I am not serious about it** : GTX 1050 Ti (4 or 2GB). This often fits into your standard desktop and does not require a new PSU. If it fits, do not buy a new computer!

Update 2019-04-03: Added RTX Titan and GTX 1660 Ti. Updated TPU section. Added startup hardware discussion.  
Update 2018-11-26: Added discussion of overheating issues of RTX cards.  
Update 2018-11-05: Added RTX 2070 and updated recommendations. Updated charts with hard performance data. Updated TPU section.  
Update 2018-08-21: Added RTX 2080 and RTX 2080 Ti; reworked performance analysis  
Update 2017-04-09: Added cost efficiency analysis; updated recommendation with NVIDIA Titan Xp  
Update 2017-03-19: Cleaned up blog post; added GTX 1080 Ti  
Update 2016-07-23: Added Titan X Pascal and GTX 1060; updated recommendations  
Update 2016-06-25: Reworked multi-GPU section; removed simple neural network memory section as no longer relevant; expanded convolutional memory section; truncated AWS section due to not being efficient anymore; added my opinion about the Xeon Phi; added updates for the GTX 1000 series  
Update 2015-08-20: Added section for AWS GPU instances; added GTX 980 Ti to the comparison relation  
Update 2015-04-22: GTX 580 no longer recommended; added performance relationships between cards  
Update 2015-03-16: Updated GPU recommendations: GTX 970 and GTX 580  
Update 2015-02-23: Updated GPU recommendations and memory calculations  
 Update 2014-09-28: Added emphasis for memory requirement of CNNs 

 **Acknowledgments** 

I want to thank  Mat Kelcey  for helping me to debug and test custom code for the GTX 970; I want to thank Sander Dieleman for making me aware of the shortcomings of my GPU memory advice for convolutional nets; I want to thank Hannes Bretschneider for pointing out software dependency problems for the GTX 580; and I want to thank Oliver Griesel for pointing out notebook solutions for AWS instances. I want to thank Brad Nemire for providing me with an RTX Titan for benchmarking purposes.

**********
[GPU](/tags/GPU.md)
[RTX](/tags/RTX.md)
[GTX](/tags/GTX.md)
[Titan](/tags/Titan.md)
