# Какие графические процессоры получить для глубокого обучения: мой опыт и советы по использованию графических процессоров для глубокого обучения

Глубокое обучение - это область с высокими вычислительными требованиями, и выбор вашего графического процессора будет в основном определять ваш опыт глубокого обучения. Но какие функции важны, если вы хотите купить новый графический процессор? ОЗУ ГП, ядра, тензорные ядра? Как сделать экономичный выбор? Этот пост будет посвящен этим вопросам и даст вам совет, который поможет вам сделать правильный выбор.  
 [TL;DR](https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/#tldr)   
Наличие быстрого GPU является очень важным аспектом, когда человек начинает изучать глубокое обучение, поскольку это позволяет быстро получить практический опыт, который является ключом к накоплению опыта, с помощью которого вы сможете применить глубокое обучение к новым задачам. Без этой быстрой обратной связи просто требуется слишком много времени, чтобы учиться на своих ошибках, и это может быть обескураживающим и разочаровывающим, чтобы продолжить глубокое обучение. С помощью графических процессоров я быстро научился применять глубокое обучение на ряде соревнований Kaggle, и мне удалось заработать второе место в конкурсе Partly Sunny с шансом Hashtags Kaggle, используя [метод глубокого обучения](https://www.kaggle.com/c/crowdflower-weather-twitter/forums/t/6488/congratulations/35640#post35640), где была задача предсказать рейтинги погоды для данного твита. В конкурсе я использовал довольно большую двухслойную глубокую нейронную сеть с выпрямленными линейными единицами и выпадением для регуляризации, и эта глубокая сеть едва помещалась в мою 6 ГБ память GPU. Графические процессоры GTX Titan, которые поддерживали меня в соревновании, были основным фактором, по которому я занял 2 место в соревновании.

## Обзор

Этот пост блога структурирован следующим образом. Сначала я расскажу о том, насколько полезно иметь несколько графических процессоров, затем я расскажу обо всех соответствующих аппаратных опциях, таких как графические процессоры NVIDIA и AMD, Intel Xeon Phis, Google TPU и новое загрузочное оборудование. Затем я расскажу, какие спецификации графических процессоров являются хорошими показателями для глубокого обучения Основная часть посвящена анализу эффективности и эффективности затрат. Я заканчиваю общими и более конкретными рекомендациями GPU.

## Несколько графических процессоров делают мои тренировки быстрее?

Когда я начал использовать несколько графических процессоров, я был рад использовать параллелизм данных для повышения производительности во время соревнования Kaggle. Однако я обнаружил, что очень сложно добиться прямого ускорения с помощью нескольких графических процессоров. Мне было любопытно об этой проблеме, и поэтому я начал проводить исследование параллелизма в глубоком обучении. Я проанализировал распараллеливание в архитектурах глубокого обучения, разработал [метод 8-разрядного квантования](https://arxiv.org/abs/1511.04561), чтобы увеличить ускорения в кластерах графических процессоров с 23x до 50x для системы из 96 графических процессоров, и опубликовал свой исследование в ICLR 2016.

Основная идея заключалась в том, что сверточные и рекуррентные сети довольно легко распараллелить, особенно если вы используете только один компьютер или 4 графических процессора. Тем не менее, полностью подключенные сети, включая трансформаторы, не так просто распараллелить и нуждаются в специализированных алгоритмах для хорошей работы.

 [ ![GPU pic](/images/911d5bf5696df767f09d580412e115eb) ](https://i1.wp.com/timdettmers.com/wp-content/uploads/2014/08/gpu-pic.jpg) 

Рисунок 1: Настройка на моем главном компьютере: вы видите три графических процессора и карту InfiniBand. Это хорошая установка для глубокого обучения?

Современные библиотеки, такие как TensorFlow и PyTorch, отлично подходят для распараллеливания рекуррентных и сверточных сетей, а для свертки можно ожидать ускорения примерно 1,9x / 2,8x / 3,5x для 2/3/4 графических процессоров. Для рекуррентных сетей длина последовательности является наиболее важным параметром, а для распространенных проблем НЛП можно ожидать аналогичного или несколько худшего ускорения по сравнению с сверточными сетями. Однако полностью подключенные сети, включая трансформаторы, обычно имеют низкую производительность для параллелизма данных, и для ускорения этих частей сети необходимы более совершенные алгоритмы. Если вы запускаете преобразователи на нескольких графических процессорах, попробуйте запустить их на 1 графическом процессоре и посмотреть, работает ли он быстрее или нет.

## Использование нескольких графических процессоров без параллелизма

Другое преимущество использования нескольких графических процессоров, даже если вы не распараллеливаете алгоритмы, состоит в том, что вы можете запускать несколько алгоритмов или экспериментов отдельно на каждом графическом процессоре. Эффективный поиск гиперпараметров является наиболее распространенным использованием нескольких графических процессоров. Вы не получаете ускорений, но быстрее получаете информацию о производительности различных настроек гиперпараметра или другой сетевой архитектуры. Это также очень полезно для новичков, так как вы можете быстро получить представление и узнать, как вы можете обучить незнакомую архитектуру глубокого обучения.

Использование нескольких графических процессоров таким способом обычно более полезно, чем запуск одной сети на нескольких графических процессорах через параллелизм данных. Следует помнить об этом при покупке нескольких графических процессоров: качества для лучшего параллелизма, такие как количество линий PCIe, не так важны при покупке нескольких графических процессоров.

Кроме того, обратите внимание, что одного GPU должно хватить практически для любой задачи. Таким образом, диапазон, который вы можете получить с 1 GPU, не будет отличаться от того, когда у вас 4 GPU. Единственное отличие состоит в том, что вы можете проводить больше экспериментов за определенное время с несколькими графическими процессорами.

## Ваши варианты: NVIDIA против AMD против Intel против Google против Amazon против Microsoft против необычного стартапа

### NVIDIA: лидер

Стандартные библиотеки NVIDIA упростили создание первых библиотек глубокого обучения в CUDA, хотя таких мощных стандартных библиотек для OpenCL от AMD не было. Это раннее преимущество в сочетании с сильной поддержкой сообщества со стороны NVIDIA быстро увеличило размер сообщества CUDA. Это означает, что если вы используете графические процессоры NVIDIA, вы легко найдете поддержку, если что-то пойдет не так, вы найдете поддержку и совет, если будете самостоятельно программировать CUDA, и обнаружите, что большинство библиотек с углубленным изучением имеют наилучшую поддержку графических процессоров NVIDIA. В последние месяцы NVIDIA все еще вкладывала в [программное обеспечение](https://devblogs.nvidia.com/new-optimizations-accelerate-deep-learning-training-gpu/) еще больше ресурсов . Например, [библиотека Apex](https://github.com/NVIDIA/apex) предлагает поддержку для стабилизации 16-разрядных градиентов в PyTorch, а также включает быстрые оптимизаторы с плавкими переходами, такие как FusedAdam. В целом, программное обеспечение является сильной стороной графических процессоров NVIDIA.

С другой стороны, теперь у NVIDIA есть политика, согласно которой использование CUDA в центрах обработки данных разрешено только для графических процессоров Tesla, но не для карт GTX или RTX. Непонятно, что подразумевается под «центрами данных», но это означает, что организации и университеты часто вынуждены покупать дорогие и неэффективные с точки зрения затрат графические процессоры Tesla из-за страха перед юридическими проблемами. Однако карты Tesla не имеют реального преимущества перед картами GTX и RTX и стоят в 10 раз дороже.

То, что NVIDIA может сделать это без каких-либо серьезных препятствий, демонстрирует силу их монополии - они могут делать все, что пожелают, и мы должны принять условия. Если вы выберете основные преимущества, которые имеют графические процессоры NVIDIA с точки зрения сообщества и поддержки, вы также должны будете согласиться с тем, что вас могут вытолкнуть по желанию.

### AMD: мощная, но лишенная поддержки

 [HIP](https://github.com/ROCm-Developer-Tools/HIP) через [ROCm](https://github.com/ROCm-Developer-Tools) объединяет графические процессоры NVIDIA и AMD под общим языком программирования, который компилируется в соответствующий язык GPU перед его сборкой в сборку GPU. Если бы у нас был весь наш код GPU в HIP, это было бы важной вехой, но это довольно сложно, потому что трудно портировать базы кода TensorFlow и PyTorch. TensorFlow и PyTorch имеют некоторую поддержку графических процессоров AMD, и все основные сети могут работать на графических процессорах AMD, но если вы хотите разрабатывать новые сети, некоторые детали могут отсутствовать, что может помешать вам реализовать то, что вам нужно. Сообщество ROCm также не слишком велико, и поэтому не просто решить проблемы быстро. AMD мало инвестирует в свое программное обеспечение для глубокого обучения, и поэтому нельзя ожидать, что программный разрыв между NVIDIA и AMD сократится.

В настоящее время производительность графических процессоров AMD в порядке. Теперь у них есть 16-разрядные вычислительные возможности, что является важной вехой, но тензорные ядра графических процессоров NVIDIA обеспечивают гораздо более высокую вычислительную производительность для трансформаторов и сверточных сетей (хотя и не так много для рекуррентных сетей на уровне слов).

В целом, я все еще не могу дать четкую рекомендацию для графических процессоров AMD для обычных пользователей, которые просто хотят, чтобы их графические процессоры работали без сбоев. У более опытных пользователей должно быть меньше проблем, и, поддерживая графические процессоры AMD и разработчиков ROCm / HIP, они вносят свой вклад в борьбу с монопольным положением NVIDIA, поскольку это принесет большую пользу всем в долгосрочной перспективе. Если вы являетесь разработчиком графических процессоров и хотите внести важный вклад в вычисления на графических процессорах, то графический процессор AMD может стать лучшим способом оказать положительное влияние в долгосрочной перспективе. Для всех остальных графические процессоры NVIDIA могут быть более безопасным выбором.

### Intel: изо всех сил

Мой личный опыт работы с Intel Xeon Phis был очень разочаровывающим, и я не считаю их реальным конкурентом для карт NVIDIA или AMD, и поэтому я буду кратким: если вы решите пойти с Xeon Phi, учтите, что вы можете столкнуться с плохой поддержка, вычислительные проблемы, которые делают участки кода медленнее, чем у процессоров, трудности с написанием оптимизированного кода, отсутствие полной поддержки функций C ++ 11, отсутствие поддержки некоторых важных шаблонов проектирования графических процессоров, плохая совместимость с другими библиотеками, использующими подпрограммы BLAS (NumPy и SciPy) и, вероятно, многие другие разочарования, с которыми я не столкнулся.

Помимо Xeon Phi, я действительно с нетерпением ждал процессор нейронной сети Intel Nervana (NNP), потому что его спецификации были чрезвычайно мощными в руках разработчика графического процессора, и это позволило бы разработать новые алгоритмы, которые могли бы пересмотреть способ использования нейронных сетей, но это было отложено до бесконечности, и ходят слухи, что большие части развитых прыгнули с лодки. NNP запланирован на Q3 / Q4 2019. Если вы хотите подождать так долго, имейте в виду, что хорошее оборудование - это еще не все, как мы видим из AMD и Intel Xeon Phi. Вполне возможно, что это произойдет в 2020 или 2021 году, пока NNP не станет конкурентоспособным с GPU или TPU.

### Google: мощная, дешевая обработка по требованию

Google TPU превратился в очень зрелый облачный продукт, который является экономически эффективным. Самый простой способ разобраться в TPU - это увидеть несколько специализированных графических процессоров, упакованных вместе, которые преследуют только одну цель: быстрое умножение матриц. Если мы посмотрим на показатели производительности V100 с поддержкой Tensor-Core и TPUv2, то обнаружим, что обе системы имеют примерно одинаковую производительность для ResNet50\[источник потерян, а не на Wayback Machine\]. Тем не менее, Google TPU является более экономичным. Поскольку TPU имеют сложную инфраструктуру распараллеливания, TPU будут иметь значительное преимущество в скорости по сравнению с GPU, если вы используете более 1 облачного TPU (эквивалентно 4 GPU).

Хотя PyTorch все еще экспериментален, он также поддерживает TPU, что поможет укрепить сообщество и экосистему TPU.

У TPU все еще есть некоторые проблемы, например, в отчете за февраль 2018 года говорится, что TPUv2 не сходился при использовании LSTM. Я не смог найти источник, если проблема была решена на данный момент.

С другой стороны, существует большой успех в обучении больших трансформаторов на ТПУ. Модели GPT-2, BERT и машинного перевода могут быть очень эффективно обучены на TPU. Согласно моим оценкам из [моего блога TPU vs GPU](http://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/), TPU примерно на 56% быстрее, чем GPU и благодаря их более низкой цене по сравнению с облачными графическими процессорами они являются отличным выбором для крупных проектов трансформаторов.

Однако одной проблемой при обучении больших моделей на TPU может быть совокупная стоимость. TPU имеют высокую производительность, которая лучше всего используется на этапе обучения. На этапе создания прототипа и вывода вы должны полагаться на не облачные опции для снижения затрат. Таким образом, обучение на TPU, но создание прототипов и вывод ваших персональных графических процессоров - лучший выбор.

В заключение следует сказать, что в настоящее время TPU лучше всего использовать для обучения сверточной сети или больших преобразователей, и их следует дополнять другими вычислительными ресурсами, а не основным ресурсом глубокого обучения.

### Amazon AWS и Microsoft Azure: надежно, но дорого

Экземпляры графических процессоров от Amazon AWS и Microsoft Azure очень привлекательны, поскольку их можно легко масштабировать в зависимости от потребностей. Это очень полезно для сроков подачи документов или для более крупных разовых проектов. Однако, как и в случае с ТПУ, сырые затраты быстро складываются. В настоящее время облачные экземпляры GPU слишком дороги для использования в изоляции, и я рекомендую иметь несколько выделенных дешевых графических процессоров для создания прототипов, прежде чем запускать финальные учебные задания в облаке.

### Необычный запуск: революционная аппаратная концепция без программного обеспечения

Существует целый ряд стартапов, целью которых является выпуск следующего поколения оборудования для глубокого обучения. Эти компании обычно имеют превосходный теоретический дизайн, а затем покупаются Google / Intel или другими, чтобы получить финансирование, необходимое для создания полного дизайна и производства чипа. Для следующего поколения чипов (3 нм) затраты на это составляют примерно 1 млрд. Долл. США, прежде чем чип может быть произведен. Когда этот этап завершен (ни одной компании пока не удалось этого сделать), основной проблемой является программное обеспечение. Ни одной компании не удалось создать программное обеспечение, которое будет работать в нынешнем стэке глубокого обучения. Для того чтобы быть конкурентоспособным, необходимо разработать полный пакет программного обеспечения, что ясно из примера AMD против NVIDIA: у AMD отличное оборудование, но только 90% программного обеспечения - этого недостаточно, чтобы конкурировать с NVIDIA.

В настоящее время ни одна компания не может приблизиться к выполнению аппаратных и программных этапов. Intel NNP может быть самым близким, но от всего этого нельзя ожидать конкурентоспособного продукта до 2020 или 2021 года. Поэтому в настоящее время нам нужно будет придерживаться GPU и TPU.

Таким образом, модное новое оборудование от вашего любимого стартапа пока можно спокойно игнорировать.

## Что делает один графический процессор быстрее другого?

 [TL;DR](https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/#tldr) 

Ваш первый вопрос может заключаться в том, что является наиболее важной функцией для быстрой производительности графического процессора для глубокого обучения: это ядра CUDA? Тактовая частота? Объем оперативной памяти?

В 2019 году выбор GPU стал еще более запутанным, чем когда-либо: 16-битные вычисления, Tensor Core, 16-битные GPU без Tensor Cores, несколько поколений GPU, которые все еще жизнеспособны (Turning, Volta, Maxwell). Но все же есть некоторые надежные показатели эффективности, которые люди могут использовать, как правило. Вот некоторые рекомендации по расстановке приоритетов для различных архитектур глубокого обучения:

Сверточные сети и трансформаторы: тензорные ядра> FLOPs> Пропускная способность памяти> 16-битная возможность
Рекуррентные сети: пропускная способность памяти> 16-битные возможности> тензорные ядра> FLOP

Это выглядит следующим образом: если я хочу использовать, например, сверточные сети, я должен сначала установить приоритетность графического процессора с тензорными ядрами, затем большим числом FLOP, затем высокой пропускной способностью памяти и затем графическим процессором с 16-битной возможностью , При расстановке приоритетов важно выбрать графический процессор, который имеет достаточно памяти для работы с интересующими вас моделями.

## Почему эти приоритеты?

Чтобы углубить ваше понимание и сделать осознанный выбор, нужно немного узнать о том, какие части аппаратного обеспечения ускоряют работу графических процессоров для двух наиболее важных тензорных операций: умножения матриц и свертки.

Простой и эффективный способ думать о матричном умножении A \* B = C состоит в том, что он ограничен пропускной способностью памяти: копирование памяти A, B в чип более затратно, чем вычисления A \* B. Это означает, что пропускная способность памяти является наиболее важной особенностью графического процессора, если вы хотите использовать LSTM и другие рекуррентные сети, которые выполняют много небольших умножений матриц. Чем меньше умножение матриц, тем важнее пропускная способность памяти.

Напротив, свертка связана со скоростью вычислений. Таким образом, TFLOP на графическом процессоре - лучший показатель производительности ResNets и других сверточных архитектур. Тензорные Ядра могут значительно увеличить FLOP.

Умножение с большой матрицей, используемое в преобразователях, является промежуточной сверткой и умножением с малой матрицей RNN. Умножение больших матриц значительно выигрывает от 16-битного хранилища, тензорных ядер и FLOP, но им все еще требуется высокая пропускная способность памяти.

Обратите внимание, что для использования преимуществ Tensor Cores вы должны использовать 16-битные данные и веса - избегайте использования 32-битных с картами RTX! Если у вас возникают проблемы с 16-разрядным обучением с использованием PyTorch, вам следует использовать динамическое масштабирование потерь, как предусмотрено в [библиотеке Apex](https://github.com/NVIDIA/apex). Если вы используете TensorFlow, вы можете реализовать масштабирование потерь самостоятельно: (1) умножьте ваши потери на большое число, (2) вычислите градиент, (3) разделите на большое число, (4) обновите ваши веса. Как правило, 16-битное обучение должно быть в порядке, но если у вас возникли проблемы с репликацией результатов с 16-битным масштабированием, это, как правило, решит проблему.

 [ ![Figure 2: Normalized performance data of GPUs and TPU. Higher is better. RTX cards assume 16-bit computation. The word RNN numbers refer to biLSTM performance for short sequences of length <100. Benchmarking was done using PyTorch 1.0.1 and CUDA 10.](/images/e1a823b055384ce330c1f3b5a7c7abf9) ](https://i1.wp.com/timdettmers.com/wp-content/uploads/2019/04/performance_RTX.png) 

Рисунок 2: Нормализованные данные производительности графических процессоров и TPU. Чем выше, тем лучше. Карты RTX предполагают 16-битное вычисление. Числа слова RNN относятся к характеристикам biLSTM для коротких последовательностей длиной <100. Тестирование проводилось с использованием PyTorch 1.0.1 и CUDA 10.

## Анализ экономической эффективности

Рентабельность графического процессора, вероятно, является наиболее важным критерием выбора графического процессора. Анализ производительности для этого обновления в блоге был сделан следующим образом:
(1) Для трансформаторов я сравнил Transformer-XL и BERT.
(2) Для RNNs типа «слово» и «char» я провел сравнительный анализ современных моделей biLSTM.
(3) Сравнительный анализ в (1) и (2) был сделан для Titan Xp, Titan RTX и RTX 2080 Ti. Для других карт я оценил разницу в производительности линейно.
(4) Я использовал существующий тест для CNN: ([1](https://www.phoronix.com/scan.php?page=article&item=nvidia-rtx2080ti-tensorflow&num=6), [2](https: / /github.com/stefan-it/dl-benchmarks), [3](https://blog.riseml.com/comparing-google-tpuv2-against-nvidia-v100-on-resnet-50-c2bbb6a51e5e), [4](https://www.pugetsystems.com/labs/hpc/RTX-2080Ti-with-NVLINK---TensorFlow-Performance-Includes-Comparison-with-GTX-1080Ti-RTX-2070-2080-2080Ti-and -Titan-V-1267 /), [5](https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/), [6](https://github.com/u39kun/deep -learning-benchmark), [7](https://www.xcelerit.com/computing-benchmarks/insights/benchmarks-deep-learning-nvidia-p100-vs-v100-gpu/)).
(5) Я использовал среднюю стоимость Amazon и eBay в качестве эталонной стоимости для GPU.

 [ ![Figure 3: Normalized performance/cost numbers for convolutional networks (CNN), recurrent networks (RNN) and transformers. Higher is better. An RTX 2060 is more than 5 times more cost-efficient than a Tesla V100. The word RNN numbers refer to biLSTM performance for short sequences of length <100. Benchmarking was done using PyTorch 1.0.1 and CUDA 10.](/images/581d889faa36a18615ffe91941b55923) ](https://i1.wp.com/timdettmers.com/wp-content/uploads/2019/04/performance_per_dollar_RTX.png) 

Рисунок 3: Нормализованные показатели производительности / стоимости для сверточных сетей (CNN), рекуррентных сетей (RNN) и трансформаторов. Чем выше, тем лучше. RTX 2060 более чем в 5 раз экономичнее Tesla V100. Числа слова RNN относятся к характеристикам biLSTM для коротких последовательностей длиной <100. Тестирование проводилось с использованием PyTorch 1.0.1 и CUDA 10.

Из этих данных мы видим, что RTX 2060 является более экономичным, чем RTX 2070, RTX 2080 или RTX 2080 Ti. Почему это так? Способность выполнять 16-битные вычисления с Tensor Cores гораздо более ценна, чем просто иметь большой корабль с большим количеством ядер Tensor Cores. С RTX 2060 вы получаете эти функции по самой низкой цене.

Однако этот анализ имеет определенные отклонения, которые следует принимать во внимание:
(1) Этот анализ сильно смещен в пользу меньших карт. Небольшие экономичные графические процессоры могут не иметь достаточно памяти для запуска моделей, которые вам интересны!
(2) Переоценка карт GTX 10xx: В настоящее время карты GTX 10XX кажутся завышенными, поскольку геймерам не нравятся карты RTX.
(3) Смещение с одним графическим процессором: один компьютер с 4 экономически неэффективными картами (4x RTX 2080 Ti) гораздо более экономически эффективен, чем 2 компьютера с наиболее экономичными картами (8x RTX 2060).

## Предупреждение: Multi-GPU RTX тепловые проблемы

Есть проблемы с RTX 2080 Ti и другими графическими процессорами RTX со стандартным двойным вентилятором, если вы используете несколько графических процессоров, которые работают рядом друг с другом. Это особенно актуально для нескольких RTX 2080 Ti на одном компьютере, но также могут быть затронуты несколько RTX 2080 и RTX 2070. Вентилятор на некоторых картах RTX - это новый дизайн, разработанный NVIDIA для улучшения работы геймеров, использующих один графический процессор (тихий, более низкий нагрев для одного графического процессора). Тем не менее, дизайн ужасен, если вы используете несколько графических процессоров с такой открытой конструкцией с двумя вентиляторами. Если вы хотите использовать несколько RTX-карт, которые работают рядом друг с другом (непосредственно в следующем слоте PCIe), вы должны получить версию с дизайном в стиле «вентилятора». Это особенно актуально для карт RTX 2080 Ti. ASUS и PNY в настоящее время предлагают на рынке модели RTX 2080 Ti с вентилятором. Если вы используете два RTX 2070, вам будет хорошо с любым вентилятором, однако, я бы также получил вентилятор в стиле вентилятора, когда вы запускаете более 2 RTX 2070 рядом друг с другом.

## Required Memory Size and 16-bit Training

The memory on a GPU can be critical for some applications like computer vision, machine translation, and certain other NLP applications and you might think that the RTX 2070 is cost-efficient, but its memory is too small with 8 GB. However, note that through 16-bit training you virtually have 16 GB of memory and any standard model should fit into your RTX 2070 easily if you use 16-bits. The same is true for the RTX 2080 and RTX 2080 Ti. Note though, that in most software frameworks you will not automatically save half of the memory by using 16-bit since some frameworks store weights in 32-bits to do more precise gradient updates and so forth. A good rule of thumb is to assume 50% more memory with 16-bit compute. So a 16-bit 8GB memory is about equivalent in size to a 12 GB 32-bit memory.

## General GPU Recommendations

Currently, my main recommendation is to get an RTX 2070 GPU and use 16-bit training. I would never recommend buying an XP Titan, Titan V, any Quadro cards, or any Founders Edition GPUs. However, there are some specific GPUs which also have their place:  
(1) For extra memory, I would recommend an RTX 2080 Ti. If you really need a lot of extra memory, the RTX Titan is the best option — but make sure you really do need that memory!  
(2) For extra performance, I would recommend an RTX 2080 Ti.  
(3) If you are short on money I would recommend any cheap GTX 10XX card from eBay (depending on how much memory you need) or an RTX 2060. If that is too expensive have a look at [Colab](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) .  
(4) If you just want to get started with deep learning a GTX 1060 (6GB) is a great option.  
(5) If you already have a GTX 1070 or better: Wait it out. An upgrade is not worth it unless you work with large transformers.  
(6) You want to learn quickly how to do deep learning: Multiple GTX 1060 (6GB).

## Deep Learning in the Cloud

Both GPU instances on AWS/Azure and TPUs in the Google Cloud are viable options for deep learning. While the TPU is a bit cheaper it is lacking the versatility and flexibility of cloud GPUs. TPUs might be the weapon of choice for training object recognition or transformer models. For other work-loads cloud GPUs are a safer bet — the good thing about cloud instances is that you can switch between GPUs and TPUs at any time or even use both at the same time.

However, mind the opportunity cost here: If you learn the skills to have a smooth work-flow with AWS/Azure, you lost time that could be spent doing work on a personal GPU, and you will also not have acquired the skills to use TPUs. If you use a personal GPU, you will not have the skills to expand into more GPUs/TPUs via the cloud. If you use TPUs you might be stuck with TensorFlow for a while if you want full features and it will not be straightforward to switch your code-base to PyTorch. Learning a smooth cloud GPU/TPU work-flow is an expensive opportunity cost and you should weight this cost if you make the choice for TPUs, cloud GPUs, or personal GPUs.

Another question is also about when to use cloud services. If you try to learn deep learning or you need to prototype then a personal GPU might be the best option since cloud instances can be pricey. However, once you have found a good deep network configuration and you just want to train a model using data parallelism then using cloud instances is a solid approach. This means that a small GPU will be sufficient for prototyping and one can rely on the power of cloud computing to scale up to larger experiments.

If you are short on money the cloud computing instances might also be a good solution: Prototype on a CPU and then roll out on GPU/TPU instances for a quick training run. This is not the best work-flow since prototyping on a CPU can be a big pain, but it can be a cost-efficient alternative.

## Conclusion 

With the information in this blog post, you should be able to reason which GPU is suitable for you. In general, I see three main strategies (1) stick with your GTX 1070 or better GPU, (2) buy a RTX GPU, (3) use some kind of GPU for prototyping and then train your model on TPUs or cloud GPUs in parallel.

## TL;DR advice

 **Best GPU overall** : RTX 2070  
 **GPUs to avoid** : Any Tesla card; any Quadro card; any Founders Edition card; Titan RTX, Titan V, Titan XP  
 **Cost-efficient but expensive** : RTX 2070  
 **Cost-efficient and cheap** :  RTX 2060, GTX 1060 (6GB).  
 **I have little money** : GTX 1060 (6GB)  
 **I have almost no money** : GTX 1050 Ti (4GB).Alternatively: CPU (prototyping) + AWS/TPU (training); or Colab.  
 **I do Kaggle: RTX 2070** . If you do not have enough money go for a GTX 1060 (6GB) or GTX Titan (Pascal) from eBay for prototyping and AWS for final training. Use fastai library.  
 **I am a competitive computer vision or machine translation researcher** : GTX 2080 Ti with the blower fan design. If you train very large networks get RTX Titans.  
 **I am an NLP researcher** : RTX 2080 Ti use 16-bit.  
 **I want to build a GPU cluster** : This is really complicated, you can get some ideas from my [multi-GPU blog post](http://timdettmers.com/2014/09/21/how-to-build-and-use-a-multi-gpu-system-for-deep-learning/) .  
 **I started deep learning and I am serious about it** : Start with an RTX 2070. Buy more RTX 2070 after 6-9 months and you still want to invest more time into deep learning. Depending on what area you choose next (startup, Kaggle, research, applied deep learning) sell your GPU and buy something more appropriate after about two years.  
 **I want to try deep learning, but I am not serious about it** : GTX 1050 Ti (4 or 2GB). This often fits into your standard desktop and does not require a new PSU. If it fits, do not buy a new computer!

Update 2019-04-03: Added RTX Titan and GTX 1660 Ti. Updated TPU section. Added startup hardware discussion.  
Update 2018-11-26: Added discussion of overheating issues of RTX cards.  
Update 2018-11-05: Added RTX 2070 and updated recommendations. Updated charts with hard performance data. Updated TPU section.  
Update 2018-08-21: Added RTX 2080 and RTX 2080 Ti; reworked performance analysis  
Update 2017-04-09: Added cost efficiency analysis; updated recommendation with NVIDIA Titan Xp  
Update 2017-03-19: Cleaned up blog post; added GTX 1080 Ti  
Update 2016-07-23: Added Titan X Pascal and GTX 1060; updated recommendations  
Update 2016-06-25: Reworked multi-GPU section; removed simple neural network memory section as no longer relevant; expanded convolutional memory section; truncated AWS section due to not being efficient anymore; added my opinion about the Xeon Phi; added updates for the GTX 1000 series  
Update 2015-08-20: Added section for AWS GPU instances; added GTX 980 Ti to the comparison relation  
Update 2015-04-22: GTX 580 no longer recommended; added performance relationships between cards  
Update 2015-03-16: Updated GPU recommendations: GTX 970 and GTX 580  
Update 2015-02-23: Updated GPU recommendations and memory calculations  
 Update 2014-09-28: Added emphasis for memory requirement of CNNs 

 **Acknowledgments** 

I want to thank  Mat Kelcey  for helping me to debug and test custom code for the GTX 970; I want to thank Sander Dieleman for making me aware of the shortcomings of my GPU memory advice for convolutional nets; I want to thank Hannes Bretschneider for pointing out software dependency problems for the GTX 580; and I want to thank Oliver Griesel for pointing out notebook solutions for AWS instances. I want to thank Brad Nemire for providing me with an RTX Titan for benchmarking purposes.

**********
[GPU](/tags/GPU.md)
[RTX](/tags/RTX.md)
[GTX](/tags/GTX.md)
[Titan](/tags/Titan.md)
